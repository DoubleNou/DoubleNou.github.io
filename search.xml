<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入理解HDFS]]></title>
    <url>%2F2019%2F07%2F12%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3HDFS%2F</url>
    <content type="text"><![CDATA[胡说八道HDFS，是Hadoop Distributed File System的简称，是Hadoop抽象文件系统的一种实现。Hadoop抽象文件系统可以与本地系统、Amazon S3等集成，甚至可以通过Web协议（webhsfs）来操作。HDFS的文件分布在集群机器上，同时提供副本进行容错及可靠性保证。例如客户端写入读取文件的直接操作都是分布在集群各个机器上的，没有单点性能压力。 HDFS设计原则 设计目标 · 存储非常大的文件· 采用流式的数据访问方式: HDFS基于这样的一个假设：最有效的数据处理 模式是一次写入、多次读取数据集经常从数据源生成或者拷贝一次· 运行于商业硬件上 HDFS不适合的应用类型 ·低延时的数据访问·大量小文件·多方读写，需要任意的文件修改 HDFS核心概念 Blocks Namenode &amp; Datanode Block Caching HDFS Federation HDFS HA(High Availability高可用性) Hadoop的HA方案 主备需共享edit log存储 DataNode需要同时往主备发送Block Report 客户端需要配置failover模式（失效备援模式，对用户透明） Standby替代Secondary NameNode 命令行接口HDFS提供了各种交互方式，例如通过Java API、HTTP、shell命令行的。命令行的交互主要通过hadoop fs来操作。例如：123hadoop fs -copyFromLocal // 从本地复制文件到HDFShadoop fs mkdir // 创建目录hadoop fs -ls // 列出文件列表 Hadoop文件系统前面Hadoop的文件系统概念是抽象的，HDFS只是其中的一种实现。Hadoop提供的实现如下图：Local是对本地文件系统的抽象，hdfs就是我们最常见的，两种web形式（webhdfs，swebhdfs）的实现通过HTTP提供文件操作接口。har是Hadoop体系下的压缩文件，当文件很多的时候可以压缩成一个大文件，可以有效减少元数据的数量。viewfs就是我们前面介绍HDFS Federation张提到的，用来在客户端屏蔽多个Namenode的底层细节。ftp顾名思义，就是使用ftp协议来实现，对文件的操作转化为ftp协议。s3a是对Amazon云服务提供的存储系统的实现，azure则是微软的云服务平台实现。 前面我们提到了使用命令行跟HDFS交互，事实上还有很多方式来操作文件系统。例如Java应用程序可以使用org.apache.hadoop.fs.FileSystem来操作，其他形式的操作也都是基于FileSystem进行封装。我们这里主要介绍一下HTTP的交互方式。WebHDFS和SWebHDFS协议将文件系统暴露HTTP操作，这种交互方式比原生的Java客户端慢，不适合操作大文件。通过HTTP，有2种访问方式，直接访问和通过代理访问 Namenode和Datanode默认打开了嵌入式web server，即dfs.webhdfs.enabled默认为true。webhdfs通过这些服务器来交互。元数据的操作通过namenode完成，文件的读写首先发到namenode，然后重定向到datanode读取（写入）实际的数据流。 采用代理的示意图如下所示。 使用代理的好处是可以通过代理实现负载均衡或者对带宽进行限制，或者防火墙设置。代理通过HTTP或者HTTPS暴露为WebHDFS，对应为webhdfs和swebhdfs URL Schema。 代理作为独立的守护进程，独立于namenode和datanode，使用httpfs.sh脚本，默认运行在14000端口 除了FileSystem直接操作，命令行，HTTTP外，还有C语言API，NFS，FUSER等方式，这里不做过多介绍。 Java接口]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[普罗米修斯监控]]></title>
    <url>%2F2019%2F07%2F11%2F%E6%99%AE%E7%BD%97%E7%B1%B3%E4%BF%AE%E6%96%AF%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[简介作为新一代的监控框架，Prometheus 具有以下特点： · 强大的多维度数据模型： 时间序列数据通过 metric 名和键值对来区分。 所有的 metrics 都可以设置任意的多维标签。 数据模型更随意，不需要刻意设置为以点分隔的字符串。 可以对数据模型进行聚合，切割和切片操作。 支持双精度浮点类型，标签可以设为全 unicode。 · 灵活而强大的查询语句（PromQL）： · 易于管理： Prometheus server 是一个单独的二进制文件，可直接在本地工作，不依赖于分布式存储。·高效：平均每个采样点仅占 3.5 bytes，且一个 Prometheus server ·可以处理数百万的 metrics。·使用 pull 模式采集时间序列数据，这样不仅有利于本机测试而且可以避免有问题的服务器推送坏的 metrics。·可以采用 push gateway 的方式把时间序列数据推送至 Prometheus server 端。·可以通过服务发现或者静态配置去获取监控的 targets。·有多种可视化图形界面。·易于伸缩。·需要指出的是，由于数据采集可能会有丢失，所以Prometheus不适用对采集数据要 100%准确的情形。但如果用于记录时间序列数据，Prometheus具有很大的查询优势，此外，Prometheus 适用于微服务的体系架构。]]></content>
  </entry>
  <entry>
    <title><![CDATA[dubbo深度搞一搞]]></title>
    <url>%2F2019%2F07%2F01%2Fdubbo%E6%B7%B1%E5%BA%A6%E6%90%9E%E4%B8%80%E6%90%9E%2F</url>
    <content type="text"><![CDATA[各层说明总结（参考dubbo源码） ==================== Business ====================Service 业务层：业务代码的接口与实现。我们实际使用 Dubbo==================== RPC ====================config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 Spring 解析配置生成配置类。 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory 。 dubbo-rpc-rpc 模块实现。com.alibaba.dubbo.rpc.proxy包 + com.alibaba.dubbo.rpc.ProxyFactory接口 。 registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService 。dubbo-registry 模块实现。cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance 。 monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService 。dubbo-monitor 模块实现。==================== Remoting ====================protocol 远程调用层：封将 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter 。dubbo-rpc-rpc 模块实现。com.alibaba.dubbo.rpc.protocol包 + com.alibaba.dubbo.rpc.Protocol接口 。 exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer 。dubbo-remoting-api 模块定义接口。com.alibaba.dubbo.remoting.exchange包。transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec 。dubbo-remoting-api 模块定义接口。com.alibaba.dubbo.remoting.transport包。serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 。dubbo-common 模块实现。com.alibaba.dubbo.common.serialize包。]]></content>
  </entry>
  <entry>
    <title><![CDATA[一套客服系统的服务端]]></title>
    <url>%2F2019%2F05%2F30%2F%E4%B8%80%E5%A5%97%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9C%8D%E5%8A%A1%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[我的客服IM模块划分gate ###创建场景、登陆jwt校验、获取会话列表和会话页、种类消息、消息推送、结束评价 ### 网络编程思想通信协议、通信接口]]></content>
  </entry>
  <entry>
    <title><![CDATA[阿里dubbo---北京MeetUp会议记录2019.5.26]]></title>
    <url>%2F2019%2F05%2F26%2F%E5%9C%A8%E5%BA%94%E7%94%A8%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[XA模式 主讲（申海强）锁–我们在传统的系统进行请求访问的时候，一次请求都要经过XA，start-end，涉及了很多分布式事务的锁。 连接–如果说链路太长，那么这个连接不能释放。 TM begin- TC 事务id - RM 注册TC 统一用TC回溯（回追一些事务）— TM end ###本地模式必须遵守ACID 牛逼的点：undoLog TC - Rm 来嵌套下，saga tcc 模式 ###提问环节 对于olap也没有很好的办法，本架构也主要在oltp进行开展。 TC高可用是否可以得到保障，一个新技术引入架构的话，是一个单点隐患，复杂的问题聚焦到一个点。 undoLog确定是存在数据库中，这就解释了本地模式为什么是acid。 TC对链路的控制，优先选择第一条链路。 其他外国大厂并没有对微服务层次上对事务有把控、只是在数据库上做了。 apache dubbo的设想 主讲（秦金卫）dubbo的想法 abc基础能力 云一体化、人工智能、大数据。悄悄告诉大家，如果创业，这三方面一定占据几个 ###生态 商场模式，all in one.所有东西都放在一个商场中，什么都有，吸引所有人进来。一旦全部链路打通后，就成了大家习惯的一种姿态。 容器模式 单独部署、单独建立 ###如何发展 聚焦于行业领域 定制化 社区完善 ###dubbo 一个发展很牛逼的东西，可以看成是eip、不用修改一行代码， 直接暴露出所有东西。 ###欢迎参与阿里开源 ###提问环节 用新技术搭新工程其实是技术选型问题。 如果是面对老系统，其实就是和旧世界作斗争，选择办法打通一条通往新世界的大门，开一个口子接入新系统 eip目前还在处于构造中。 sentinel 网关限流 赵亦豪（宿何）sentinel微服务稳定性的场景 场景： 激增流量 下游服务的不稳定性可行性： 流量控制 流量整型 熔断降级 系统自适应保护 要在系统出现堕机前进行，进行保护 ###1.6.0新特性 注解的支持改进 异常类型限定 网关流量控制 更好的errorCallBack的返回 开源万岁 ###提问环节 一旦集成后，动态server的管理：可以搞成一种独立模式的一种agent。 sentinel熔断没有一个半打开的功能。豪猪有 dubbo服务自省设计与实现 主讲（今天最牛逼的小马哥）什么是服务自省 dubbo可以暴露很多协议，dubbo应用在运行时处理和分析dubbo的服务元信息 dubbo对外提供的是服务，cloud是接口。微服务还有些地方不是特别成熟，本次升级dubbo自省模式，加入了metaData原数据。 zk向nacos的迁移一句话，早晚的事，zk其实挺被诟病的，只是因为没有更好的一个选择，注册中心这个概念，正是因为确定保证它的稳定性，所以要开发出的一个良好的模式]]></content>
  </entry>
  <entry>
    <title><![CDATA[我是菜鸟---java基础通用复习篇]]></title>
    <url>%2F2019%2F05%2F15%2F%E6%88%91%E6%98%AF%E8%8F%9C%E9%B8%9F-java%E5%9F%BA%E7%A1%80%E9%80%9A%E7%94%A8%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[java集合、java并发、java网络、java虚拟机 面向对象是java的一句代表 面向对象的特征？四点记住！封装、继承、多态、抽象。 面向对象—封装封装给对象提供了隐藏内部特性和行为的能力，对象提供一些给其他对象改变自己内部参数的方法。这么一说比较绕，简单来讲就是我们常见的四种类修饰符：private、friendly、projected、public。这些修饰符给对象赋予了不同的访问权限。 封装的好处：通过隐藏对象的属性来保护对象内部的状态，提高了代码可读性、维护性。模块化。 面向对象—继承继承，给对象提供了从基类获取字段和方法的能力。继承提供了代码的重用行，也可以在不修改类的情况下给现存的类添加新特性。 面向对象—多态多态，是编程语言给不同的底层数据类型做相同的接口展示的一种能力。一个多态类型上的操作，可以应用到其他类型的值上面。 面向对象—抽象抽象，是把想法从具体的实例中分离出来的步骤，因此，要根据他们的功能而不是实现细节来创建类。Java 支持创建只暴漏接口而不包含方法实现的抽象的类。这种抽象技术的主要目的是把类的行为和实现细节分离开。 面向过程与面向对象的区别性能高低不同，自己想想哪个高低？ 重载和重写的区别？1）重写 override 方法名、参数、返回值相同。子类方法不能缩小父类方法的访问权限。子类方法不能抛出比父类方法更多的异常(但子类方法可以不抛出异常)。存在于父类和子类之间。方法被定义为 final 不能被重写。 2）重载 overload 参数类型、个数、顺序至少有一个不相同。不能重载只有返回值不同的方法名。存在于父类和子类、同类中。 Java 中，什么是构造方法？什么是构造方法重载？什么是拷贝构造方法？1）构造方法 当新对象被创建的时候，构造方法会被调用。每一个类都有构造方法。在程序员没有给类提供构造方法的情况下，Java 编译器会为这个类创建一个默认的构造方法。 2）构造方法重载 Java 中构造方法重载和方法重载很相似。可以为一个类创建多个构造方法。每一个构造方法必须有它自己唯一的参数列表。 3）拷贝构造方法 Java 不支持像 C++ 中那样的拷贝构造方法，这个不同点是因为如果你不自己写构造方法的情况下，Java 不会创建默认的拷贝构造方法。 JDK、JRE、JVM 分别是什么关系？🐷JDKJDK 即为 Java 开发工具包，包含编写 Java 程序所必须的编译、运行等开发工具以及 JRE。开发工具如： 用于编译 Java 程序的 javac 命令。用于启动 JVM 运行 Java 程序的 Java 命令。用于生成文档的 Javadoc 命令。用于打包的 jar 命令等等。 简单说，就是 JDK 包含 JRE 包含 JVM。 🐷JRE JRE 即为 Java 运行环境，提供了运行 Java 应用程序所必须的软件环境，包含有 Java 虚拟机（JVM）和丰富的系统类库。系统类库即为 Java 提前封装好的功能类，只需拿来直接使用即可，可以大大的提高开发效率。 简单说，就是 JRE 包含 JVM。 🐷JVM 为什么 Java 被称作是“平台无关的编程语言”？Java 源文件( .java )被编译成能被 Java 虚拟机执行的字节码文件( .class )。Java 被设计成允许应用程序可以运行在任意的平台，而不需要程序员为每一个平台单独重写或者是重新编译。Java 虚拟机让这个变为可能，因为它知道底层硬件平台的指令长度和其他特性。 JDK 各版本的新特性？JDK5 ~ JDK10 ，看 https://www.jianshu.com/p/37b52f1ebd4a 。JDK11 ，看 https://www.jianshu.com/p/81b65eded96c 。对于大多数面试官，肯定不会问你 JDK 各版本的新特性，更多的会问 JDK8 引入了什么重要的特性？一般上，关键的回答是Lambda 表达式和集合之流式操作，然后说说你在项目中怎么使用的。 JVM 即为 Java 虚拟机，提供了字节码文件(.class)的运行环境支持。 Java 和 C++ 的区别？都是面向对象的语言，都支持封装、继承和多态。Java 不提供指针来直接访问内存，程序内存更加安全。Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。【重要】Java 有自动内存管理机制，不需要程序员手动释放无用内存。 什么是字节码？采用字节码的最大好处是什么？这个问题，面试官可以衍生提问，Java 是编译执行的语言，还是解释执行的语言。 Java 源代码=&gt; 编译器 =&gt; JVM 可执行的 Java 字节码(即虚拟指令)=&gt; JVM =&gt; JVM 中解释器 =&gt; 机器可执行的二进制机器码 =&gt; 程序运行 每一种平台的解释器是不同的，但是实现的虚拟机是相同的。Java 源程序经过编译器编译后变成字节码，字节码由虚拟机解释执行，虚拟机将每一条要执行的字节码送给解释器，解释器将其翻译成特定机器上的机器码，然后在特定的机器上运行。这也就是解释了 Java 的编译与解释并存的特点。 java集合集合框架我觉得不管是学习知识还死活开发实战都是必要的，不可苟且。 一个工具类中的集合可以提高很多的性能。 map collection map set list。 递归的使用递归的使用要定义递归头和递归体。递归头定义出什么时候不调用自己，否则会出现死循环。递归体定义什么什么时候调用自己。但递归太耗费性能了，考虑的话完全可以用循环来替代。 面向过程和面向对象c语言面向过程。java面向过程。都是设计软件的基本思想，相辅相成，不是对立的。面向过程，遇到事了，专注于怎么做，比如说第一步怎么做、第二步怎么做，就是面向过程。面向过程适合简单任务，简单任务是指不需要协作的事务。面向对象，解决复杂问题，宏观上使用面向对象把握，但是微观处理仍然是面向过程。 对象可以看为数据管理的方式，也可以看为内存中的一个内存块，包含了很多数据。程序发展之初是不需要对象的，只需要基本数据类型就可以了。但是事物的发展总是量变引起质变。 类和对象对象的概念由小作坊到大楼、到公司、到大楼。 对象和类的概念，类可以看作对象的模版。类中包含了成员变量和方法。 见到new就是调用一个类的构造方法，new出来了一个实例，这个无参构造方法默认是系统自动创建的。 java虚拟机 jvm说下几个jvm中的概念吧：栈、堆、方法区。 ######栈 栈描述的就是方法执行的模型。每个方法被调用都会创建一个栈帧（存储局部变量、操作数、方法出口等）。 java为每个线程提供一个栈，用于存放该线程执行的方法信息 栈属于线程私有，不能实现共享。 栈的特性，结合方法的使用来思考。 栈是由系统自动分配，速度快，栈是一个连续的内存空间。 ######堆 堆用于存储对象创建好的对象和数组，数组也是对象。 jvm只有一个堆，线程共享。 堆的内存不连续。 ######方法区 方法区其实也是堆。 用于存储类相关的信息。 用于存储程序中永远不变或者唯一的内容：class信息、静态变量、字符串常量、class对象。 一次简单的函数调用，需要去画一次内存分配和执行流程123456789101112131415161718192021222324252627282930public class execise &#123; private String name; private Integer age; void study()&#123; System.out.println("学习中，学习人："+name); &#125; void age()&#123; System.out.println("学习的年龄"+age); &#125; User user; public static void main(String[] args) &#123; execise execise = new execise(); execise.age = 30; execise.name = "nounounou"; execise.study(); execise.age(); User user = new User(); user.setPassword("123"); &#125;&#125; 构造方法构造方法的方法名必须和类名保持一致 如果自己定义了一个类的构造方法，那么如果像这样 User user = new User();系统就不管了，就要自己重新定义。 this关键字表示创建好的对象 垃圾回收机制GB collection垃圾回收机制 我们提前说下比较有意思一个故事，C++和java对比最头疼的就是内存管理需要程序员自己去解决。但是java有了垃圾回收器，解决了这个问题。 C++就像学校的食堂，java就像外面的餐馆。 垃圾回收器发现垃圾，算法大致了解一下(1)可达法和(2)引用计数法但是引用计数法有个缺陷就是两个对象之间循环引用。 我建议在垃圾回收的过程也画一遍图 maniro major full gc this创建对象分为四步 分配对象空间，并将对象成员变量初始化为0或空。 执行属性值的显示初始化。 执行构造方法。 返回对象的地址给相关变量。 == 和 equal记住一点就可以，基本类型可以看作最简单的类型，用哪个都可以比较但是想一下，涉及一个对象的话，==号就失效了。为什么呢&gt;比较的是地址hascode，两个对象的hashcode是不一样的，一般常用的object类都重写了equal的方法。equal可以比较他们的内容。所以在撸码中，非要比较两个对象是否相等，那就重写这个类的equal方法。比较里面的某一个参数是否相等。 两个对象的地址不一致。 封装对像类中的封装不要暴露给外面使用，一般会设置为private。 我们看看集中访问权限：private、default、protected、public 同一个类-&gt;同一个包-&gt;子类-&gt;全局 不管三七二十一，类中属性变量都私有。 多态我们去调一个方法，参数不同，行为不同。 数组数组其实也是一个对象，记住里面对应每一个组标的数据可以看作是对象的成员变量。 finalfinal修饰的方法不能重写、类不能继承、修饰字符串则为字符串常量。 数组` public static void main(String[] args) { String abc = &quot;hello java&quot;; String def = &quot;HELLo java&quot;; System.out.println(abc.substring(0, 9)); System.out.println(abc == def); System.out.println(abc.equalsIgnoreCase(def)); System.out.println(abc.replaceAll(&quot;java&quot;,&quot;asd&quot;)); System.out.println(abc.compareTo(&quot;123&quot;)); String[] array = {&quot;1&quot;,&quot;2&quot;,&quot;3&quot;}; System.out.println(JSON.toJSONString(testDeleteArrayOfIndex(array, 0))); System.out.println(ArrayUtils.toString(testInsertArrayOfIndex(array, 1,2))); } /** * 删除指定数组元素 * @param array * @param index * @return */ public static String[] testDeleteArrayOfIndex(String[] array, Integer index){ // String[] array2 = new String[array.length -1]; System.arraycopy(array,index+1 , array, index, array.length - index -1); array[array.length -1 ] = null; return array; } public static String[] testInsertArrayOfIndex(String[] array, Object param, Integer index){ // String[] array2 = new String[array.length -1]; array = extendArray(array, 1); System.out.println(array.length); System.arraycopy(array, index , array, index+1, array.length-index - 1); array[index] = param.toString(); return array; } /** * 数组扩容 * @param array * @param length * @return */ public static String[] extendArray(String[] array, Integer length){ String[] s1 = {&quot;1&quot;,&quot;2&quot;,&quot;3&quot;}; String[] s2 = new String[s1.length + length]; System.arraycopy(s1, 0 , s2, 0, s1.length); return s2; }` 二分查找` public static void main(String[] args) { String[] array = {&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;}; System.out.println(midSort(array, &quot;6&quot;)); } public static int midSort(String[] array, String value){ int left = 0; int right = array.length - 1; while ( left &lt;= right){ int mid = (left + right)/2 ; if(array[mid].equals(value)){ return mid; } if(Integer.valueOf(value) &gt; Integer.valueOf(array[mid])){ left = mid+1; } if(Integer.valueOf(value) &lt; Integer.valueOf(array[mid])){ right = mid-1; } } return -1; } ` List接口` public static void main(String[] args) { List list01 = new ArrayList&lt;&gt;(); list01.add(“01”); list01.add(“02”); list01.add(“03”); List list02 = new ArrayList&lt;&gt;(); list02.add(“01”); list02.add(“02”); list02.add(“04”); list02.add(“05”); list01.removeAll(list02); System.out.println(list01); System.out.println(list01); } `]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我是菜鸟---java基础类型篇]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%88%91%E6%98%AF%E8%8F%9C%E9%B8%9F-java%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[局部变量、全局变量、静态变量的意思和生命周期注意点：局部变量是必须赋值才能使用，全局变量即使不赋值也有默认值，静态变量是全局的生命周期最长。 常量只要给的值就不能再改变了，int age = 1；1是常量，age是变量。final 修饰的字符又是字符常量final int age。 基本数据类型（primitive data） ----数值型 整数类型（byte、short、int、long）浮点类型(float、double) （3大类、8小类） ----基本数据类型 ----字符型 char 2个字节 ----布尔型 boolean 1位 注意⚠️ 一个bit 数据类型 ----引用数据类型 类、接口、数组 占用4个字节。代表对象地址 整型变量/常量byte、short、int、long 十进制正常数字、八进制015、十六进制0X16、二进制0b1101; 浮点型float、double float a = 3.14F; double b = 1.14;浮点数是不精确的 ，一定不要用做比较。如果想比较浮点数，要用java.math包下面的BigDecimal和BigInteger BigDecimal.valueOf(0.01); 布尔类型true and false 顺序结构、选择结构、循环结构这节其实很易懂，但很多人忽略了，一个很小很小的程序，或者大的操作系统说白了都是变量+选择结构+循环结构组成的。 break、continuebreak跳出整套循、continue跳出本次循环。另外还有带标签的break、continue。这个是可以跳转的。之前有个goto关键字，是java一个保留字读。如果大量使用goto语句，程序结构可能会失去控制。所以我们使用带标签的break和continue。 场景：希望在一个嵌套循环中，跳出到外侧循环， outer:for（）{ if(){ break outer;}}]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础复习---jvm的几大分区和oom的情况]]></title>
    <url>%2F2019%2F05%2F13%2Fjava%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0-jvm%E7%9A%84%E5%87%A0%E5%A4%A7%E5%88%86%E5%8C%BA%E5%92%8Coom%E7%9A%84%E6%83%85%E5%86%B5%2F</url>
    <content type="text"></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[豪猪熔断器]]></title>
    <url>%2F2019%2F05%2F04%2F%E8%B1%AA%E7%8C%AA%E7%86%94%E6%96%AD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[熔断器的思想和我们日常所用的保险丝一样，是一款保护全局服务正常调用的工具，防止服务雪崩。]]></content>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ribbon]]></title>
    <url>%2F2019%2F05%2F02%2FRibbon%2F</url>
    <content type="text"><![CDATA[Ribbon的概念，实现客户端的软件式负载均衡。 负载均衡概念：集中式LB （F5）硬件层面 和 进程式LB软件层面。 Ribbon配置初步 正确的思维和落地方法论。 GAV-maven @Enable*** 123456789 @Configurationpublic class webConfig &#123; @Bean @LoadBalanced public RestTemplate getRestTemplate()&#123; return new RestTemplate(); &#125;&#125; Ribbon和eureka整合后，consumer可以直接调用不用再关心地址和端口号。 Ribbon的负载均衡默认轮询算法 配置多个eureka+多个消费+多个注册。每个注册独有DB。 Ribbon核心组件IRule在configuration 1234567891011121314 @Configurationpublic class webConfig &#123; @Bean @LoadBalanced public RestTemplate getRestTemplate()&#123; return new RestTemplate(); &#125; @Bean public IRule myRule()&#123; return new RadomRobinRule(); &#125;&#125; 根据特定算法选出来一个负载算法。 Ribbon自定义自定义配置类，配置类不要放在compent的同目录下面。在启动类上面打@RobinClient(“微服务名称”,Rule.class) 在配置类中的，重新new一个实现了负载算法的实例子。]]></content>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通读mysql的（持续更新）]]></title>
    <url>%2F2019%2F05%2F01%2F%E9%80%9A%E8%AF%BBmysql%E7%9A%84%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[作为一名工程师，不论前端后端，不熟悉甚至不了解mysql，真的很失败。本篇是持续更新篇，也是给自己一个复习空间。 2019年5月1日—mysql的逻辑架构一些基本概念吧，上学的时候并不想背这些，其实怪自己没有下功夫去实践，实践几遍，不用背也能说出个123。ACID、数据库的并发。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop分布式的节操安装实践]]></title>
    <url>%2F2019%2F04%2F29%2Fhadoop%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E8%8A%82%E6%93%8D%E5%AE%89%E8%A3%85%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[前言Hadoop是一个由Apache基金会所开发的分布式系统基础架构。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算.本教程将指导如何用苹果macOS系统安装Hadoop。 安装介绍此篇来点真格的安装过程，可以按照流程走一遍，少走很多安装的大坑…..我当时安装了几乎小半个月，哎。菜是原罪…. 另外，为什么说这是hadoop的分布式的节操呢？以下我分三块来说，例如hadoop的单机模式（流氓模式）、hadoop伪分布式模式（无赖模式）、hadoop完全分布式模式（正经人）。 这三种模式其实都是hadoop，就是我们安装的方式不一样。 流氓三剑客单机模式（standalone） 单机模式是Hadoop的默认模式。这种模式在一台单机上运行，没有分布式文件系统，而是直接读写本地操作系统的文件系统。当首次解压Hadoop的源码包时，Hadoop无法了解硬件安装环境，便保守地选择了最小配置。在这种默认模式下所有3个XML文件均为空。当配置文件为空时，Hadoop会完全运行在本地。因为不需要与其他节点交互，单机模式就不使用HDFS，也不加载任何Hadoop的守护进程。该模式主要用于开发调试MapReduce程序的应用逻辑。 伪分布模式（Pseudo-Distributed Mode） 这种模式也是在一台单机上运行，但用不同的Java进程模仿分布式运行中的各类结点 伪分布模式在“单节点集群”上运行Hadoop，其中所有的守护进程都运行在同一台机器上。该模式在单机模式之上增加了代码调试功能，允许你检查内存使用情况，HDFS输入输出，以及其他的守护进程交互。 全分布模式（Fully Distributed Mode） Hadoop守护进程运行在一个集群上。 依靠无赖走天下自古流氓怕圣人、圣人怕无赖。我们先摆平无赖再去当圣人。 无赖模式、不，是伪分布式模式的安装用一台单机就可以模拟，废话少说。安装！我是在mac上模拟的，安装器也是Homebrew、如果在Linux环境下，自行选择安装器哈，流程都是一样的。 打开shell命令，直接上—brew install hadoop 安装过程会提示重要的信息，如下： $JAVA_HOME has been set to be the output of: /usr/libexec/java_home Hadoop的安装需要配置JAVA_HOME，用 brew安装，就已经帮我们配置好了。Java_Home老铁们知道干啥的吧….不多说了啊，系统环境变量。 测试hadoop安装是否成功 分布式模式需要在多台电脑上面测试，这里只测试前面两种，即单机模式和伪分布式模式。走一遍hadoop的mapReduce任务吧，一个基础操作就是输入输出。在hadoop的安装目录中，我的是这个/usr/local/Cellar/hadoop/3.1.1/libexec/share/hadoop/mapreduce。看见里面的client-jar包了吧，ok，我们到 cd /usr/local/Cellar/hadoop/3.1.1/创建input和output目录进入input目录 cd input，创建两个文件 echo ‘hello world’ &gt; file1.txt 、echo ‘hello hadoop’ &gt; file2.txt。 运行示例检测单机模式,在shell上直接走！ hadoop jar ./libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar wordcount ./input ./output 显示结果more output/part-r-00000 —- hadoop 1 hello 2 world 1 进阶无赖伪分布式要修改的hadoop的配置了，这些配置是很关键的，有时间的同学可以挖一挖。 so，cd 到 /usr/local/Cellar/hadoop/3.1.1/libexec/etc/hadoop中 修改Core-site.xml这是hadoop和hdfs的核心配置，有些配置是公用的。 修改为： 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/Cellar/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;configuration&gt; s.default.name 保存了NameNode的位置，HDFS和MapReduce组件都需要用到它，这就是它出现在core-site.xml 文件中而不是 hdfs-site.xml文件中的原因 修改mapred-site.xml.template 修改为： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 修改为： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 变量dfs.replication指定了每个HDFS数据库的复制次数。 通常为3, 由于我们只有一台主机和一个伪分布式模式的DataNode，将此值修改为1。 hadoop namenode -format./sbin/start-all.sh 用示例测试 估计圆周率PI的值 hadoop jar ./libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 2 5 运行结果 Estimated value of Pi is 3.60000000000000000000 靠谱的一篇正经人，Hadoop完全分布式教程完全分布式和伪分布式区别就是集群的搭建，加入了zk的配置。具体流程其实不复杂。 相关文档：https://blog.csdn.net/sunqingok/article/details/87210767]]></content>
      <tags>
        <tag>大数据，hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅分析HDFS]]></title>
    <url>%2F2019%2F04%2F28%2F%E6%B5%85%E5%88%86%E6%9E%90HDFS%2F</url>
    <content type="text"><![CDATA[前言和大家说说我接触大数据的经历吧。在去年刚毕业的我，一脸懵逼的进了公司，当时所属我的一个标签就是菜狗。什么都是新的，什么也都是挑战的。更糟糕的是面对突入其来的压力，走了很多弯路。其实回想下陌生的恐惧支配了我大部分的经历。唯一的欣慰点是这样的压力促进了我加快学习。压力和工作气氛稍有好转的几天后，一个叫做HBase的玩具交给了我，让我玩一玩。简要来说— 目标 运用HBase把海量数据查询和写入搞定，代替传统的关系型数据库mysql，我们的mysql日均写入量百万。在接手的时候总库的量已经破了4亿。没错，你在一个亿量级的表里面找条数据（索引失效），你想想后果。其实可以避免索引失效的，具体什么场景我后续来说。 HBase属于新应用，怎么搞自己去看吧，我tm居然从服务端学了起来，虽然很浅读，但基本HBase架构读了一遍。数据流怎么走的也有了基础掌握，返回头才看client端。我们应用方属于c端，大数据部门原来替我们做了server的管理。（其实我觉得，服务端和客户端都要学，因为调优的参数应用要懂吧、实际业务难免要做业务补偿，比如旧的历史消息bulkload进HBase、安全机制） 实际业务需求，这里所说的就是我们不敢，甚至任何一家公司新运用HBase也不敢直接取代mysql的地位。但想用HBase，那怎么保留mysql和HBase平等地位呢？欢迎留言探讨。 压测，我居然搞了jemeter接口测试，过程你懂的。其实HBase有专业的压测，支持多线程。YCSB！ HBase的基础—HDFS大家不要被大数据搞晕了，其实没什么恐怖的，虽然学起来有些难度，HDFS是大数据存储的底层系统了，从这里我们可以了解到整个Hadoop生态圈，我们先搞定这个，再去玩HBase。进而玩起spark、sqoop、Flink。 先给大家讲个故事一次赵栋请客吃饭，他叫了二十多个人，忆江南并没有大桌，只能分小桌落座。so，众口难调，为了照顾大家，每桌都是自己点菜，赵栋也是忙前忙后，忙着了解大家都点了些啥，毕竟他要结账。服务员一下子面对六桌，菜有点多，有些菜忘了上哪桌，赵栋也会告诉他们。吃着吃着，突然博英桌点的一道猪蹄汤进了一个苍蝇。赵栋说：“没事儿，隔壁桌也点了这个汤。我去给你盛一碗。” 吃完饭，大家都很开心，一起踏上去小竹签的路上… 你以为我在说废话，讲故事。这个故事就是HDFS的精髓！ 初识HDFS—分布式文件系统先不谈分布式，先说说文件系统：文件系统就是用来管理文件的一个系统，比如你今天送我的mac pro，我在命令行敲一个ls，就能看到当前目录的文件。就算你没有钱、没送我mac。没关系，你给我装一个linux系统，我运行ls出来的显示，同样是直观的文件系统。文件系统官方来说是存储文件、管理文件，并且提供文件系统的crud。问题来了，这些文件存在哪？硬盘！ 知道了吧，万事万物想要持久化都要必进硬盘，别跟我扯什么云….云最后去哪了。 单台机器，单个硬盘的场景。假设，我有一个文件（600G），但我的mac只有256G，我存个鬼。so，怎么办。有人跟我说加硬盘…你干点别的吧。把这个文件拆开呀！！！我有钱，我有10台mac，那我就拆成10份。每份60G，完全可以放进单台256G的mac了。多台机器形成的系统就是分布式文件系统。那么这里又有一个问题，单台机器单台ls，多台机器每个都要ls吗？否，提供一个统一的查询接口，对使用者而言就像操作一台机器一样！这就是HDFS下的 hdfs dfs -ls ，命令的精髓。也就是分布式文件系统的意思。 文件切片赵栋请吃饭，一下子来20多人，怎么可能坐一桌，拆啊！！！这不就是文件切片的一个意思么。大文件切块后分别存储在多台机器，然后提供统一的操作接口，是不是分布式文件系统hdfs也是挺简单的。 潜在问题我把文件分了这么多份，我想找一个文件所在位置。我怎么去找到确定的一台机器呢？有人说，遍历一遍所有机器….哈哈，这样就太慢了，而且效率很低，是一个o(n)的算法。还有啊，我们刚刚说一个文件分别存在几台机器上，假如其中一台机器坏了，那么这个文件就不能访问了？有人说，机器故障率是0.000001%，然后他就觉得没问题。他这样想属于正常思维，但是一点就是他说的是单台机器。我们有10000000台机器，每台机器的事故率如上，那么这套系统的事故率是多少呢？10%….哪有这么多台机器！？你问问阿里的工程师他们有多少台虚机、物理机….如果要存储PB级或者EB级的数据，成千上万台机器组成的集群是很常见的，所以说分布式系统比单机系统要复杂得多呀。 潜在问题破壁1——上菜是溯源破壁者的使命都是从他们的宿愿说起，了解下HDFS架构至关重要，才能解决这些潜在问题。再回溯到刚才说的吃饭情景。 服务员上菜的同时，并不知道这是哪桌的菜，“what fuck！5桌人，我该问谁？”。只能问赵栋，菜都是赵栋点的 同等，我们看看HDFS的架构图像不像… 自己类比下啊，你可以把每一桌看作是一个DataNode、每一道菜看成一个文件切片，服务员相当于Client。文件切片具体放到哪个文件中，要去NameNode中寻找。DataNode是真正存储数据的地方，NameNode相当于一个管理者master，它知道每一个DataNode的存储情况，client其实就是那个对外操作的统一接口。讲到这，引申一下，服务员不管上菜（写数据），还是找哪桌的盘子空了（读数据）都要先去赵栋这里询问下，赵栋（NameNode）统一回复服务员。这样时间复杂度是O(1)。效率提升了不少。 破壁2——掉进猪蹄汤的苍蝇假如说某一个DataNode坏了怎么办？像我刚才说的坏了一台机器。再类比吃饭什么情景呢？对，苍蝇掉进了猪蹄汤里。这一桌的人都不想动筷了，但又对这些美食抗拒不住。其他桌有相同的菜啊！这桌人全部去了隔壁桌…. 听到这里，有没有发现什么。没错，hdfs在写入一个数据块的时候，不会仅仅写入一个DataNode，而是会写入到多个DataNode中，这样，如果其中一个DataNode坏了，还可以从其余的DataNode中拿到数据，保证了数据不丢失。多数据备份不仅在这里体现，大数据生态里面有很多也用到了这样的做法，例如kafka的多数据备份，但是都要分布在不同的机器上，保证一台机器挂了，可以在其他机器节点上找到数据。这样对存储空间的要求就很大，现在SSD都降价了，别说你的存储空间不够用。 破壁3——上菜都是服务员的事儿服务员把美食从后厨端过来的时候，就已经知道要把四盘一样的菜轮番上到不同的四桌。这里代表数据分片已经由Client端做了。 破壁4—— 一套完整的酒席从服务员上菜分桌，到同学们吃饭，再到离场撤盘子。这一套完整的酒席，就是HDFS一套完整的交互，流读取。HDFS查询： HDFS读取： 写入这里有个问题，就是我们之前说的多数据备份问题。这里不能交给Client做，那不得累死服务员，由同学们自己安排就好了。也就是采用目前比较流行的binlog方法。mysql读写分离也采用这个方法，mysql转数据仓库也用binlog。DataNode之间自己同步，效率有很大提升。服务员只专心上菜。 中间难免的小插曲，比如说赵栋出去接电话，他这一个NameNode不在了（Down机），服务员正好来上菜。怎么办？所以只能准备多个NameNode，也就是NameNode集群，选举Master进行更替。如果只有一个赵栋的话，出现不在场行为，这酒席进行不下去了。必备的NameNode是必要的。NameNode中存储了整个系统的元数据metadata，是指描述数据的数据，这里指描述文件的数据，比如文件路径，文件被分为几块？每个块在哪些DataNode上等。 假如所有NameNode都要重启怎么办？防止这种情况，可以持久化到硬盘上啊，重启之后再从硬盘把数据恢复到内存不就行了？那要把所有数据都先写一遍硬盘、再重启后读一遍硬盘、每个metadata都要访问一遍….想想效率问题。HDFS采用WAl思想，所有的写入行为都被追踪到了日志中，一旦系统重启，恢复这个wal日志行为到重启前的样子就可以。 那WAL这个日志行为你想想跑了几个月有多大，一旦重启…还要回溯到几个月前的行为再来一遍写入吗？不现实。所以hdfs的设计者也想到了这个问题。你看看之前hdfs的架构图，里面有一个SecondNameNode，就是用来解决这个问题。 刚刚其实只说了一半，NameNode确实会回放editlog，但是不是每次都从头回放，它会先加载一个fsimage，这个文件是之前某一个时刻整个NameNode的文件元数据的内存快照，然后再在这个基础上回放editlog，完成后，会清空editlog，再把当前文件元数据的内存状态写入fsimage，方便下一次加载。一句话，简单来说全量导入变成了增量导入。 WAL只是解决了不全部回追数据的问题，NameNode还是很大的问题没有解决。SecondNameNode出场，他会定期将WAL日志存入fsimage中，然后清空WAL日志，就相当于跑个定时。刚才说的，主NameNode挂了，集群中另一个NameNode顶上，可不是这个SecondNameNode啊，完全两回事。 破壁4—相爱相杀的Master和Activehadoop2.x之前，整个集群只能有一个NameNode，是有可能发生单点故障的，所以hadoop1.x有本身的不稳定性。但是hadoop2.x之后，我们可以在集群中配置多个NameNode，就不会有这个问题了，但是配置多个NameNode，需要注意的地方就更多了，系统就更加复杂了。Hadoop1.0也想到了系统的稳定性，但这和系统的设计理念是有关系的，虽然hadoop1.x存在一个NameNode单点，但是它大大简化了系统的复杂度，并且数据量在一定范围内时，NameNode并没有这么容易挂，所以那个时代是被接受的。但是随着数据量越来越大，这个单点始终是个隐患，所以设计者不得不升级为更加复杂的hadoop2.x，来保证NameNode的高可靠。俗话说一山不容二虎，两个NameNode只能有一个是活跃状态active，另一个是备份状态standby，我们看一下两个NameNode的架构图。 变的复杂了吧，多看几遍就会了。active的NameNode挂了之后，standby的NameNode要马上接替它，所以它们的数据要时刻保持一致，在写入数据的时候，两个NameNode内存中都要记录数据的元信息，并保持一致。这个JournalNode就是用来在两个NameNode中同步数据的，并且standby NameNode实现了SecondNameNode的功能。注意，hadoop2.x如果只部署一个NameNode，还是会用SecondNameNode。 总结HDFS优缺点：hdfs可以存储海量数据，并且是高可用的，任何一台机器挂了都有备份，不会影响整个系统的使用，也不会造成数据丢失。HDFS不适合存大批量的小文件，每一个小文件都有元信息，它们都存在NameNode里面，可能造成NameNode的内存不足。HDFS设计之初就是为了存储，不实用与编辑。所以，控制好你的业务方向…随机读写的效率很低。 1、hdfs是一个分布式文件系统，简单理解就是多台机器组成的一个文件系统。 2、hdfs中有3个重要的模块，client对外提供统一操作接口，DataNode真正存储数据，NameNode协调和管理数据，是一个典型的master-slave架构。 3、hdfs会对大文件进行切块，并且每个切块会存储备份，保证数据的高可用，适合存储大数据。 4、NameNode通过fsimage和editlog来实现数据恢复和高可用。 5、hdfs不适用于大量小文件存储，不支持并发写入，不支持文件随机修改，查询效率大概在秒级。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次生动的TCP3次握手和4次挥手]]></title>
    <url>%2F2019%2F04%2F26%2F%E7%94%9F%E5%8A%A8%E7%9A%84%E4%B8%80%E6%AC%A1TCP3%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C4%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[前言TCP握手挥手过程很简单，并不复杂。问题是记了忘，忘了记。以此循环，地老天荒。本渣还是决定用简单的描述手码一遍握手挥手的过程。师弟师妹们也可以清楚一下零散的知识点。 TCP三次握手TCP三次握手就好比两个人在街上隔着一条大街互相瞅了一眼（别担心，这就是发生在东北），但是因为天儿不好，这俩人不能确定是否是心里的那个他，没法直接交手。只能尴尬的先招手确定下对方是否认识自己。重点来了！！！刘能首先向赵四招手（syn），赵四看到刘能向自己招手，嘴角撇了撇。刘能看到赵四嘴角撇了，确认了赵四成功辨认出了自己(进入estalished状态)。但是赵四还有点狐疑，向四周看了一看，有没有可能刘能是在看别人呢，他也需要确认一下。所以赵四也向刘能招了招手(syn)，刘能看到赵四向自己招手后知道对方是在寻求自己的确认，于是也学着嘴角撇了撇(ack)，赵四看到对方的撇嘴后确认了刘能就是在向自己打招呼(进入established状态)。于是俩人加快步伐，走到了一起，相互拥抱，互喊“亲家！！！”。我们看到这个过程中一共是四个动作，刘能招手–赵四撇嘴–赵四招手–刘能撇嘴。其中赵四连续进行了2个动作，先是撇嘴(回复对方)，然后再次招手(寻求确认)，实际上可以将这两个动作合一，招手的同时撇嘴(syn+ack)。于是四个动作就简化成了三个动作，刘能招手–赵四撇嘴并招手–刘能撇嘴。这就是三次握手的本质，中间的一次动作是两个动作的合并。我们看到有两个中间状态，synsent和synrcvd，这两个状态叫着「半打开」状态，就是向对方招手了，但是还没来得及看到对方的撇嘴。synsent是主动打开方的「半打开」状态，synrcvd是被动打开方的「半打开」状态。客户端是主动打开方，服务器是被动打开方。 syn_sent: syn package has been sentsyn_rcvd: syn package has been received TCP传输TCP 数据传输就是两个人隔空对话，差了一点距离，所以需要对方反复确认听见了自己的话。刘能喊了一句话(data)，赵四听见了之后要向刘能回复自己听见了(ack)。如果刘能喊了一句，半天没听到赵四回复，刘能就认为自己的话被大风吹走了，赵四没听见，所以需要重新喊话，这就是tcp重传。也有可能是赵四听到了刘能的话，但是赵四向刘能的回复被大风吹走了，以至于刘能没听见赵四的回复。刘能并不能判断究竟是自己的话被大风吹走了还是赵四的回复被大风吹走了，刘能也不用管，重传一下就是。 既然会重传，赵四就有可能同一句话听见了两次，这就是「去重」。「重传」和「去重」工作操作系统的网络内核模块都已经帮我们处理好了，用户层是不用关心的。 刘能可以向赵四喊话，同样赵四也可以向刘能喊话，因为tcp链接是「双工的」，双方都可以主动发起数据传输。不过无论是哪方喊话，都需要收到对方的确认才能认为对方收到了自己的喊话。 刘能虽然磕巴但话多，一说连说了八句话，赵四说话也费劲，这时候赵四可以不用一句一句回复，而是连续听了这八句话之后，一起向对方回复说前面你说的八句话我都听见了，这就是批量ack。但是刘能也不能一次性说了太多话，赵四的脑子短时间可能无法消化太多，两人之间需要有协商好的合适的发送和接受速率，这个就是「TCP窗口大小」。 网络环境的数据交互同人类之间的对话还要复杂一些，它存在数据包乱序的现象。同一个来源发出来的不同数据包在「网际路由」上可能会走过不同的路径，最终达到同一个地方时，顺序就不一样了。操作系统的网络内核模块会负责对数据包进行排序，到用户层时顺序就已经完全一致了。 TCP 四次挥手TCP断开链接的过程和建立链接的过程比较类似，只不过中间的两部并不总是会合成一步走，所以它分成了4个动作，刘能挥手(fin)——赵四伤感地撇嘴(ack)——赵四挥手(fin)——刘能伤感地撇嘴(ack)。 之所以中间的两个动作没有合并，是因为tcp存在「半关闭」状态，也就是单向关闭。刘能已经挥了手，可是人还没有走，只是不再说话，但是耳朵还是可以继续听，赵四呢继续喊话。等待赵四累了，也不再说话了，超刘能挥了挥手，刘能伤感地撇嘴了一下，才彻底结束了。上面有一个非常特殊的状态time_wait，它是主动关闭的一方在回复完对方的挥手后进入的一个长期状态，这个状态标准的持续时间是4分钟，4分钟后才会进入到closed状态，释放套接字资源。不过在具体实现上这个时间是可以调整的。 它就好比主动分手方要承担的责任，是你提出的要分手，你得付出代价。这个后果就是持续4分钟的time_wait状态，不能释放套接字资源(端口)，就好比守寡期，这段时间内套接字资源(端口)不得回收利用。 它的作用是重传最后一个ack报文，确保对方可以收到。因为如果对方没有收到ack的话，会重传fin报文，处于time_wait状态的套接字会立即向对方重发ack报文。 同时在这段时间内，该链接在对话期间于网际路由上产生的残留报文(因为路径过于崎岖，数据报文走的时间太长，重传的报文都收到了，原始报文还在路上)传过来时，都会被立即丢弃掉。4分钟的时间足以使得这些残留报文彻底消逝。不然当新的端口被重复利用时，这些残留报文可能会干扰新的链接。 4分钟就是2个MSL，每个MSL是2分钟。MSL就是maximium segment lifetime——最长报文寿命。这个时间是由官方RFC协议规定的。至于为什么是2个MSL而不是1个MSL，我还没有看到一个非常满意的解释。四次挥手也并不总是四次挥手，中间的两个动作有时候是可以合并一起进行的，这个时候就成了三次挥手，主动关闭方就会从finwait1状态直接进入到timewait状态，跳过了finwait_2状态。 总结TCP状态转换是一个非常复杂的过程，本文仅对一些简单的基础知识点进行了类比讲解。关于TCP的更多知识还需要读者去搜寻相关技术文章进入深入学习。如果读者对TCP的基础知识掌握得比较牢固，高级的知识理解起来就不会太过于吃力。]]></content>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[破解数据库连接工具navicat（亲测可用）]]></title>
    <url>%2F2019%2F04%2F25%2F%E7%A0%B4%E8%A7%A3%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E5%B7%A5%E5%85%B7navicat%EF%BC%88%E4%BA%B2%E6%B5%8B%E5%8F%AF%E7%94%A8%EF%BC%89%2F</url>
    <content type="text"><![CDATA[破解步骤下载1.最好下载Navicat Premium 12版本，至于mac 还是 win 自行选择。另外选择破解版本也要看下网上的介绍。本人用12版本已经够用了。生成秘钥对2.生成自己的RSA公钥私钥对 （这里直接使用大神的密钥，也可以自己生成测试可以用） 这步与windows版破解相同，可以用open ssl工具生成，也可以使用其他工具生成，注意密钥是2048位的，PKCS#8格式为了节省时间，可以使用我提供的一对密钥。公钥： —–BEGIN PUBLIC KEY—–MIIBITANBgkqhkiG9w0BAQEFAAOCAQ4AMIIBCQKCAQB8vXG0ImYhLHvHhpi5FS3gd2QhxSQiU6dQ04F1OHB0yRRQ3NXF5py2NNDw962i4WP1zpUOHh94/mg/KA8KHNJXHtQVLXMRms+chomsQCwkDi2jbgUa4jRFN/6N3QejJ42jHasY3MJfALcnHCY3KDEFh0N89FV4yGLyDLr+TLqpRecg9pkPnOp++UTSsxz/e0ONlPYrra/DiaBjsleAESZSI69sPD9xZRt+EciXVQfybI/2SYeAdXMm1B7tHCcFlOxeUgqYV03VEqiC0jVMwRCd+03NU3wvEmLBvGOmNGudocWIF/y3VOqyW1byXFLeZxl7s+Y/SthxOYXzu3mF+2/pAgMBAAE=—–END PUBLIC KEY—– 私钥： —–BEGIN RSA PRIVATE KEY—–MIIEogIBAAKCAQB8vXG0ImYhLHvHhpi5FS3gd2QhxSQiU6dQ04F1OHB0yRRQ3NXF5py2NNDw962i4WP1zpUOHh94/mg/KA8KHNJXHtQVLXMRms+chomsQCwkDi2jbgUa4jRFN/6N3QejJ42jHasY3MJfALcnHCY3KDEFh0N89FV4yGLyDLr+TLqpRecg9pkPnOp++UTSsxz/e0ONlPYrra/DiaBjsleAESZSI69sPD9xZRt+EciXVQfybI/2SYeAdXMm1B7tHCcFlOxeUgqYV03VEqiC0jVMwRCd+03NU3wvEmLBvGOmNGudocWIF/y3VOqyW1byXFLeZxl7s+Y/SthxOYXzu3mF+2/pAgMBAAECggEAK5qZbYt8wenn1uZg6onRwJ5bfUaJjApL+YAFx/ETtm83z9ByVbx4WWT7CNC7fK1nINy20/mJrOTZkgIxx6otiNC4+DIsACJqol+RLoo8I9pk77Ucybn65ZteOz7hVZIU+8j6LzW0KDt6yowXe75r7G/NEpfibNc3Zz81+oDd2x+bHyGbzc9QcePIVuEzkof6jgpbWrQZU14itx9lVxEgj/fbMccvBx8brR/l9ClmDZd9Y6TWsF1rfJpF3+DPeqFkKCiD7PGz3bs4O/ZdZrfV21ZNVusBW49G6bU63gQVKsOf1qGo3efbAW1HVxgTQ/lExVdcMvdenZm+ADKpL4/wUQKBgQDOfBjn3OC2IerUFu18EgCS7pSjTSibXw+TeX3D5zwszLC091G2rGlT5DihBUhMfesNdpoZynrs4YB6Sz9C3wSGAB8AM/tNvPhtSVtbMHmrdT2DEEKCvLkORNBnt+8aTu2hGRanw9aL1189gzwrmXK5ZuuURfgLrB9ihrvjo4VznQKBgQCapx13dEA1MwapBiIa3k8hVBCoGPsEPWqM33RBdUqUsP33f9/PCx00j/akwmjgQNnBlAJoY7LOqPCyiwOkEf40T4IlHdzYntWQQvHhfBwqSgdkTE9tKj43Ddr7JVFRL6yMSbW39qAp5UX/+VzOLGAlfzJ8CBnkXwGrnKPCVbnZvQKBgQCd+iof80jlcCu3GteVrjxMLkcAbb8cqG1FWpVTNe4/JFgqDHKzPVPUgG6nG2CGTWxxv4UFKHpGE/11E28SHYjbcOpHAH5LqsGy84X2za649JkcVmtclUFMXm/Ietxvl2WNdKF1t4rFMQFIEckOXnd8y/Z/Wcz+OTFF82l7L5ehrQKBgFXl9m7v6e3ijpN5LZ5A1jDL0Yicf2fmePUP9DGbZTZbbGR46SXFpY4ZXEQ9GyVbv9dOT1wN7DXvDeoNXpNVzxzdAIt/H7hN2I8NL+4vEjHG9n4WCJO4v9+yWWvfWWA/m5Y8JqusV1+N0iiQJ6T4btrE4JSVp1P6FSJtmWOKW/T9AoGAcMhPMCL+N+AvWcYt4Y4mhelvDG8e/Jj4U+lwS3g7YmuQuYx7h5tjrS33w4o20g/3XudPMJHhA3z+d8b3GaVM3ZtcRM3+Rvk+zSOcGSwn3yDy4NYlv9bdUj/4H+aU1Qu1ZYojFM1Gmbe4HeYDOzRsJ5BhNrrV12h27JWkiRJ4F/Q= —–END RSA PRIVATE KEY—–安装程序3 安装程序，并替换应用包内容目录中rpk文件的公钥 安装完毕后打开finder，找到应用程序，右键显示包内容，打开目录 /Contents/Resources，编辑rpk文件，将公钥替换并保存。算出有效的Mac版序列号密钥4 使用我算好的密钥可以跳过此步，继续第四步，节省时间。中文版64位密钥序列号： NAVH-T4PX-WT8W-QBL5 英文版64位密钥序列号： NAVG-UJ8Z-EVAP-JAUW解密请求码，生成激活码打开应用，断网！！！，点击注册，输入密钥 NAVH-T4PX-WT8W-QBL5，然后手动激活复制请求码，使用私钥解密请求码，得到激活码明文注意必须自己解密，因为解密后得到的“DI”是不同的激活码明文示例：{ “K” : “NAVHT4PXWT8WQBL5”, “P” : “Mac 10.13”, “DI” : “ODQ2Yjg2ZDBjMTEzMjhh”}在线RSA私钥解密：http://tool.chacuo.net/cryptrsaprikey 将得到的激活码明文进行修改，修改后格式如下{“K”:”NAVHT4PXWT8WQBL5”, “N”:”52pojie”, “O”:”52pojie.cn”, “DI”:”ODQ2Yjg2ZDBjMTEzMjhh”, “T”:1516939200}激活码明文格式最好复制我的，改变 “ “ 内的字符即可，在同一行哦，不要换行，否则激活失败！！！“K” “DI” 都替换成自己机器解密的信息，”N” “O” “T”自己定义 加密激活码明文，使用私钥加密激活码明文在线RSA私钥加密：http://tool.chacuo.net/cryptrsaprikey 复制加密后激活码信息到程序激活窗口，点击激活，即可成功激活完整教程贴图地址https://blog.csdn.net/xhd731568849/article/details/79751188]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rest微服务构建案例]]></title>
    <url>%2F2019%2F04%2F24%2FRest%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%84%E5%BB%BA%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[记住一句话：约定 &gt; 配置 &gt; 编码Rest微服务一个基础构建是整个微服务的一个基础，先说下总体介绍。 有以下几个部分： 1.mvc、mybatis（一带而过了） 2.maven（坐标、仓库、依赖、聚合、继承）。理论要熟练下。 maven的命令：maven -U clean package install 这是我常用的，也是在更新了包所必备去更新到本地的命令。其他工程才能依赖最新的jar包。 父工程包含很多子工程，子工程公用的依赖包可以提到父工程中。 撸码provider （一）构建父工程，分布式开发所必备的建工程流程。构建父工程要点就是maven 的打包packaging要为pom。goupId和artifactId都要起好，否则很容易混淆。 构建完pom父工程后，新构建的pom中有几大几个部分—- 1.头部(重要！)Other-&gt; Maven Moudle。goupId和artifactId都要起好还是一样的要起好，packaging这时侯 变了！变成了jar包。在构建框里面的置灰部分，你也会看见parent的工程也就是上方所说的父工程。点击确定！ 创建完毕后，返回头去父工程的pom文件中查看下，发现新增moudleName 构建实体类的样子记得最好用工具lombok （1）@AllArgsConstructor @NoArgsConstructor @Data @Accessors(chain = true) public class entity { private Long id; private String msg; private String DBSource; } （2）在yml中配置我们所需要的配置信息，例：mybatis、spring日志config、重要的是spring-application-name！！！！暴露出去的微服务名称。 （3）创建数据库genetator，自动生成mapper、entity、dao。 （4）给数据库造点假数据。对了，这里数据库的库和表要自己建好，否则（3）进行不下去。这一步省去了你很多时间，真正提高工作效率。如何运用，请看管理系统标签 （5）整合service层 （6）整合controller层 都在各层记得打@啊！！！ 添加主启动类：SpringApplication.run(所属类,args);启动！done consumer 构建consumer过程和上方大部分一致，直到（5）部，不再调用service层进行数据库操作了。 这里！！！！在controller中加入@Autowired RestTemplate restTemplate; 操作— restTemplate.getForObject();//传参3个参数，请求服务的URL（provider的controller地址），传递参数（provider api 对应参数），返回类型（可选boollean、有很多） restTemplate.postForObject();//同上 添加主启动类：SpringApplication.run(所属类,args);启动！done。实验下，发现consumer的调用直接调到了provider。 Eureka 假如我们要引进clould一个新组件，将provider引入Eureka，基本上有两步 1.新增Eureka依赖 2.@EnableEurekaServer 3.zuul 4.业务逻辑 5.注册中心/info/自定义ip/修改显示节点名称 6.Eureka的自我保护，在server主页面上出现红色字体，注意这个不是报错。是server的一种监听机制。 这里我们做个测试，我们频繁操作下provider配置文件下面的节点名称（instance id），会在server发现出现了上方的红色字体。自我保护机制：一句话， 好死不如赖活着。server 会保留了我们抖动的节点名称。某一时刻某个微服务不可用了，Eureka它不会立刻清理，依旧对微服务的信息进行保存。 应对网络异常的一种措施。 总结本期微服务的构建案例，总结其精髓就是rest api！同时也是和dubbo不同的所在。剩下搞过mvc开发的，熟悉其开发流程的同学来说，只是增添了一些小应用有没有，当然这些都是构建在SpringBoot的基础上搞的，SpringBoot这个框架，本人是真的喜欢，但是喜欢的同时千万要对其自动配置、起步依赖、Actuator、命令行界面要有深入理解。]]></content>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka的生产+消费]]></title>
    <url>%2F2019%2F04%2F23%2Fkafka%E7%9A%84%E7%94%9F%E4%BA%A7-%E6%B6%88%E8%B4%B9%2F</url>
    <content type="text"><![CDATA[消息队列的内容很丰富，目前我常用的是kafka，所以第一篇献给我的kafka，后续我们继续钻研下，RocketMQ、ActiveMQ。kafka的原生写法，不繁琐，直接看见配置项就👌，如果你是刚入门大数据或者是刚接手kafka的话难免有蒙圈的感觉，But！！！这不要紧，先蒙在其中，后续你接触多了大数据的东西，就会发现，很多组件用了很多的配置项，加载进配置项就可以用了。当然我说的还是比较原生的写法，刚刚写完了一遍HBase的原生，一时脱离不出底层的范围哈哈哈。 心灵鸡汤一下：刚入职的小白遇到不会的问题，甚者被批评，千万不要气馁，觉得自己不适合干这个。行业没有不适合的，只有干的开不开心。坚持一下没准你就有小骄傲了呢？废话少说，开始撸码生产者public class KafkaProvider{ private final Producer&lt;String, String&gt; producer; public final static String TOPIC = &quot;TEST-TOPIC&quot;; public KafkaProvider(){ Properties props = new Properties();//配置项！这行代码是kafka的源头。 props.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);//zookeeper地址，集群用逗号分隔 //这里如果不熟悉的同学，可以看看kafa的架构图。每一个 ![](kafka架构.jpg) props.put(&quot;acks&quot;, &quot;all&quot;);//记录完整提交，最慢但是最大可能的持久化 props.put(&quot;retries&quot;, 3);//请求失败的重试次数 props.put(&quot;batch.size&quot;, 16384);//batch大小 props.put(&quot;linger.ms&quot;, 1);// 默认情况即使缓冲区有剩余的空间，也会立即发送请求，设置一段时间用来等待从而将缓冲区填的更多，单位为毫秒，producer发送数据会延迟1ms，可以减少发送到kafka服务器的请求数据 props.put(&quot;buffer.memory&quot;, 33554432);// 提供给生产者缓冲内存总量 props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);//写话方式 props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //生成生产者 producer = new KafkaProducer&lt;String, String&gt;(props); } //在入口函数调用即可，具体应用情形根据项目实际来看 public void producer(){ int messageNo = 1; final int COUNT =1000; while (1){ String num = String.valueOf(messageNo); String data = &quot;hello kafka message:&quot; + num; ProducerRecord record = new ProducerRecord(TOPIC,data); producer.send(record); messageNo ++ ; System.out.println(messageNo); try { sleep(1000); } catch ( InterruptedException e ) { e.printStackTrace(); } } } }消费者public class KafkaConsumer { private final Consumer consumer ; public final static String TOPIC = “TEST-TOPIC”; private ExecutorService executors; public KafkaCon(){ Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);//zk同生产者一致 props.put(&quot;group.id&quot;, &quot;2&quot;);//分组Id！！！！ props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);//自动提交，这是对offset的操作，有些需要对offset更加精准的处理，需要进行手动提交。 props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); consumer = new KafkaConsumer&lt;String, String&gt;(props); consumer.subscribe(Arrays.asList(TOPIC)); execute(10); } public void execute(int workerNum) { executors = new ThreadPoolExecutor(workerNum, workerNum, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue(1000), new ThreadPoolExecutor.CallerRunsPolicy()); Thread t = new Thread(new Runnable(){//启动一个子线程来监听kafka消息 @Override public void run(){ while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(200); for (final ConsumerRecord record : records) { System.out.println(&quot;【Kafka】监听到kafka的TOPIC【&quot; + record.topic() + &quot;】的消息&quot;); System.out.println(&quot;【Kafka】消息内容：&quot; + record.value()); executors.submit(new ConsumerWorker(record)); } } }}); t.start(); } } 这是标准的一个kafka配置信息，生产实战中的应用很简单，千万记住几个关键词：produce、consumer、topic、group.id、partition、broker，schema的含义，主要还是理解吧。提几个问题哈？如果你想多个消费者消费一个topic怎么办？partion中的消息是顺序的，多个partion间的消息呢？]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>消息队列，kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 代码请求URl样例]]></title>
    <url>%2F2019%2F04%2F23%2Fjava-%E4%BB%A3%E7%A0%81%E8%AF%B7%E6%B1%82URl%E6%A0%B7%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[习惯了在开发中运用工具（postman等）调试接口，几乎忘了用代码可以更好的处理实际情况。 Demo样例 /** * 发送post请求 * @param url 路径 * @param jsonObject 参数(json类型) * @param encoding 编码格式 * @return * @throws ParseException * @throws IOException */ public static String send(String url, JSONObject jsonObject,String encoding) throws ParseException, IOException{ String body = &quot;&quot;; //创建httpclient对象 CloseableHttpClient client = HttpClients.createDefault(); //创建post方式请求对象 HttpPost httpPost = new HttpPost(url); //装填参数 StringEntity s = new StringEntity(jsonObject.toString(), &quot;utf-8&quot;); //参数体有按要求也要进行Content-type赋值 s.setContentType(&quot;application/json&quot;); s.setContentEncoding(new BasicHeader(HTTP.CONTENT_TYPE, &quot;application/json&quot;)); //设置参数到请求对象中 httpPost.setEntity(s); System.out.println(&quot;请求地址：&quot;+url); // System.out.println(“请求参数：”+nvps.toString()); //设置header信息 //指定报文头【Content-type】、【User-Agent】选择其一就ok httpPost.setHeader(&quot;Content-type&quot;, &quot;application/x-www-form-urlencoded&quot;); httpPost.setHeader(&quot;Content-type&quot;, &quot;application/json&quot;); httpPost.setHeader(&quot;User-Agent&quot;, &quot;Mozilla/4.0 (compatible; MSIE 5.0; Windows NT; DigExt)&quot;); //执行请求操作，并拿到结果（同步阻塞） CloseableHttpResponse response = client.execute(httpPost); //获取结果实体 HttpEntity entity = response.getEntity(); if (entity != null) { //按指定编码转换结果实体为String类型 body = EntityUtils.toString(entity, encoding); } EntityUtils.consume(entity); //释放链接 response.close(); return body; }]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot深入剖析，够用了。]]></title>
    <url>%2F2019%2F04%2F21%2FSpringBoot%E6%94%B9%E9%80%A0%2F</url>
    <content type="text"><![CDATA[刚入职的时候，发现我们接手项目是ssm和SpringBoot混用，这个”混用”听起来奇怪哈？一句话就是很多依赖项用了SpringBoot又用了原生，还用了ssm中的一些写法。直观看上去一句my god～～～对新入职的应届生来说，多少会有混淆，用起来也会很撇脚（碍手？）。但本菜狗认为这真是一个学习的好机会，从最原生的写一遍，写到SpringBoot，写到地老天荒。另外悄悄告诉你，我们的之前组件都是原生的哈哈哈哈，根本没有Compent，你知道怎么实现的吗？ 以上都是废话 干货 我介绍下SpringBoot的原理，也偷偷抓紧复习下。—2019年4月21日 构建耽误了几天哈—2019年4月29日本文不东扯西扯、只专注剖析SpringBoot。废话不多说，撸码！ 这是一个已经建好的启动类，我用的编译器是IDEA，我也推荐各位用上这款软件，我只专注java开发哈，至于kotlin，Lua….我是没有用这个软件玩过。至于这么到这一步，麻烦看下标签中题目：构建微服务实战的介绍，很简单。简单说两句吧，我用IDEA构建项目一般都是用在线化构建，很方便，可以选择很多starter。不过新手我建议这么做， 还是老老实实用在线化下方的maven模块去构建。具体什么原因，请看完本篇再翻过头想想。 自己造一个启动类在我们建好的工程中，添加一个启动函数，这个应该会吧？注意在入口函数上打上@SpringBootApplication，完整入口函数如下。123456@SpringBootApplicationpublic class Main &#123; public static void main(String []args) &#123; SpringApplication.run(Main.class, args); &#125;&#125; 启动！没问题。 世界从@SpringBootApplication开始。我刚让大家在启动类打上的@SpringBootApplication，为什么被称为世界的入口。来进入这个@看看。这里面讲解有点长啊，我建议大家坚持下，一口气读完，断断续续的读效果不好。我开始断断续续的看源码，效果真的差。 这里面有三个重要标签@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan。即 @SpringBootApplication = (默认属性)@Configuration + @EnableAutoConfiguration + @ComponentScan。 @SpringBootConfiguratio拿@SpringBootConfiguration来说，这里面其实就是要一个@Configuration的注解，来定义了这个Class为配置类。再引申来说@Configuration的作用，你暂且把它看成要往Spring注入实例的一个大门。 官方来说，javaConfig形式的ioc容器的配置类使用的都是@Configuration的注解。SpringBoot社区推荐javaConfig的配置形式。过去常写mvc的同学对javaConfig有点陌生哈，其实简单一句话就是把过去的xml的各种配置类，放进了代码中。有人就很反感这种，认为配置就是配置，代码就是代码啊，把配置融进代码反而复杂了感觉，我开始也这么认为。后来，我的表现….真香！ 举几个简单例子回顾下，XML跟config配置方式的区别： （1）表达形式层面： 基于XML配置的方式是这样：1234567&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd" default-lazy-init="true"&gt; &lt;!--bean定义--&gt;&lt;/beans&gt; 而基于JavaConfig的配置方式是这样： 1234@Configurationpublic class Configuration&#123; //bean定义&#125; （2）注册bean定义层面 基于XML的配置形式是这样：123&lt;bean id="DubboService" class="..DubboServiceImpl"&gt; ...&lt;/bean&gt; 而基于JavaConfig的配置形式是这样的：1234567@Configurationpublic class DubboConfiguration&#123; @Bean public DubboService dubboService()&#123; return new DubboServiceImpl(); &#125;&#125; 任何一个标注了@Bean的方法，其返回值将作为一个bean定义注册到Spring的IoC容器，方法名将默认成该bean定义的id。也可以自己取名字，起名字按规范一般也都是方法名。 1@Bean(name = "dubboService") （3）表达依赖注入关系层面 为了表达bean与bean之间的依赖关系，在XML形式中一般是这样： 1234&lt;bean id="DubboService" class="..DubboServiceImpl"&gt; &lt;propery name ="dependencyService" ref="dependencyService" /&gt;&lt;/bean&gt;&lt;bean id="dependencyService" class="DependencyServiceImpl"&gt;&lt;/bean&gt; 而基于JavaConfig的配置形式是这样的：123456789101112@Configurationpublic class Configuration&#123; @Bean public DubboService dubboService()&#123; return new DubboServiceImpl(dependencyService()); &#125; @Bean public DependencyService dependencyService()&#123; return new DependencyServiceImpl(); &#125;&#125; 如果一个bean的定义依赖其他bean，则直接调用对应的JavaConfig类中依赖bean的创建方法就可以了。Spring注入实例大门@Configuration，现在请记住他的搭档--@Bean，他们是一对的。我们来看一个完整的案例。 .xml的123456&lt;beans&gt; &lt;bean id = "car" class="com.test.Car"&gt; &lt;property name="wheel" ref = "wheel"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id = "wheel" class="com.test.Wheel"&gt;&lt;/bean&gt; &lt;/beans&gt; 等同于1234567891011121314@Configuration public class Configuration &#123; @Bean public Car car() &#123; Car car = new Car(); car.setWheel(wheel()); return car; &#125; @Bean public Wheel wheel() &#123; return new Wheel(); &#125; &#125; @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。@Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan这个注解在xml中也有同样体现，@ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义]、@Controller、@Service最终将这些bean定义加载到IoC容器中。1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans"&gt; &lt;bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;&lt;/value&gt; ... &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package="url" /&gt; &lt;import resource="url"/&lt;/beans&gt; 如果不指定某一个扫描路径，则默认Spring框架实现会从声明@ComponentScan所在类的package进行扫描。所以这里面有个知识点，main启动类要在最根的目录上，会有组件扫描不进来的现象。 @EnableAutoConfiguration为什么这个@EnableAutoConfiguration我要放在最后说，这个是SpringBoot牛逼的体现，各位有没有之前在Spring上开启某些组件的，@Enable**等，@EnableScheduling是通过@Import将Spring调度框架相关的bean定义都加载到IoC容器。@EnableMBeanExport是通过@Import将JMX相关的bean定义加载到IoC容器。而@EnableAutoConfiguration也是借助@Import的帮助，将所有符合自动配置条件的bean定义加载到IoC容器，其实核心意思是一个。我们进入@EnableAutoConfiguration这个注解中看看。 由于import的支持，SpringBoot得以收集了各类场景的组件的支持。@Import(AutoConfigurationImportSelector.class)，在这个import的类AutoConfigurationImportSelector.class中，可以帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器。就像一只“八爪鱼”一样，借助于Spring框架原有的一个工具类：SpringFactoriesLoader的支持，@EnableAutoConfiguration可以智能的自动配置功效才得以大功告成！ 来看看这个Boss—SpringFactoriesLoader藏的并不深。123456789protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, "No auto configuration classes found in META-INF/spring.factories. If you " + "are using a custom packaging, make sure that file is correct."); return configurations;&#125; 其中 SpringFactoriesLoader.loadFactoryNames 是关键。再进入一层看看。 12345678910111213141516171819202122232425262728293031323334353637383940public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) &#123; String factoryClassName = factoryClass.getName(); return (List)loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());&#125;private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; MultiValueMap&lt;String, String&gt; result = (MultiValueMap)cache.get(classLoader); if (result != null) &#123; return result; &#125; else &#123; try &#123; Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources("META-INF/spring.factories") : ClassLoader.getSystemResources("META-INF/spring.factories"); LinkedMultiValueMap result = new LinkedMultiValueMap(); while(urls.hasMoreElements()) &#123; URL url = (URL)urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); Iterator var6 = properties.entrySet().iterator(); while(var6.hasNext()) &#123; Entry&lt;?, ?&gt; entry = (Entry)var6.next(); String factoryClassName = ((String)entry.getKey()).trim(); String[] var9 = StringUtils.commaDelimitedListToStringArray((String)entry.getValue()); int var10 = var9.length; for(int var11 = 0; var11 &lt; var10; ++var11) &#123; String factoryName = var9[var11]; result.add(factoryClassName, factoryName.trim()); &#125; &#125; &#125; cache.put(classLoader, result); return result; &#125; catch (IOException var13) &#123; throw new IllegalArgumentException("Unable to load factories from location [META-INF/spring.factories]", var13); &#125; &#125;&#125; 稳住别慌，到关键了！！！loadFactoryNames方法中的loadSpringFactories执行了这么一段Enumeration urls = classLoader != null ? classLoader.getResources(“META-INF/spring.factories”) : ClassLoader.getSystemResources(“META-INF/spring.factories”); classLoader.getResources(“META-INF/spring.factories”)看到没，这个META-INF文件下的spring.factories内容中获取我们准备加载的class的URL，这个META-INF就是再那spring.factories有什么呢？来睁大眼睛看！神不神奇，所以，@EnableAutoConfiguration自动配置的魔法骑士就变成了：从classpath中搜寻所有的META-INF/spring.factories配置文件，并将其中org.springframework.boot.autoconfigure.EnableutoConfiguration对应的配置项通过反射（JavaRefletion）实例化为对应的标注了@Configuration的JavaConfig形式的IoC容器配置类，然后汇总为一个并加载到IoC容器。从AutoConfiguration往下看，会看到很多自动配置的很多信息。有我们经常使用的种种比如这个 1org.springframework.boot.autoconfigure.quartz.QuartzAutoConfiguration,\ 这是一个定时任务的自动配置项，我们进入这个类里看看从@EnableConfigurationProperties(QuartzProperties.class)再进入QuartzProperties.class类中，发现注解项@ConfigurationProperties(“spring.quartz”)，这是什么啊？！这不就是，你在yml配置文件写入Spring.quartz所属的配置项了么，立图为证。 所以一个EnableAutoConfiguration帮我们解决了很多事情，不用再引入一对properties了，只需要在yml（有的使用properties，我推荐使用yml）配置文件中，加上自己想要的配置参数就可以了。 跟我来一遍SpringBoot的跑的流程run()方法—执行说完我们启动类的三大注解我们再次回到我们的启动类哈哈哈！ 123456789101112/** * @program: microservicecloudprovider * @description: * @author: Nou * @create: 2019-04-29 18:48 **/@SpringBootApplicationpublic class Main &#123; public static void main(String []args) &#123; SpringApplication.run(Main.class, args); &#125;&#125; 在SpringApplcation执行的run方法中，执行一个静态方法 123public static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) &#123; return run(new Class[]&#123;primarySource&#125;, args);&#125; new Class[]{primarySource}, args —new 出了一个Spring的实例，然后直接调用这个实例。在初始化的时候，SpringBoot会提前干这么几件事。 在类路径下（classPath）面找下是否有org.springframework.web.context.ConfigurableWebApplicationContext这个类，这是创建web环境的准备。是否创建一个为web准备的ApplicationContext类。 使用SpringFactoriesLoader在应用的classpath中查找并加载所有可用的ApplicationContextInitializer。 使用SpringFactoriesLoader在应用的classpath中查找并加载所有可用的ApplicationListener。以上两个也就是我刚才圈出来的启动项，联系起来了吧！！！ 推断并设置main方法的定义类。在启动类里面可以有些针对SpringBoot的配置，例如banner还有关闭webContext等。 run()方法—执行SpringApplication实例初始化完成并且完成设置后，就开始执行run方法的逻辑了，首先遍历执行所有通过SpringFactoriesLoader可以查找到并加载的SpringApplicationRunListener。调用它们的started()方法，告诉这些SpringApplicationRunListener，“嘿，SpringBoot应用要开始执行咯！”。 run()方法—加载配置参数创建并配置当前Spring Boot应用将要使用的Environment（包括配置要使用的PropertySource以及Profile），就是我们刚才所说在yml加载所有可用的参数。联系起来了吧。 run()方法—回调通知遍历所有SpringApplicationRunListener的environmentPrepared()，回调通知已经准备好环境 run()方法—遍历 根据用户是否明确设置了applicationContextClass类型以及初始化阶段的推断结果，决定该为当前SpringBoot应用创建什么类型的ApplicationContext并创建完成，然后根据条件决定是否添加ShutdownHook，决定是否使用自定义的BeanNameGenerator，决定是否使用自定义的ResourceLoader，当然，最重要的，将之前准备好的Environment设置给创建好的ApplicationContext使用。 钩子ShutdownHook的作用我这里没有深入研究，总之想要优雅关闭，准备一个钩子是最好的，保证所有线程都执行完毕。钩子也可以直接用我的哈哈。 1234567891011121314151617 Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; try &#123; LOG.info("shutdown start"); Thread.sleep(2000); LOG.info("shutdown end"); &#125; catch (InterruptedException e) &#123; &#125; synchronized (Main.class) &#123; running = false; Main.class.notify(); &#125; &#125;&#125;); ApplicationContext创建好之后，SpringApplication会再次借助Spring-FactoriesLoader，查找并加载classpath中所有可用的ApplicationContext-Initializer，然后遍历调用这些ApplicationContextInitializer的initialize（applicationContext）方法来对已经创建好的ApplicationContext进行进一步的处理。 run()方法—ApplicationContext 遍历调用所有SpringApplicationRunListener的contextPrepared()方法。 把刚才yml中加载的配置参数加入ApplicationContext中。 遍历调用所有SpringApplicationRunListener的contextLoaded()方法。 遍历调用所有SpringApplicationRunListener的contextLoaded()方法。 查找当前ApplicationContext中是否注册有CommandLineRunner，如果有，则遍历执行它们。 正常情况下，遍历执行SpringApplicationRunListener的finished()方法、（如果整个过程出现异常，则依然调用所有SpringApplicationRunListener的finished()方法，只不过这种情况下会将异常信息一并传入处理） run()背后的故事 这是run进入的创建SpringBoot的实例最后的一个构造类 123456789101112131415161718 public SpringApplication(ResourceLoader resourceLoader, Class... primarySources) &#123; this.sources = new LinkedHashSet(); this.bannerMode = Mode.CONSOLE; this.logStartupInfo = true; this.addCommandLineProperties = true; this.addConversionService = true; this.headless = true; this.registerShutdownHook = true; this.additionalProfiles = new HashSet(); this.isCustomEnvironment = false; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, "PrimarySources must not be null"); this.primarySources = new LinkedHashSet(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); this.setInitializers(this.getSpringFactoriesInstances(ApplicationContextInitializer.class)); this.setListeners(this.getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = this.deduceMainApplicationClass();&#125; 看看有没有我们刚才说的listener和ShutdownHook以及nitializers等，想再再深入就去SpringApplication这个类中看看这里面初始化的一些东西。 这些代码我教你怎么截有人问我，你这些原理和源码怎么收集和找到，我说下，我挺痛苦的。一篇博文至少要搞一下午时间，还要截图、撸码、码字、甚至还要早早搞完手头上的需求。所以有些遗漏在所难免，所以教你怎么去看这些东西吧，就是打Debug。每一个关键点，打上Debug，一层一层执行，框架也是代码码出来的，没有什么难的住的。 总结执行流程SpringBoot启动结构，我们发现启动流程主要分为三个部分： 第一部分进行SpringApplication的初始化模块，配置一些基本的环境变量、资源、构造器、监听器；第二部分实现了应用具体的启动方案，包括启动流程的监听模块、加载配置环境模块、及核心的创建上下文环境模块；第三部分是自动化配置模块，该模块作为springboot自动配置核心，在后面的分析中会详细讨论。在下面的启动程序中我们会串联起结构中的主要功能。 关键点都在SpringApplication这个类里面。 我们不如来再跟我走一次图示再看一遍流程，方便大家用撸码结合的来看。 在该构造方法内，我们可以发现其调用了一个初始化的this方法 主要是为SpringApplication对象赋一些初值。构造函数执行完毕后，我们回到run方法 该方法中实现了如下几个关键步骤： 创建了应用的监听器SpringApplicationRunListeners并开始监听 加载SpringBoot配置环境(ConfigurableEnvironment)，如果是通过web容器发布，会加载StandardEnvironment，其最终也是继承了ConfigurableEnvironment。 配置环境(Environment)加入到监听器对象中(SpringApplicationRunListeners) 创建run方法的返回对象：ConfigurableApplicationContext(应用配置上下文)，我们可以看一下创建方法：方法会先获取显式设置的应用上下文(applicationContextClass)，如果不存在，再加载默认的环境配置（通过是否是web environment判断），默认选择AnnotationConfigApplicationContext注解上下文（通过扫描所有注解类来加载bean），最后通过BeanUtils实例化上下文对象，并返回。ConfigurableApplicationContext类图如下：主要看其继承的两个方向：LifeCycle：生命周期类，定义了start启动、stop结束、isRunning是否运行中等生命周期空值方法ApplicationContext：应用上下文类，其主要继承了beanFactory(bean的工厂类) 接下来的refreshContext(context)方法，将是实现Spring-Boot-stater-*的关键。包括spring.factories的加载，bean的实例化等核心工作。 配置结束后，Springboot做了一些基本的收尾工作，返回了应用环境上下文。回顾整体流程，Springboot的启动，主要创建了配置环境(environment)、事件监听(listeners)、应用上下文(applicationContext)，并基于以上条件，在容器中开始实例化我们需要的Bean，至此，通过SpringBoot启动的程序已经构造完成，接下来我们来探讨自动化配置是如何实现。 自动化配置的话，你就按照我开篇的思路思考就可以，完全是你在yml写什么相关配置参数，并且在maven中导入相关的依赖，SpringBoot就会自动给你加进入。有没有很爽。总结一个图，虽然是别人的，我感觉很经典的一个导入图。 为什么这么爽，因为maven依赖的传递性，我们只要依赖starter就可以依赖到所有需要自动配置的类，实现开箱即用的功能。也体现出Springboot简化了Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。所以Spring-boot为我们提供了统一的starter可以直接配置好相关的类，触发自动配置所需的依赖。 结束的寡逼谢谢大家，有问题，下面留言一起探讨。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>dubbo</tag>
        <tag>cloud</tag>
        <tag>kafka</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
</search>
