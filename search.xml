<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HBase基本命令]]></title>
    <url>%2F2019%2F08%2F22%2Fhbase%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[进入hbase的shell hbase提供了一个shell的终端给用户交互 1[root@hadoop3 conf]# hbase shell 退出使用quit或者ctrl+c 需要关闭hadoop的安全模式不然进行一些操作，比如scan会卡住 进入到hadoop的bin目录下 在启动hbase之前 1[root@hadoop3 conf]# hadoop dfsadmin -safemode leave 创建表 1create ‘表名’,’列族1’,’列族2’,…’列族n’ 查看所有表 1list 描述表 12345678describe 'user'Table user is ENABLED COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; 'info1', DATA_BLOCK_ENCODING =&gt; 'NONE', BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '1', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', TTL =&gt; 'FOREVER', KEEP_DELETED_CELLS =&gt; 'FALSE', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'&#125; &#123;NAME =&gt; 'info2', DATA_BLOCK_ENCODING =&gt; 'NONE', BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '1', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', TTL =&gt; 'FOREVER', KEEP_DELETED_CELLS =&gt; 'FALSE', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'&#125; 2 row(s) in 0.0790 seconds 删除表 12345678hbase(main):033:0&gt; disable 'user'0 row(s) in 1.4110 secondshbase(main):034:0&gt; drop 'user'0 row(s) in 0.2330 secondshbase(main):035:0&gt; listTABLE 0 row(s) in 0.0450 seconds 判断表是否存在 123hbase(main):040:0&gt; exists 'user'Table user does exist 0 row(s) in 0.0980 seconds 向表中添加数据 12hbase(main):041:0&gt; put 'user','1234','info1:name','zhangsan'0 row(s) in 0.1310 seconds hbase没有直接修改操作，但是可以覆盖，只要rowkey跟列族列名一致就会覆盖 比如这里要修改上面插入数据的info:name为’eve’ 扫描整个表 1hbase(main):043:0&gt; scan 'user' 查询记录数 1234hbase(main):066:0&gt; count 'user'2 row(s) in 0.0340 seconds=&gt; 2 查询 123456hbase(main):067:0&gt; get 'user','1234'COLUMN CELL info1:age timestamp=1547125476542, value=18 info1:name timestamp=1547125469727, value=eve info2:favor timestamp=1547125482454, value=eat 3 row(s) in 0.0330 seconds 查询某一个列族 12345hbase(main):068:0&gt; get 'user','1234','info1'COLUMN CELL info1:age timestamp=1547125476542, value=18 info1:name timestamp=1547125469727, value=eve 2 row(s) in 0.0280 seconds 查询某个时间戳版本不知道时间戳先将这一个列族修改为能选择三个版本的列族，可以随便选择查多少个版本，查的版本只要比存的版本少就行 12hbase(main):069:0&gt; alter 'user' ,&#123;NAME=&gt;'info1',VERSIONS=&gt;3&#125;hbase(main):070:0&gt; get 'user'，'1234'，&#123;COLUMN=&gt;'info1:name',VERSIONS=&gt;3&#125; 知道时间戳 1hbase(main):071:0&gt; get 'user', '1234', &#123;COLUMN =&gt; 'info1:name',TIMESTAMP =&gt; 1538014481194&#125; 删除记录删除列族下的某一列 1hbase(main):071:0&gt; delete 'user','1234','info1:name' 删除某一列族 1hbase(main):072:0&gt; delete 'user','1234','info2' 删除某一行 1hbase(main):073:0&gt; deleteall 'user','1234' 清空表 1234567hbase(main):074:0&gt; truncate 'user'Truncating 'user' table (it may take a while): - Disabling table... - Truncating table...0 row(s) in 1.4540 secondshbase(main):075:0&gt; scan 'user'ROW COLUMN+CELL]]></content>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Aop实例@Aspect、@Before、@AfterReturning@Around 注解方式配置]]></title>
    <url>%2F2019%2F08%2F19%2F%E5%A4%A7%E5%86%99%E7%9A%84Spring%2F</url>
    <content type="text"><![CDATA[用过spring框架进行开发的人，多多少少会使用过它的AOP功能，都知道有@Before、@Around和@After等advice。最近，为了实现项目中的输出日志和权限控制这两个需求，我也使用到了AOP功能。我使用到了@Before、@Around这两个advice。但在，使用过程中，却对它们的执行顺序并不清楚。为了弄清楚在不同情况下，这些advice到底是以怎么样的一个顺序进行执行的，我作了个测试，在此将其记录下来，以供以后查看。 前提 对于AOP相关类(aspect、pointcut等)的概念，本文不作说明。 对于如何让spring框架扫描到AOP，本文也不作说明。 情况一: 一个方法只被一个Aspect类拦截当一个方法只被一个Aspect拦截时，这个Aspect中的不同advice是按照怎样的顺序进行执行的呢？请看: 添加 PointCut类123456/** * 切点 */@Pointcut("@annotation(com.guazi.gelada.rest.web.aop.RequestSysLog)")public void logPointCut() &#123;&#125; 添加Aspect类1234567891011121314151617181920212223242526272829303132333435/** * 前置通知 * * @param joinPoint 连接点 */ @Before("logPointCut()") public void printSysLog(JoinPoint joinPoint) &#123; MethodSignature signature = (MethodSignature) joinPoint.getSignature(); //请求方法 String methodName = signature.getName(); //获取request HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); LOG.info("inner web accept request method = &#123;&#125; and paramName = &#123;&#125;, params = &#123;&#125;", methodName, signature.getParameterNames(), joinPoint.getArgs()); &#125; @Around("logPointCut()") public void around(JoinPoint joinPoint)&#123; LOG.info("around"); &#125; @After("logPointCut()") public void after(JoinPoint joinPoint) &#123; System.out.println("After"); &#125; @AfterReturning("logPointCut()") public void afterReturning(JoinPoint joinPoint) &#123; System.out.println("AfterReturning"); &#125; @AfterThrowing("logPointCut()") public void afterThrowing(JoinPoint joinPoint) &#123; System.out.println("AfterThrowing"); &#125; 添加测试用Controller添加一个用于测试的controller，这个controller中只有一个方法，但是它会根据参数值的不同，会作出不同的处理：一种是正常返回一个对象，一种是抛出异常(因为我们要测试@AfterThrowing这个advice) 123456789101112131415161718192021222324252627282930313233343536373839404142@RestController@RequestMapping(value = "/aop")public class AopTestController &#123; @ResponseStatus(HttpStatus.OK) @RequestMapping(value = "/test", method = RequestMethod.GET) public Result test(@RequestParam boolean throwException) &#123; // case 1 if (throwException) &#123; System.out.println("throw an exception"); throw new TestException("mock a server exception"); &#125; // case 2 System.out.println("test OK"); return new Result() &#123;&#123; this.setId(111); this.setName("mock a Result"); &#125;&#125;; &#125; public static class Result &#123; private int id; private String name; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125;&#125; 看到输出结果123456around before test OKaround2afterafterReturning 要是出现异常情况看下返回 12345aroundbefore throw an exceptionafter afterThrowing 结论在一个方法只被一个aspect类拦截时，aspect类内部的 advice 将按照以下的顺序进行执行： 正常执行顺序 异常执行顺序 如果有多个aspect呢？多个aspect进行pointCut，顺序不定，可以用@order指定执行顺序]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase分享问题总结]]></title>
    <url>%2F2019%2F07%2F19%2FHBase%E5%88%86%E4%BA%AB%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[rowkey的put排序问题，涉及底层region和HFile的排序。 HBase为什么这么快。 主键的所有属性？ 为什么提前region分区，等待他自动分裂也可以？ HMaster的主从切换? 问题为什么这么快？ 写入快 写入内存、顺序写入。 删除更新功能：删除数据不是进行实质上的删除，也就是磁盘上仍然存在此条数据。只不过是对删除的数据打上了墓碑标记。利用墓碑标记，读数据会忽略此条数据。当进行小文件合并时，才会进行实质上删除。 剩下的合并、flush、compact都由region进行搞 读取快B+树的应用场景：主要用在传统的行数据库中，因为查询速度快。但是如有有大量的数据需要查询时就暴露出其弊端。 LSM树的应用场景：Hbase就是使用了LSM树。 主要的实现方式：写数据时，第一步，写到预写日志中，目的是防止数据在写入时丢失； 第二步，将数据放入到内存中。 第三步，当内存的大小超过指定值，会把内存中的数据写入到磁盘上。 需要注意一个关键点：磁盘的数据是有序的，这是利用预写日志和内存把随机写数据进行排序后写入，因此也能保证稳定的数据插入速率。 LSM的优点：能快速进行数据的合并和拆分。 负责架构 一个RS上只有一个HLog client职责： 12345678910111213第 1 步：Client 请求 ZooKeeper 获取.META.所在的 RegionServer 的地址。 .META.：记录了用户所有表拆分出来的的 Region 映射信息，.META.可以有多个 Regoin第 2 步：Client 请求.META.所在的 RegionServer 获取访问数据所在的 RegionServer 地址，Client 会将.META.的相关信息 cache 下来，以便下一次快速访问。第 3 步：Client 请求数据所在的 RegionServer，获取所需要的数据在这里会有一个问题，那就是 Client 会缓存.META.的数据，用来加快访问，既然有缓存，那它什么时候更新？如果.META.更新了，比如 Region1不在RerverServer2上了，被转移到了RerverServer3上。Client 的缓存没有更新会有什么情况呢？其实，Client 的元数据缓存不更新，当.META.的数据发生更新。如上面的例子，由于 Region1的位置发生了变化，Client 再次根据缓存去访问的时候，会出现错误，当出现异常达到重试次数后就会去.META.所在的 RegionServer 获取最新的数据，如果.META.所在的 RegionServer也变了，Client 就会去 ZooKeeper 上获取.META.所在的 RegionServer 的最新地址其实这也是region的一种寻址方式！！！(client要对某一行数据做读写的过程) ZooKeeper的职责： 12341、ZooKeeper为HBase提供 Failover 机制，选举 Master，避免单点 Master 单点故障问题2、存储所有 Region 的寻址入口：.META.表在哪台服务器上，.META.这张表的位置信息3、实时监控 RegionServer 的状态，将 RegionServer 的上线和下线信息实时通知给 Master4、存储HBase的Schema，包括有哪些Table，每个Table有哪些Column Family Master职责： 123451、为 RegionServer 分配 Region2、负责 RegionServer 的负载均衡3、发现失效的 RegionServer 并重新分配其上的 Region4、HDFS 上的垃圾文件（HBase）回收5、处理 Schema 更新请求（表的创建，删除，修改，列簇的增加等等） RegionServer的职责： 12341、RegionServer 维护 Master 分配给它的 Region，处理对这些 Region 的 IO 请求2、负责和底层的文件系统 HDFS 的交互，存储数据到 HDFS3、负责 Store 中的 HFile 的合并工作4、RegionServer 负责 Split 在运行过程中变得过大的 Region，负责 Compact 操作 以上可以看到，client 访问 HBase 上数据的过程并不需要 Master 参与（寻址访问 ZooKeeper 和RegioneServer，数据读写访问 RegioneServer），Master 仅仅维护Table 和 Region 的元数据信息，负载很低。.META. 存的是所有的 Region 的位置信息，那么 RegioneServer 当中 Region 在进行分裂之后的新产生的 Region，是由 Master 来决定发到哪个 RegioneServer，这就意味着，只有 Master知道 new Region 的位置信息，所以，由 Master 来管理.META.这个表当中的数据的 CRUD 在没有 Region 分裂的情况，Master 宕机一段时间是可以忍受的。 物理架构123456789101、Table 中的所有行都按照 RowKey 的字典序排列。2、Table 在行的方向上分割为多个 HRegion。3、HRegion 按大小分割的(默认 10G)，每个表一开始只有一个 HRegion，随着数据不断插入表，HRegion 不断增大，当增大到一个阀值的时候，HRegion 就会等分会两个新的 HRegion。 当表中的行不断增多，就会有越来越多的 HRegion。 4、HRegion 是 Hbase 中分布式存储和负载均衡的最小单元。最小单元就表示不同的 HRegion可以分布在不同的 HRegionServer 上。但一个 HRegion 是不会拆分到多个 server 上的(也就是没有hdfs中block的副本功能)。5、HRegion 虽然是负载均衡的最小单元，但并不是物理存储的最小单元。事实上，HRegion由一个或者多个 Store 组成，每个 Store 保存一个 Column Family。每个 Strore 又由一个MemStore 和 0 至多个 StoreFile 组成 写操作先写入memstore中，当memstore达到一定的阈值时，HRegionServer会启动flushcache进程写入到storefile中，每次写入形成单独的一个hfile， 当storefile达到一定的阈值时，会把当前的region分裂成两个region，并由master把新分裂出来的region分配到相应的HRegionServer服务器上，实现负载均衡。 客户端在检索数据时，先在memstore中查找，没有再到storefile中查找。6、每个 Region Server 维护一个 HLog，而不是每个 Region 一个。这样不同 region(来自不同 table)的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，因此可以提高对 table 的写性能。 带来的麻烦是，如果一台 region server下线，为了恢复其上的 Region，需要将 RegionServer 上的 log 进行拆分，然后分发到其它RegionServer 上进行恢复 读写过程：123456789101112131415161718192021222324读请求过程：&lt;1&gt;、client通过ZK找到.META.表所在的HRegionServer，client通过.META.表所在的HRegionServer找到所需数据的HRegionServer&lt;2&gt;、然后到数据所在的HRegionServer上获取相关数据&lt;3&gt;、HRegionServer 定位到目标数据所在的 Region，发出查询请求&lt;4&gt;、Region 先在 Memstore 中查找，命中则返回&lt;5&gt;、如果在 Memstore 中找不到，则在 Storefile 中扫描 为了能快速的判断要查询的数据在不在这个 StoreFile 中，应用了 BloomFilterBloomFilter，布隆过滤器：迅速判断一个元素是不是在一个庞大的集合内，但是他有一个弱点：它有一定的误判率误判率：原本不存在与该集合的元素，布隆过滤器有可能会判断说它存在，但是，如果布隆过滤器，判断说某一个元素不存在该集合，那么该元素就一定不在该集合内写请求过程：&lt;1&gt;、Client 先根据 RowKey 找到对应的 Region 所在的 RegionServer&lt;2&gt;、Client 向 RegionServer 提交写请求&lt;3&gt;、RegionServer 找到目标 Region&lt;4&gt;、Region 检查数据是否与 Schema 一致&lt;5&gt;、如果客户端没有指定版本，则获取当前系统时间作为数据版本&lt;6&gt;、将更新写入 WAL Log&lt;7&gt;、将更新写入 Memstore&lt;8&gt;、判断 Memstore 的是否需要 flush 为 StoreFile 文件Hbase 在做数据插入操作时，首先要找到 RowKey 所对应的的 Region，怎么找到的？其实这个简单，因为.META.表存储了每张表每个 Region 的起始 RowKey 了。在实际生产中注意：在做海量数据的插入操作，应该避免出现递增 rowkey 的 put 操作，如果 put 操作的所有 RowKey 都是递增的，那么试想，当插入一部分数据的时候刚好进行分裂，那么之后的所有数据都开始往分裂后的第二个 Region 插入，就造成了数据热点现象 数据在更新时首先写入 HLog(WAL Log)，再写入内存(MemStore)中，MemStore 中的数据是排序的，当 MemStore 累计到一定阈值(默认是 128M)时，就会创建一个新的 MemStore，并且将老的 MemStore 添加到 flush 队列，由单独的线程 flush 到磁盘上，成为一个 StoreFile。于此同时，系统会在 ZooKeeper 中记录一个 redo point，表示这个时刻之前的变更已经持久化了。当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用 HLog(WAL Log)来恢复 checkpoint 之后的数据。 Memstore 执行刷盘操作的的触发条件：123456789&lt;1&gt;、全局内存控制：当所有 memstore 占整个 heap 的最大比例的时候，会触发刷盘的操作。 这个参数是hbase.regionserver.global.memstore.upperLimit，默认为整个heap内存的40%。 这个全局的参数是控制内存整体的使用情况，但这并不意味着全局内存触发的刷盘操作会将所有的MemStore都进 行输盘，而是通过另外一个参数hbase.regionserver.global.memstore.lowerLimit 来控制，默认是整个 heap 内存的 35%。 当flush 到所有 memstore 占整个 heap 内存的比率为 35%的时候，就停止刷盘。这么做主要是为了减少刷盘对业务带来的影响，实现平滑系统负载的目的。&lt;2&gt;、当 MemStore 的大小达到 hbase.hregion.memstore.flush.size 大小的时候会触发刷盘，默认 128M 大小。&lt;3&gt;、前面说到 HLog 为了保证 HBase 数据的一致性，那么如果 HLog 太多的话，会导致故障恢复的时间太长，因此 HBase 会对 HLog 的最大个数做限制。 当达到 Hlog 的最大个数的时候，会强制刷盘。这个参数是 hase.regionserver.max.logs，默认是 32 个。 HLog默认是在HRegionServer上一小时形成一个新的HLog，在memstore的数据flush到storefile后，也会把之前的的HLog删除，其实也就是尽量保证最新的HLog保留的是memstore中的log，已经flush的数据的log会在后面进行删除。&lt;4&gt;、可以通过 HBase Shell 或者 Java API 手工触发 flush 的操作。 HBase 的三种默认的 Split 策略：1231、ConstantSizeRegionSplitPolicy2、IncreasingToUpperBoundRegionSplitPolicy3、SteppingSplitPolicy StoreFile 是只读的，一旦创建后就不可以再修改。因此 HBase 的更新/修改其实是不断追加的操作。当一个 Store 中的 StoreFile 达到一定的阈值后，就会进行一次合并(minor_compact,major_compact)，将对同一个 key 的修改合并到一起，形成一个大的 StoreFile，当 StoreFile的大小达到一定阈值后，又会对 StoreFile 进行 split，等分为两个 StoreFile。由于对表的更新是不断追加的，compact 时，需要访问 Store 中全部的 StoreFile 和 MemStore，将他们按rowkey 进行合并，由于 StoreFile 和 MemStore 都是经过排序的，并且 StoreFile 带有内存中索引，合并的过程还是比较快。 Minor_Compact 和 Major_Compact 的区别：12&lt;1&gt;、Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。&lt;2&gt;、Major 操作是对 Region 下的 HStore 下的所有 StoreFile 执行合并操作，最终的结果是整理合并出一个文件。 HBase 只是增加数据，所有的更新和删除操作，都是在 Compact 阶段做的，所以用户写操作只需要进入到内存即可立即返回，从而保证 I/O 高性能。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis扫盲]]></title>
    <url>%2F2019%2F07%2F16%2FMyBatis%E6%89%AB%E7%9B%B2%2F</url>
    <content type="text"><![CDATA[什么是Mybatis？ Mybaits的优点？ MyBatis框架的缺点？ MyBatis框架适用场合？]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解HDFS]]></title>
    <url>%2F2019%2F07%2F12%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3HDFS%2F</url>
    <content type="text"><![CDATA[胡思乱想HDFS，是Hadoop Distributed File System的简称，是Hadoop抽象文件系统的一种实现。Hadoop抽象文件系统可以与本地系统、Amazon S3等集成，甚至可以通过Web协议（webhsfs）来操作。HDFS的文件分布在集群机器上，同时提供副本进行容错及可靠性保证。例如客户端写入读取文件的直接操作都是分布在集群各个机器上的，没有单点性能压力。 HDFS的小目标 设计目标 · 存储非常大的文件· 采用流式的数据访问方式: HDFS基于这样的一个假设：最有效的数据处理 模式是一次写入、多次读取数据集经常从数据源生成或者拷贝一次· 运行于商业硬件上 HDFS不适合的应用类型 ·低延时的数据访问·大量小文件·多方读写，需要任意的文件修改 HDFS几个核心概念 Blocks 物理磁盘中有块的概念，磁盘的物理Block是磁盘操作最小的单元，读写操作均以Block为最小单元，一般为512 Byte。文件系统在物理Block之上抽象了另一层概念，文件系统Block物理磁盘Block的整数倍。通常为几KB。Hadoop提供的df、fsck这类运维工具都是在文件系统的Block级别上进行操作。 HDFS的Block块比一般单机文件系统大得多，默认为128M。HDFS的文件被拆分成block-sized的chunk，chunk作为独立单元存储。比Block小的文件不会占用整个Block，只会占据实际大小。例如， 如果一个文件大小为1M，则在HDFS中只会占用1M的空间，而不是128M。 HDFS的Block为什么这么大？ 是为了最小化查找（seek）时间，控制定位文件与传输文件所用的时间比例。假设定位到Block所需的时间为10ms，磁盘传输速度为100M/s。如果要将定位到Block所用时间占传输时间的比例控制1%，则Block大小需要约100M。 但是如果Block设置过大，在MapReduce任务中，Map或者Reduce任务的个数 如果小于集群机器数量，会使得作业运行效率很低。 Block抽象的好处 Block的拆分使得单个文件大小可以大于整个磁盘的容量，构成文件的Block可以分布在整个集群， 理论上，单个文件可以占据集群中所有机器的磁盘。 Block的抽象也简化了存储系统，对于Block，无需关注其权限，所有者等内容（这些内容都在文件级别上进行控制）。Block作为容错和高可用机制中的副本单元，即以Block为单位进行复制。 Namenode &amp; Datanode 整个HDFS集群由Namenode和Datanode构成master-worker（主从）模式。Namenode负责构建命名空间，管理文件的元数据等，而Datanode负责实际存储数据，负责读写工作。 (1) Namenode Namenode存放文件系统树及所有文件、目录的元数据。元数据持久化为2种形式：namespcae image/edit log但是持久化数据中不包括Block所在的节点列表，及文件的Block分布在集群中的哪些节点上，这些信息是在系统重启的时候重新构建（通过Datanode汇报的Block信息）。在HDFS中，Namenode可能成为集群的单点故障，Namenode不可用时，整个文件系统是不可用的。HDFS针对单点故障提供了2种解决机制： (2) 备份持久化元数据 将文件系统的元数据同时写到多个文件系统， 例如同时将元数据写到本地文件系统及NFS。这些备份操作都是同步的、原子的。 (3) Secondary Namenode Secondary节点定期合并主Namenode的namespace image和edit log， 避免edit log过大，通过创建检查点checkpoint来合并。它会维护一个合并后的namespace image副本， 可用于在Namenode完全崩溃时恢复数据。Secondary Namenode通常运行在另一台机器，因为合并操作需要耗费大量的CPU和内存。其数据落后于Namenode，因此当Namenode完全崩溃时，会出现数据丢失。 通常做法是拷贝NFS中的备份元数据到Second，将其作为新的主Namenode。在HA（High Availability高可用性）中可以运行一个Hot Standby，作为热备份，在Active Namenode故障之后，替代原有Namenode成为Active Namenode。 (4) Datanode 数据节点负责存储和提取Block，读写请求可能来自namenode，也可能直接来自客户端。数据节点周期性向Namenode汇报自己节点上所存储的Block相关信息。 Block Caching DataNode通常直接从磁盘读取数据，但是频繁使用的Block可以在内存中缓存。默认情况下，一个Block只有一个数据节点会缓存。但是可以针对每个文件可以个性化配置。 作业调度器可以利用缓存提升性能，例如MapReduce可以把任务运行在有Block缓存的节点上。 用户或者应用可以向NameNode发送缓存指令（缓存哪个文件，缓存多久）， 缓存池的概念用于管理一组缓存的权限和资源。 HDFS Federation 我们知道NameNode的内存会制约文件数量，HDFS Federation提供了一种横向扩展NameNode的方式。在Federation模式中，每个NameNode管理命名空间的一部分，例如一个NameNode管理/user目录下的文件， 另一个NameNode管理/share目录下的文件。 每个NameNode管理一个namespace volumn，所有volumn构成文件系统的元数据。每个NameNode同时维护一个Block Pool，保存Block的节点映射等信息。各NameNode之间是独立的，一个节点的失败不会导致其他节点管理的文件不可用。 客户端使用mount table将文件路径映射到NameNode。mount table是在Namenode群组之上封装了一层，这一层也是一个Hadoop文件系统的实现，通过viewfs:协议访问。 HDFS HA(High Availability高可用性) 在HDFS集群中，NameNode依然是单点故障（SPOF: Single Point Of Failure）。元数据同时写到多个文件系统以及Second NameNode定期checkpoint有利于保护数据丢失，但是并不能提高可用性。 这是因为NameNode是唯一一个对文件元数据和file-block映射负责的地方， 当它挂了之后，包括MapReduce在内的作业都无法进行读写。 当NameNode故障时，常规的做法是使用元数据备份重新启动一个NameNode。元数据备份可能来源于： ·多文件系统写入中的备份 Second NameNode的检查点文件 启动新的Namenode之后，需要重新配置客户端和DataNode的NameNode信息。另外重启耗时一般比较久，稍具规模的集群重启经常需要几十分钟甚至数小时，造成重启耗时的原因大致有： 1） 元数据镜像文件载入到内存耗时较长。 2） 需要重放edit log 3） 需要收到来自DataNode的状态报告并且满足条件后才能离开安全模式提供写服务。 Hadoop的HA方案 主备需共享edit log存储 DataNode需要同时往主备发送Block Report 客户端需要配置failover模式（失效备援模式，对用户透明） Standby替代Secondary NameNode 采用HA的HDFS集群配置两个NameNode，分别处于Active和Standby状态。当Active NameNode故障之后，Standby接过责任继续提供服务，用户没有明显的中断感觉。一般耗时在几十秒到数分钟。 HA涉及到的主要实现逻辑有 ·主备需共享edit log存储。 主NameNode和待命的NameNode共享一份edit log，当主备切换时，Standby通过回放edit log同步数据。 ·共享存储通常有2种选择 NFS：传统的网络文件系统QJM：quorum journal managerQJM是专门为HDFS的HA实现而设计的，用来提供高可用的edit log。QJM运行一组journal node，edit log必须写到大部分的journal nodes。通常使用3个节点，因此允许一个节点失败，类似ZooKeeper。注意QJM没有使用ZK，虽然HDFS HA的确使用了ZK来选举主Namenode。一般推荐使用QJM。 ·DataNode需要同时往主备发送Block Report 因为Block映射数据存储在内存中（不是在磁盘上），为了在Active NameNode挂掉之后，新的NameNode能够快速启动，不需要等待来自Datanode的Block Report，DataNode需要同时向主备两个NameNode发送Block Report。 ·客户端需要配置failover模式（失效备援模式，对用户透明） Namenode的切换对客户端来说是无感知的，通过客户端库来实现。客户端在配置文件中使用的HDFS URI是逻辑路径，映射到一对Namenode地址。客户端会不断尝试每一个Namenode地址直到成功。 ·Standby替代Secondary NameNode 如果没有启用HA，HDFS独立运行一个守护进程作为Secondary Namenode。定期checkpoint，合并镜像文件和edit日志。如果当主Namenode失败时，备份Namenode正在关机（停止 Standby），运维人员依然可以从头启动备份Namenode，这样比没有HA的时候更省事，算是一种改进，因为重启整个过程已经标准化到Hadoop内部，无需运维进行复杂的切换操作。NameNode的切换通过代failover controller来实现。failover controller有多种实现，默认实现使用ZooKeeper来保证只有一个Namenode处于active状态。每个Namenode运行一个轻量级的failover controller进程，该进程使用简单的心跳机制来监控Namenode的存活状态并在Namenode失败时触发failover。Failover可以由运维手动触发，例如在日常维护中需要切换主Namenode，这种情况graceful(优雅的) failover，非手动触发的failover称为ungraceful failover。在ungraceful failover的情况下，没有办法确定失败（被判定为失败）的节点是否停止运行，也就是说触发failover后，之前的主Namenode可能还在运行。QJM一次只允许一个Namenode写edit log，但是之前的主Namenode仍然可以接受读请求。Hadoop使用fencing来杀掉之前的Namenode。Fencing通过收回之前Namenode对共享的editlog的访问权限、关闭其网络端口使得原有的Namenode不能再继续接受服务请求。使用STONITH技术也可以将之前的主Namenode关机。 最后，HA方案中Namenode的切换对客户端来说是不可见的，前面已经介绍过，主要通过客户端库来完成。 命令行接口HDFS提供了各种交互方式，例如通过Java API、HTTP、shell命令行的。命令行的交互主要通过hadoop fs来操作。例如：123hadoop fs -copyFromLocal // 从本地复制文件到HDFShadoop fs mkdir // 创建目录hadoop fs -ls // 列出文件列表 Hadoop文件系统前面Hadoop的文件系统概念是抽象的，HDFS只是其中的一种实现。Hadoop提供的实现如下图：Local是对本地文件系统的抽象，hdfs就是我们最常见的，两种web形式（webhdfs，swebhdfs）的实现通过HTTP提供文件操作接口。har是Hadoop体系下的压缩文件，当文件很多的时候可以压缩成一个大文件，可以有效减少元数据的数量。viewfs就是我们前面介绍HDFS Federation张提到的，用来在客户端屏蔽多个Namenode的底层细节。ftp顾名思义，就是使用ftp协议来实现，对文件的操作转化为ftp协议。s3a是对Amazon云服务提供的存储系统的实现，azure则是微软的云服务平台实现。 前面我们提到了使用命令行跟HDFS交互，事实上还有很多方式来操作文件系统。例如Java应用程序可以使用org.apache.hadoop.fs.FileSystem来操作，其他形式的操作也都是基于FileSystem进行封装。我们这里主要介绍一下HTTP的交互方式。WebHDFS和SWebHDFS协议将文件系统暴露HTTP操作，这种交互方式比原生的Java客户端慢，不适合操作大文件。通过HTTP，有2种访问方式，直接访问和通过代理访问 Namenode和Datanode默认打开了嵌入式web server，即dfs.webhdfs.enabled默认为true。webhdfs通过这些服务器来交互。元数据的操作通过namenode完成，文件的读写首先发到namenode，然后重定向到datanode读取（写入）实际的数据流。 采用代理的示意图如下所示。 使用代理的好处是可以通过代理实现负载均衡或者对带宽进行限制，或者防火墙设置。代理通过HTTP或者HTTPS暴露为WebHDFS，对应为webhdfs和swebhdfs URL Schema。 代理作为独立的守护进程，独立于namenode和datanode，使用httpfs.sh脚本，默认运行在14000端口 除了FileSystem直接操作，命令行，HTTTP外，还有C语言API，NFS，FUSER等方式，这里不做过多介绍。 Java接口实际的应用中，对HDFS的大多数操作还是通过FileSystem来操作，这部分重点介绍一下相关的接口，主要关注HDFS的实现类DistributedFileSystem及相关类。 读操作 可以使用URL来读取数据，或者直接使用FileSystem操作。 URL 获取流12345InputStream in = null;try &#123; in = new URL("hdfs://master/user/hadoop").openStream();&#125;finally&#123; IOUtils.closeStream(in); FileSystem api获取流 · 首先获取FileSystem实例，一般使用静态get工厂方法123public static FileSystem get(Configuration conf) throws IOExceptionpublic static FileSystem get(URI uri , Configuration conf) throws IOExceptionpublic static FileSystem get(URI uri , Configuration conf，String user) throws IOException · 如果是本地文件，通过getLocal获取本地文件系统对象： 1public static LocalFileSystem getLocal(COnfiguration conf) thrown IOException · 调用FileSystem的open方法获取一个输入流:12public FSDataInputStream open(Path f) throws IOExceptionpublic abstarct FSDataInputStream open(Path f , int bufferSize) throws IOException 默认情况下，open使用4KB的Buffer，可以根据需要自行设置。 ·使用FSDataInputStream进行数据操作FSDataInputStream是java.io.DataInputStream的特殊实现，在其基础上增加了随机读取、部分读取的能力 ·随机读取操作通过Seekable接口定义：1234public interface Seekable &#123; void seek(long pos) throws IOException; long getPos() throws IOException;&#125; 写数据 在HDFS中，文件使用FileSystem类的create方法及其重载形式来创建，create方法返回一个输出流FSDataOutputStream，可以调用返回输出流的getPos方法查看当前文件的位移，但是不能进行seek操作，HDFS仅支持追加操作。 创建时，可以传递一个回调接口Peofressable，获取进度信息 append(Path f)方法用于追加内容到已有文件，但是并不是所有的实现都提供该方法，例如Amazon的文件实现就没有提供追加功能。 123456789101112131415String localSrc = args[0];String dst = args[1]; InputStream in = new BufferedInputStream(new FileInputStream(localSrc)); Configuration conf = new Configuration();FileSystem fs = FileSystem.get(URI.create(dst),conf); OutputStream out = fs.create(new Path(dst), new Progressable()&#123; public vid progress()&#123; System.out.print(.); &#125;&#125;); IOUtils.copyBytes(in , out, 4096,true); 目录操作 使用mkdirs（）方法,会自动创建没有的上级目录 HDFS中元数据封装在FileStatus类中，包括长度、block size，replicaions，修改时间、所有者、权限等信息。使用FileSystem提供的getFileStatus方法获取FileStatus。exists()方法判断文件或者目录是否存在； 列出文件（list），则使用listStatus方法，可以查看文件或者目录的信息 1public abstract FileStatus[] listStatus(Path f) throws FileNotFoundException,IOException; PathFilter用于自定义文件名过滤，不能根据文件属性进行过滤，类似于java.io.FileFilter。例如下面这个例子排除到给定正则表达式的文件： 1234public interfacePathFilter&#123; boolean accept(Path path);&#125; 删除数据 使用FileSystem的delete()方法 1public boolean delete(Path f , boolean recursive) throws IOException; recursive参数在f是个文件的时候被忽略。如果f是文件并且recursice为true，则删除整个目录，否则抛出异常. 数据流(读写流程）接下来详细介绍HDFS读写数据的流程，以及一致性模型相关的一些概念。 读文件 大致读文件的流程如下： 1）客户端传递一个文件Path给FileSystem的open方法 2）DFS采用RPC远程获取文件最开始的几个block的datanode地址。Namenode会根据网络拓扑结构决定返回哪些节点（前提是节点有block副本），如果客户端本身是Datanode并且节点上刚好有block副本，直接从本地读取。 3）客户端使用open方法返回的FSDataInputStream对象读取数据（调用read方法） 4）DFSInputStream（FSDataInputStream实现了改类）连接持有第一个block的、最近的节点，反复调用read方法读取数据 5）第一个block读取完毕之后，寻找下一个block的最佳datanode，读取数据。如果有必要，DFSInputStream会联系Namenode获取下一批Block 的节点信息(存放于内存，不持久化），这些寻址过程对客户端都是不可见的。 6）数据读取完毕，客户端调用close方法关闭流对象 在读数据过程中，如果与Datanode的通信发生错误，DFSInputStream对象会尝试从下一个最佳节点读取数据，并且记住该失败节点， 后续Block的读取不会再连接该节点读取一个Block之后，DFSInputStram会进行检验和验证，如果Block损坏，尝试从其他节点读取数据，并且将损坏的block汇报给Namenode。客户端连接哪个datanode获取数据，是由namenode来指导的，这样可以支持大量并发的客户端请求，namenode尽可能将流量均匀分布到整个集群。Block的位置信息是存储在namenode的内存中，因此相应位置请求非常高效，不会成为瓶颈。 写文件 步骤分解1）客户端调用DistributedFileSystem的create方法 2）DistributedFileSystem远程RPC调用Namenode在文件系统的命名空间中创建一个新文件，此时该文件没有关联到任何block。 这个过程中，Namenode会做很多校验工作，例如是否已经存在同名文件，是否有权限，如果验证通过，返回一个FSDataOutputStream对象。 如果验证不通过，抛出异常到客户端。 3）客户端写入数据的时候，DFSOutputStream分解为packets（数据包），并写入到一个数据队列中，该队列由DataStreamer消费。 4）DateStreamer负责请求Namenode分配新的block存放的数据节点。这些节点存放同一个Block的副本，构成一个管道。 DataStreamer将packet写入到管道的第一个节点，第一个节点存放好packet之后，转发给下一个节点，下一个节点存放 之后继续往下传递。 5）DFSOutputStream同时维护一个ack queue队列，等待来自datanode确认消息。当管道上的所有datanode都确认之后，packet从ack队列中移除。 6）数据写入完毕，客户端close输出流。将所有的packet刷新到管道中，然后安心等待来自datanode的确认消息。全部得到确认之后告知Namenode文件是完整的。 Namenode此时已经知道文件的所有Block信息（因为DataStreamer是请求Namenode分配block的），只需等待达到最小副本数要求，然后返回成功信息给客户端。 Namenode如何决定副本存在哪个Datanode？ HDFS的副本的存放策略是可靠性、写带宽、读带宽之间的权衡。默认策略如下： 第一个副本放在客户端相同的机器上，如果机器在集群之外，随机选择一个（但是会尽可能选择容量不是太慢或者当前操作太繁忙的）第二个副本随机放在不同于第一个副本的机架上。第三个副本放在跟第二个副本同一机架上，但是不同的节点上，满足条件的节点中随机选择。更多的副本在整个集群上随机选择，虽然会尽量避免太多副本在同一机架上。副本的位置确定之后，在建立写入管道的时候，会考虑网络拓扑结构。下面是可能的一个存放策略: 这样选择很好滴平衡了可靠性、读写性能 可靠性：Block分布在两个机架上写带宽：写入管道的过程只需要跨越一个交换机读带宽：可以从两个机架中任选一个读取 一致性模型 一致性模型描述文件系统中读写操纵的可见性。HDFS中，文件一旦创建之后，在文件系统的命名空间中可见： 123Path p = new Path("p");fs.create(p);assertTaht(fs.exists(p),is(true)); 但是任何被写入到文件的内容不保证可见，即使对象流已经被刷新。 12345Path p = new Path(“p”); OutputStream out = fs.create(p); out.write(“content”.getBytes(“UTF-8”)); out.flush(); assertTaht(fs.getFileStatus(p).getLen,L); // 为0，即使调用了flush 关闭对象流时，内部会调用hflush方法,但是hflush不保证datanode数据已经写入到磁盘，只是保证写入到datanode的内存， 因此在机器断电的时候可能导致数据丢失，如果要保证写入磁盘，使用hsync方法，hsync类型与fsync（）的系统调用，fsync提交某个文件句柄的缓冲数据。 12345FileOutputStreamout = new FileOutPutStream(localFile);out.write("content".getBytes("UTF-8"));out.flush();out.getFD().sync();assertTaht(localFile.getLen,is(((long,"content".length()))); 使用hflush或hsync会导致吞吐量下降，因此设计应用时，需要在吞吐量以及数据的健壮性之间做权衡。 另外，文件写入过程中，当前正在写入的Block对其他Reader不可见。 Hadoop节点距离 在读取和写入的过程中，namenode在分配Datanode的时候，会考虑节点之间的距离。HDFS中，距离没有采用带宽来衡量，因为实际中很难准确度量两台机器之间的带宽。Hadoop把机器之间的拓扑结构组织成树结构，并且用到达公共父节点所需跳转数之和作为距离。事实上这是一个距离矩阵的例子。下面的例子简明地说明了距离的计算： 同一数据中心，同一机架，同一节点距离为0 同一数据中心，同一机架，不同节点距离为2 同一数据中心，不同机架，不同节点距离为4 不同数据中心，不同机架，不同节点距离为6 Hadoop集群的拓扑结构需要手动配置，如果没配置，Hadoop默认所有节点位于同一个数据中心的同一机架上。 一次sqoop的离线从mysql—&gt;hdfs]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[普罗米修斯监控]]></title>
    <url>%2F2019%2F07%2F11%2F%E6%99%AE%E7%BD%97%E7%B1%B3%E4%BF%AE%E6%96%AF%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[简介作为新一代的监控框架，Prometheus 具有以下特点： · 强大的多维度数据模型： 时间序列数据通过 metric 名和键值对来区分。 所有的 metrics 都可以设置任意的多维标签。 数据模型更随意，不需要刻意设置为以点分隔的字符串。 可以对数据模型进行聚合，切割和切片操作。 支持双精度浮点类型，标签可以设为全 unicode。 · 灵活而强大的查询语句（PromQL）： · 易于管理： Prometheus server 是一个单独的二进制文件，可直接在本地工作，不依赖于分布式存储。·高效：平均每个采样点仅占 3.5 bytes，且一个 Prometheus server ·可以处理数百万的 metrics。·使用 pull 模式采集时间序列数据，这样不仅有利于本机测试而且可以避免有问题的服务器推送坏的 metrics。·可以采用 push gateway 的方式把时间序列数据推送至 Prometheus server 端。·可以通过服务发现或者静态配置去获取监控的 targets。·有多种可视化图形界面。·易于伸缩。·需要指出的是，由于数据采集可能会有丢失，所以Prometheus不适用对采集数据要 100%准确的情形。但如果用于记录时间序列数据，Prometheus具有很大的查询优势，此外，Prometheus 适用于微服务的体系架构。]]></content>
  </entry>
  <entry>
    <title><![CDATA[dubbo项目结构搞一搞]]></title>
    <url>%2F2019%2F07%2F01%2Fdubbo%E6%B7%B1%E5%BA%A6%E6%90%9E%E4%B8%80%E6%90%9E%2F</url>
    <content type="text"><![CDATA[各层说明总结（参考dubbo源码） ==================== Business ====================Service 业务层：业务代码的接口与实现。我们实际使用 Dubbo==================== RPC ====================config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 Spring 解析配置生成配置类。 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory 。 dubbo-rpc-rpc 模块实现。com.alibaba.dubbo.rpc.proxy包 + com.alibaba.dubbo.rpc.ProxyFactory接口 。 registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService 。dubbo-registry 模块实现。cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance 。 monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService 。dubbo-monitor 模块实现。==================== Remoting ====================protocol 远程调用层：封将 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter 。dubbo-rpc-rpc 模块实现。com.alibaba.dubbo.rpc.protocol包 + com.alibaba.dubbo.rpc.Protocol接口 。 exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer 。dubbo-remoting-api 模块定义接口。com.alibaba.dubbo.remoting.exchange包。transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec 。dubbo-remoting-api 模块定义接口。com.alibaba.dubbo.remoting.transport包。serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 。dubbo-common 模块实现。com.alibaba.dubbo.common.serialize包。 分享 Dubbo 的项目结构。 万事第一步拉取dubbo源码 代码统计 这里先分享一个小技巧。笔者在开始源码学习时，会首先了解项目的代码量。 第一种方式，使用 IDEA Statistic 插件，统计整体代码量。 还有一种方式，使用 Shell 脚本命令逐个 Maven 模块统计 。 1命令：find . -name "*.java"|xargs cat|grep -v -e ^$ -e ^\s*\/\/.*$|wc -l 这个命令只过滤了部分注释，所以相比 IDEA Statistic 会偏多。 当然，考虑到准确性，胖友需要手动 cd 到每个 Maven 项目的 src/main/java 目录下，以达到排除单元测试的代码量。 项目一览 逐个分析每一个工程3.1 dubbo-common dubbo-common 公共逻辑模块：提供工具类和通用模型。 工具类比较好理解，通用模型是什么？举个例子，com.alibaba.dubbo.common.URL ： 所有扩展点参数都包含 URL 参数，URL 作为上下文信息贯穿整个扩展点设计体系。URL 采用标准格式：protocol://username:password@host:port/path?key=value&amp;key=value 。 3.2 dubbo-remoting dubbo-remoting 远程通信模块：提供通用的客户端和服务端的通讯功能。 dubbo-remoting-zookeeper ，相当于 Zookeeper Client ，和 Zookeeper Server 通信。 dubbo-remoting-api ， 定义了 Dubbo Client 和 Dubbo Server 的接口。 实现 dubbo-remoting-api dubbo-remoting-grizzly ，基于 Grizzly 实现。 dubbo-remoting-http ，基于 Jetty 或 Tomcat 实现。 dubbo-remoting-mina ，基于 Mina 实现。 dubbo-remoting-netty ，基于 Netty 3 实现。 dubbo-remoting-netty4 ，基于 Netty 4 实现。 dubbo-remoting-p2p ，P2P 服务器。注册中心 dubbo-registry-multicast 项目的使用该项目。从最小化的角度来看，我们只需要看： dubbo-remoting-api + dubbo-remoting-netty4 dubbo-remoting-zookeeper 3.3 dubbo-rpcdubbo-rpc 远程调用模块：抽象各种协议，以及动态代理，只包含一对一的调用，不关心集群的管理。 dubbo-rpc-api ，抽象各种协议以及动态代理，实现了一对一的调用。 其他模块，实现 dubbo-rpc-api ，提供对应的协议实现。在 《用户指南 —— 协议参考手册》 中，可以看到每种协议的介绍。 另外，dubbo-rpc-default 对应 dubbo:// 协议。 3.4 dubbo-clusterdubbo-cluster 集群模块：将多个服务提供方伪装为一个提供方，包括：负载均衡, 集群容错，路由，分组聚合等。集群的地址列表可以是静态配置的，也可以是由注册中心下发。 注册中心下发，由 dubbo-registry 提供特性。 -容错 com.alibaba.dubbo.rpc.cluster.Cluster 接口 + com.alibaba.dubbo.rpc.cluster.support 包。 Cluster 将 Directory 中的多个 Invoker 伪装成一个 Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个。 拓展参见 《Dubbo 用户指南 —— 集群容错》 和 《Dubbo 开发指南 —— 集群扩展》 文档。 目录 com.alibaba.dubbo.rpc.cluster.Directory 接口 + com.alibaba.dubbo.rpc.cluster.directory 包。 Directory 代表了多个 Invoker ，可以把它看成 List ，但与 List 不同的是，它的值可能是动态变化的，比如注册中心推送变更。 路由 com.alibaba.dubbo.rpc.cluster.Router 接口 + com.alibaba.dubbo.rpc.cluster.router 包。 负责从多个 Invoker 中按路由规则选出子集，比如读写分离，应用隔离等。 拓展参见 《Dubbo 用户指南 —— 路由规则》 和 《Dubbo 开发指南 —— 路由拓展》 文档。 配置 com.alibaba.dubbo.rpc.cluster.Configurator 接口 + com.alibaba.dubbo.rpc.cluster.configurator 包。 拓展参见 《Dubbo 用户指南 —— 配置规则》 文档。 负载均衡 com.alibaba.dubbo.rpc.cluster.LoadBalance 接口 + com.alibaba.dubbo.rpc.cluster.loadbalance 包。 LoadBalance 负责从多个 Invoker 中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选。 拓展参见 《Dubbo 用户指南 —— 负载均衡》 和 《Dubbo 开发指南 —— 负载均衡拓展》 文档。 合并结果 com.alibaba.dubbo.rpc.cluster.Merger 接口 + com.alibaba.dubbo.rpc.cluster.merger 包。 合并返回结果，用于分组聚合。 拓展参见 《Dubbo 用户指南 —— 分组聚合》 和 《Dubbo 开发指南 —— 合并结果扩展》 文档。 整体流程如下： 3.5 dubbo-registry dubbo-registry 注册中心模块：基于注册中心下发地址的集群方式，以及对各种注册中心的抽象。 dubbo-registry-api ，抽象注册中心的注册与发现接口。 其他模块，实现 dubbo-registry-api ，提供对应的注册中心实现。在 《用户指南 —— 注册中心参考手册》 中，可以看到每种注册中心的介绍。 另外，dubbo-registry-default 对应 Simple 注册中心。 拓展参见 《Dubbo 开发指南 —— 注册中心扩展》 文档。 3.6 dubbo-monitordubbo-monitor 监控模块：统计服务调用次数，调用时间的，调用链跟踪的服务。 拓展参见 《Dubbo 开发指南 —— 监控中心扩展》 。 目前社区里，有对 Dubbo 监控中心进行重构的项目，例如 ： https://github.com/handuyishe/dubbo-monitor https://github.com/zhongxig/dubbo-d-monitor 3.7 dubbo-config dubbo-config 配置模块：是 Dubbo 对外的 API，用户通过 Config 使用Dubbo，隐藏 Dubbo 所有细节。 dubbo-config-api ，实现了 API 配置 和 属性配置 功能。 dubbo-config-spring ，实现了 XML 配置 和 注解配置 功能。 推荐阅读 《Dubbo 开发指南 —— 配置设计》。 3.8 dubbo-containerdubbo-container 容器模块：是一个 Standlone 的容器，以简单的 Main 加载 Spring 启动，因为服务通常不需要 Tomcat/JBoss 等 Web 容器的特性，没必要用 Web 容器去加载服务。 dubbo-container-api ：定义了 com.alibaba.dubbo.container.Container 接口，并提供 加载所有容器启动的 Main 类。 实现 dubbo-container-api dubbo-container-spring ，提供了 com.alibaba.dubbo.container.spring.SpringContainer 。 dubbo-container-log4j ，提供了 com.alibaba.dubbo.container.log4j.Log4jContainer 。 dubbo-container-logback ，提供了 com.alibaba.dubbo.container.logback.LogbackContainer 。 拓展参考 《Dubbo 用户指南 —— 服务容器》 和 《Dubbo 开发指南 —— 容器扩展》 文档。 3.9 dubbo-filter dubbo-filter 过滤器模块：提供了内置的过滤器。 dubbo-filter-cache ，缓存过滤器。 拓展参考 《Dubbo 用户指南 —— 结果缓存》 和 《Dubbo 开发指南 —— 缓存拓展》 文档。 dubbo-filter-validation ，参数验证过滤器。 拓展参考 《Dubbo 用户指南 —— 参数验证》 和 《Dubbo 开发指南 —— 验证扩展》 文档。 3.10 dubbo-plugin dubbo-plugin 插件模块：提供了内置的插件。 dubbo-qos ，提供在线运维命令。 拓展参考 《Dubbo 用户指南 —— 新版本 telnet 命令使用说明》 和 《Dubbo 开发指南 —— Telnet 命令扩展》 文档。 3.11 hessian-litehessian-lite ：Dubbo 对 Hessian 2 的 序列化 部分的精简、改进、BugFix 。 3.14 Maven POM 3.14.1 dubbo-all dubbo/all/pom.xml ，Dubbo All Pom ，定义了 Dubbo 的打包脚本。 我们在使用 Dubbo 库时，引入该 pom 文件。]]></content>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一套客服系统的服务端]]></title>
    <url>%2F2019%2F05%2F30%2F%E4%B8%80%E5%A5%97%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9C%8D%E5%8A%A1%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[我的客服IM模块划分gate ###创建场景、登陆jwt校验、获取会话列表和会话页、种类消息、消息推送、结束评价 ### 网络编程思想通信协议、通信接口]]></content>
  </entry>
  <entry>
    <title><![CDATA[阿里dubbo---北京MeetUp会议记录2019.5.26]]></title>
    <url>%2F2019%2F05%2F26%2F%E5%9C%A8%E5%BA%94%E7%94%A8%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[XA模式 主讲（申海强）锁–我们在传统的系统进行请求访问的时候，一次请求都要经过XA，start-end，涉及了很多分布式事务的锁。 连接–如果说链路太长，那么这个连接不能释放。 TM begin- TC 事务id - RM 注册TC 统一用TC回溯（回追一些事务）— TM end ###本地模式必须遵守ACID 牛逼的点：undoLog TC - Rm 来嵌套下，saga tcc 模式 ###提问环节 对于olap也没有很好的办法，本架构也主要在oltp进行开展。 TC高可用是否可以得到保障，一个新技术引入架构的话，是一个单点隐患，复杂的问题聚焦到一个点。 undoLog确定是存在数据库中，这就解释了本地模式为什么是acid。 TC对链路的控制，优先选择第一条链路。 其他外国大厂并没有对微服务层次上对事务有把控、只是在数据库上做了。 apache dubbo的设想 主讲（秦金卫）dubbo的想法 abc基础能力 云一体化、人工智能、大数据。悄悄告诉大家，如果创业，这三方面一定占据几个 ###生态 商场模式，all in one.所有东西都放在一个商场中，什么都有，吸引所有人进来。一旦全部链路打通后，就成了大家习惯的一种姿态。 容器模式 单独部署、单独建立 ###如何发展 聚焦于行业领域 定制化 社区完善 ###dubbo 一个发展很牛逼的东西，可以看成是eip、不用修改一行代码， 直接暴露出所有东西。 ###欢迎参与阿里开源 ###提问环节 用新技术搭新工程其实是技术选型问题。 如果是面对老系统，其实就是和旧世界作斗争，选择办法打通一条通往新世界的大门，开一个口子接入新系统 eip目前还在处于构造中。 sentinel 网关限流 赵亦豪（宿何）sentinel微服务稳定性的场景 场景： 激增流量 下游服务的不稳定性可行性： 流量控制 流量整型 熔断降级 系统自适应保护 要在系统出现堕机前进行，进行保护 ###1.6.0新特性 注解的支持改进 异常类型限定 网关流量控制 更好的errorCallBack的返回 开源万岁 ###提问环节 一旦集成后，动态server的管理：可以搞成一种独立模式的一种agent。 sentinel熔断没有一个半打开的功能。豪猪有 dubbo服务自省设计与实现 主讲（今天最牛逼的小马哥）什么是服务自省 dubbo可以暴露很多协议，dubbo应用在运行时处理和分析dubbo的服务元信息 dubbo对外提供的是服务，cloud是接口。微服务还有些地方不是特别成熟，本次升级dubbo自省模式，加入了metaData原数据。 zk向nacos的迁移一句话，早晚的事，zk其实挺被诟病的，只是因为没有更好的一个选择，注册中心这个概念，正是因为确定保证它的稳定性，所以要开发出的一个良好的模式]]></content>
  </entry>
  <entry>
    <title><![CDATA[我是菜鸟---java基础通用复习篇]]></title>
    <url>%2F2019%2F05%2F15%2F%E6%88%91%E6%98%AF%E8%8F%9C%E9%B8%9F-java%E5%9F%BA%E7%A1%80%E9%80%9A%E7%94%A8%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[java集合、java并发、java网络、java虚拟机 面向对象是java的一句代表 面向对象的特征？四点记住！封装、继承、多态、抽象。 面向对象—封装封装给对象提供了隐藏内部特性和行为的能力，对象提供一些给其他对象改变自己内部参数的方法。这么一说比较绕，简单来讲就是我们常见的四种类修饰符：private、friendly、projected、public。这些修饰符给对象赋予了不同的访问权限。 封装的好处：通过隐藏对象的属性来保护对象内部的状态，提高了代码可读性、维护性。模块化。 面向对象—继承继承，给对象提供了从基类获取字段和方法的能力。继承提供了代码的重用行，也可以在不修改类的情况下给现存的类添加新特性。 面向对象—多态多态，是编程语言给不同的底层数据类型做相同的接口展示的一种能力。一个多态类型上的操作，可以应用到其他类型的值上面。 面向对象—抽象抽象，是把想法从具体的实例中分离出来的步骤，因此，要根据他们的功能而不是实现细节来创建类。Java 支持创建只暴漏接口而不包含方法实现的抽象的类。这种抽象技术的主要目的是把类的行为和实现细节分离开。 面向过程与面向对象的区别性能高低不同，自己想想哪个高低？ 重载和重写的区别？1）重写 override 方法名、参数、返回值相同。子类方法不能缩小父类方法的访问权限。子类方法不能抛出比父类方法更多的异常(但子类方法可以不抛出异常)。存在于父类和子类之间。方法被定义为 final 不能被重写。 2）重载 overload 参数类型、个数、顺序至少有一个不相同。不能重载只有返回值不同的方法名。存在于父类和子类、同类中。 Java 中，什么是构造方法？什么是构造方法重载？什么是拷贝构造方法？1）构造方法 当新对象被创建的时候，构造方法会被调用。每一个类都有构造方法。在程序员没有给类提供构造方法的情况下，Java 编译器会为这个类创建一个默认的构造方法。 2）构造方法重载 Java 中构造方法重载和方法重载很相似。可以为一个类创建多个构造方法。每一个构造方法必须有它自己唯一的参数列表。 3）拷贝构造方法 Java 不支持像 C++ 中那样的拷贝构造方法，这个不同点是因为如果你不自己写构造方法的情况下，Java 不会创建默认的拷贝构造方法。 JDK、JRE、JVM 分别是什么关系？🐷JDKJDK 即为 Java 开发工具包，包含编写 Java 程序所必须的编译、运行等开发工具以及 JRE。开发工具如： 用于编译 Java 程序的 javac 命令。用于启动 JVM 运行 Java 程序的 Java 命令。用于生成文档的 Javadoc 命令。用于打包的 jar 命令等等。 简单说，就是 JDK 包含 JRE 包含 JVM。 🐷JRE JRE 即为 Java 运行环境，提供了运行 Java 应用程序所必须的软件环境，包含有 Java 虚拟机（JVM）和丰富的系统类库。系统类库即为 Java 提前封装好的功能类，只需拿来直接使用即可，可以大大的提高开发效率。 简单说，就是 JRE 包含 JVM。 🐷JVM 为什么 Java 被称作是“平台无关的编程语言”？Java 源文件( .java )被编译成能被 Java 虚拟机执行的字节码文件( .class )。Java 被设计成允许应用程序可以运行在任意的平台，而不需要程序员为每一个平台单独重写或者是重新编译。Java 虚拟机让这个变为可能，因为它知道底层硬件平台的指令长度和其他特性。 JDK 各版本的新特性？JDK5 ~ JDK10 ，看 https://www.jianshu.com/p/37b52f1ebd4a 。JDK11 ，看 https://www.jianshu.com/p/81b65eded96c 。对于大多数面试官，肯定不会问你 JDK 各版本的新特性，更多的会问 JDK8 引入了什么重要的特性？一般上，关键的回答是Lambda 表达式和集合之流式操作，然后说说你在项目中怎么使用的。 JVM 即为 Java 虚拟机，提供了字节码文件(.class)的运行环境支持。 Java 和 C++ 的区别？都是面向对象的语言，都支持封装、继承和多态。Java 不提供指针来直接访问内存，程序内存更加安全。Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。【重要】Java 有自动内存管理机制，不需要程序员手动释放无用内存。 什么是字节码？采用字节码的最大好处是什么？这个问题，面试官可以衍生提问，Java 是编译执行的语言，还是解释执行的语言。 Java 源代码=&gt; 编译器 =&gt; JVM 可执行的 Java 字节码(即虚拟指令)=&gt; JVM =&gt; JVM 中解释器 =&gt; 机器可执行的二进制机器码 =&gt; 程序运行 每一种平台的解释器是不同的，但是实现的虚拟机是相同的。Java 源程序经过编译器编译后变成字节码，字节码由虚拟机解释执行，虚拟机将每一条要执行的字节码送给解释器，解释器将其翻译成特定机器上的机器码，然后在特定的机器上运行。这也就是解释了 Java 的编译与解释并存的特点。 java集合集合框架我觉得不管是学习知识还死活开发实战都是必要的，不可苟且。 一个工具类中的集合可以提高很多的性能。 map collection map set list。 递归的使用递归的使用要定义递归头和递归体。递归头定义出什么时候不调用自己，否则会出现死循环。递归体定义什么什么时候调用自己。但递归太耗费性能了，考虑的话完全可以用循环来替代。 面向过程和面向对象c语言面向过程。java面向过程。都是设计软件的基本思想，相辅相成，不是对立的。面向过程，遇到事了，专注于怎么做，比如说第一步怎么做、第二步怎么做，就是面向过程。面向过程适合简单任务，简单任务是指不需要协作的事务。面向对象，解决复杂问题，宏观上使用面向对象把握，但是微观处理仍然是面向过程。 对象可以看为数据管理的方式，也可以看为内存中的一个内存块，包含了很多数据。程序发展之初是不需要对象的，只需要基本数据类型就可以了。但是事物的发展总是量变引起质变。 类和对象对象的概念由小作坊到大楼、到公司、到大楼。 对象和类的概念，类可以看作对象的模版。类中包含了成员变量和方法。 见到new就是调用一个类的构造方法，new出来了一个实例，这个无参构造方法默认是系统自动创建的。 java虚拟机 jvm说下几个jvm中的概念吧：栈、堆、方法区。 ######栈 栈描述的就是方法执行的模型。每个方法被调用都会创建一个栈帧（存储局部变量、操作数、方法出口等）。 java为每个线程提供一个栈，用于存放该线程执行的方法信息 栈属于线程私有，不能实现共享。 栈的特性，结合方法的使用来思考。 栈是由系统自动分配，速度快，栈是一个连续的内存空间。 ######堆 堆用于存储对象创建好的对象和数组，数组也是对象。 jvm只有一个堆，线程共享。 堆的内存不连续。 ######方法区 方法区其实也是堆。 用于存储类相关的信息。 用于存储程序中永远不变或者唯一的内容：class信息、静态变量、字符串常量、class对象。 一次简单的函数调用，需要去画一次内存分配和执行流程123456789101112131415161718192021222324252627282930public class execise &#123; private String name; private Integer age; void study()&#123; System.out.println("学习中，学习人："+name); &#125; void age()&#123; System.out.println("学习的年龄"+age); &#125; User user; public static void main(String[] args) &#123; execise execise = new execise(); execise.age = 30; execise.name = "nounounou"; execise.study(); execise.age(); User user = new User(); user.setPassword("123"); &#125;&#125; 构造方法构造方法的方法名必须和类名保持一致 如果自己定义了一个类的构造方法，那么如果像这样 User user = new User();系统就不管了，就要自己重新定义。 this关键字表示创建好的对象 垃圾回收机制GB collection垃圾回收机制 我们提前说下比较有意思一个故事，C++和java对比最头疼的就是内存管理需要程序员自己去解决。但是java有了垃圾回收器，解决了这个问题。 C++就像学校的食堂，java就像外面的餐馆。 垃圾回收器发现垃圾，算法大致了解一下(1)可达法和(2)引用计数法但是引用计数法有个缺陷就是两个对象之间循环引用。 我建议在垃圾回收的过程也画一遍图 maniro major full gc this创建对象分为四步 分配对象空间，并将对象成员变量初始化为0或空。 执行属性值的显示初始化。 执行构造方法。 返回对象的地址给相关变量。 == 和 equal记住一点就可以，基本类型可以看作最简单的类型，用哪个都可以比较但是想一下，涉及一个对象的话，==号就失效了。为什么呢&gt;比较的是地址hascode，两个对象的hashcode是不一样的，一般常用的object类都重写了equal的方法。equal可以比较他们的内容。所以在撸码中，非要比较两个对象是否相等，那就重写这个类的equal方法。比较里面的某一个参数是否相等。 两个对象的地址不一致。 封装对像类中的封装不要暴露给外面使用，一般会设置为private。 我们看看集中访问权限：private、default、protected、public 同一个类-&gt;同一个包-&gt;子类-&gt;全局 不管三七二十一，类中属性变量都私有。 多态我们去调一个方法，参数不同，行为不同。 数组数组其实也是一个对象，记住里面对应每一个组标的数据可以看作是对象的成员变量。 finalfinal修饰的方法不能重写、类不能继承、修饰字符串则为字符串常量。 数组` public static void main(String[] args) { String abc = &quot;hello java&quot;; String def = &quot;HELLo java&quot;; System.out.println(abc.substring(0, 9)); System.out.println(abc == def); System.out.println(abc.equalsIgnoreCase(def)); System.out.println(abc.replaceAll(&quot;java&quot;,&quot;asd&quot;)); System.out.println(abc.compareTo(&quot;123&quot;)); String[] array = {&quot;1&quot;,&quot;2&quot;,&quot;3&quot;}; System.out.println(JSON.toJSONString(testDeleteArrayOfIndex(array, 0))); System.out.println(ArrayUtils.toString(testInsertArrayOfIndex(array, 1,2))); } /** * 删除指定数组元素 * @param array * @param index * @return */ public static String[] testDeleteArrayOfIndex(String[] array, Integer index){ // String[] array2 = new String[array.length -1]; System.arraycopy(array,index+1 , array, index, array.length - index -1); array[array.length -1 ] = null; return array; } public static String[] testInsertArrayOfIndex(String[] array, Object param, Integer index){ // String[] array2 = new String[array.length -1]; array = extendArray(array, 1); System.out.println(array.length); System.arraycopy(array, index , array, index+1, array.length-index - 1); array[index] = param.toString(); return array; } /** * 数组扩容 * @param array * @param length * @return */ public static String[] extendArray(String[] array, Integer length){ String[] s1 = {&quot;1&quot;,&quot;2&quot;,&quot;3&quot;}; String[] s2 = new String[s1.length + length]; System.arraycopy(s1, 0 , s2, 0, s1.length); return s2; }` 二分查找` public static void main(String[] args) { String[] array = {&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;}; System.out.println(midSort(array, &quot;6&quot;)); } public static int midSort(String[] array, String value){ int left = 0; int right = array.length - 1; while ( left &lt;= right){ int mid = (left + right)/2 ; if(array[mid].equals(value)){ return mid; } if(Integer.valueOf(value) &gt; Integer.valueOf(array[mid])){ left = mid+1; } if(Integer.valueOf(value) &lt; Integer.valueOf(array[mid])){ right = mid-1; } } return -1; } ` List接口` public static void main(String[] args) { List list01 = new ArrayList&lt;&gt;(); list01.add(“01”); list01.add(“02”); list01.add(“03”); List list02 = new ArrayList&lt;&gt;(); list02.add(“01”); list02.add(“02”); list02.add(“04”); list02.add(“05”); list01.removeAll(list02); System.out.println(list01); System.out.println(list01); } `]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我是菜鸟---java基础类型篇]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%88%91%E6%98%AF%E8%8F%9C%E9%B8%9F-java%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[局部变量、全局变量、静态变量的意思和生命周期注意点：局部变量是必须赋值才能使用，全局变量即使不赋值也有默认值，静态变量是全局的生命周期最长。 常量只要给的值就不能再改变了，int age = 1；1是常量，age是变量。final 修饰的字符又是字符常量final int age。 基本数据类型（primitive data） ----数值型 整数类型（byte、short、int、long）浮点类型(float、double) （3大类、8小类） ----基本数据类型 ----字符型 char 2个字节 ----布尔型 boolean 1位 注意⚠️ 一个bit 数据类型 ----引用数据类型 类、接口、数组 占用4个字节。代表对象地址 整型变量/常量byte、short、int、long 十进制正常数字、八进制015、十六进制0X16、二进制0b1101; 浮点型float、double float a = 3.14F; double b = 1.14;浮点数是不精确的 ，一定不要用做比较。如果想比较浮点数，要用java.math包下面的BigDecimal和BigInteger BigDecimal.valueOf(0.01); 布尔类型true and false 顺序结构、选择结构、循环结构这节其实很易懂，但很多人忽略了，一个很小很小的程序，或者大的操作系统说白了都是变量+选择结构+循环结构组成的。 break、continuebreak跳出整套循、continue跳出本次循环。另外还有带标签的break、continue。这个是可以跳转的。之前有个goto关键字，是java一个保留字读。如果大量使用goto语句，程序结构可能会失去控制。所以我们使用带标签的break和continue。 场景：希望在一个嵌套循环中，跳出到外侧循环， outer:for（）{ if(){ break outer;}}]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础复习---jvm的几大分区和oom的情况]]></title>
    <url>%2F2019%2F05%2F13%2Fjava%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0-jvm%E7%9A%84%E5%87%A0%E5%A4%A7%E5%88%86%E5%8C%BA%E5%92%8Coom%E7%9A%84%E6%83%85%E5%86%B5%2F</url>
    <content type="text"></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[豪猪熔断器]]></title>
    <url>%2F2019%2F05%2F04%2F%E8%B1%AA%E7%8C%AA%E7%86%94%E6%96%AD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[熔断器的思想和我们日常所用的保险丝一样，是一款保护全局服务正常调用的工具，防止服务雪崩。]]></content>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ribbon]]></title>
    <url>%2F2019%2F05%2F02%2FRibbon%2F</url>
    <content type="text"><![CDATA[Ribbon的概念，实现客户端的软件式负载均衡。 负载均衡概念：集中式LB （F5）硬件层面 和 进程式LB软件层面。 Ribbon配置初步 正确的思维和落地方法论。 GAV-maven @Enable*** 123456789 @Configurationpublic class webConfig &#123; @Bean @LoadBalanced public RestTemplate getRestTemplate()&#123; return new RestTemplate(); &#125;&#125; Ribbon和eureka整合后，consumer可以直接调用不用再关心地址和端口号。 Ribbon的负载均衡默认轮询算法 配置多个eureka+多个消费+多个注册。每个注册独有DB。 Ribbon核心组件IRule在configuration 1234567891011121314 @Configurationpublic class webConfig &#123; @Bean @LoadBalanced public RestTemplate getRestTemplate()&#123; return new RestTemplate(); &#125; @Bean public IRule myRule()&#123; return new RadomRobinRule(); &#125;&#125; 根据特定算法选出来一个负载算法。 Ribbon自定义自定义配置类，配置类不要放在compent的同目录下面。在启动类上面打@RobinClient(“微服务名称”,Rule.class) 在配置类中的，重新new一个实现了负载算法的实例子。]]></content>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通读mysql的（持续更新）]]></title>
    <url>%2F2019%2F05%2F01%2F%E9%80%9A%E8%AF%BBmysql%E7%9A%84%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[作为一名工程师，不论前端后端，不熟悉甚至不了解mysql，真的很失败。本篇是持续更新篇，也是给自己一个复习空间。 2019年5月1日—mysql的逻辑架构一些基本概念吧，上学的时候并不想背这些，其实怪自己没有下功夫去实践，实践几遍，不用背也能说出个123。ACID、数据库的并发。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop分布式的节操安装实践]]></title>
    <url>%2F2019%2F04%2F29%2Fhadoop%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E8%8A%82%E6%93%8D%E5%AE%89%E8%A3%85%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[前言Hadoop是一个由Apache基金会所开发的分布式系统基础架构。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算.本教程将指导如何用苹果macOS系统安装Hadoop。 安装介绍此篇来点真格的安装过程，可以按照流程走一遍，少走很多安装的大坑…..我当时安装了几乎小半个月，哎。菜是原罪…. 另外，为什么说这是hadoop的分布式的节操呢？以下我分三块来说，例如hadoop的单机模式（流氓模式）、hadoop伪分布式模式（无赖模式）、hadoop完全分布式模式（正经人）。 这三种模式其实都是hadoop，就是我们安装的方式不一样。 流氓三剑客单机模式（standalone） 单机模式是Hadoop的默认模式。这种模式在一台单机上运行，没有分布式文件系统，而是直接读写本地操作系统的文件系统。当首次解压Hadoop的源码包时，Hadoop无法了解硬件安装环境，便保守地选择了最小配置。在这种默认模式下所有3个XML文件均为空。当配置文件为空时，Hadoop会完全运行在本地。因为不需要与其他节点交互，单机模式就不使用HDFS，也不加载任何Hadoop的守护进程。该模式主要用于开发调试MapReduce程序的应用逻辑。 伪分布模式（Pseudo-Distributed Mode） 这种模式也是在一台单机上运行，但用不同的Java进程模仿分布式运行中的各类结点 伪分布模式在“单节点集群”上运行Hadoop，其中所有的守护进程都运行在同一台机器上。该模式在单机模式之上增加了代码调试功能，允许你检查内存使用情况，HDFS输入输出，以及其他的守护进程交互。 全分布模式（Fully Distributed Mode） Hadoop守护进程运行在一个集群上。 依靠无赖走天下自古流氓怕圣人、圣人怕无赖。我们先摆平无赖再去当圣人。 无赖模式、不，是伪分布式模式的安装用一台单机就可以模拟，废话少说。安装！我是在mac上模拟的，安装器也是Homebrew、如果在Linux环境下，自行选择安装器哈，流程都是一样的。 打开shell命令，直接上—brew install hadoop 安装过程会提示重要的信息，如下： $JAVA_HOME has been set to be the output of: /usr/libexec/java_home Hadoop的安装需要配置JAVA_HOME，用 brew安装，就已经帮我们配置好了。Java_Home老铁们知道干啥的吧….不多说了啊，系统环境变量。 测试hadoop安装是否成功 分布式模式需要在多台电脑上面测试，这里只测试前面两种，即单机模式和伪分布式模式。走一遍hadoop的mapReduce任务吧，一个基础操作就是输入输出。在hadoop的安装目录中，我的是这个/usr/local/Cellar/hadoop/3.1.1/libexec/share/hadoop/mapreduce。看见里面的client-jar包了吧，ok，我们到 cd /usr/local/Cellar/hadoop/3.1.1/创建input和output目录进入input目录 cd input，创建两个文件 echo ‘hello world’ &gt; file1.txt 、echo ‘hello hadoop’ &gt; file2.txt。 运行示例检测单机模式,在shell上直接走！ hadoop jar ./libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar wordcount ./input ./output 显示结果more output/part-r-00000 —- hadoop 1 hello 2 world 1 进阶无赖伪分布式要修改的hadoop的配置了，这些配置是很关键的，有时间的同学可以挖一挖。 so，cd 到 /usr/local/Cellar/hadoop/3.1.1/libexec/etc/hadoop中 修改Core-site.xml这是hadoop和hdfs的核心配置，有些配置是公用的。 修改为： 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/Cellar/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;configuration&gt; s.default.name 保存了NameNode的位置，HDFS和MapReduce组件都需要用到它，这就是它出现在core-site.xml 文件中而不是 hdfs-site.xml文件中的原因 修改mapred-site.xml.template 修改为： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 修改为： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 变量dfs.replication指定了每个HDFS数据库的复制次数。 通常为3, 由于我们只有一台主机和一个伪分布式模式的DataNode，将此值修改为1。 hadoop namenode -format./sbin/start-all.sh 用示例测试 估计圆周率PI的值 hadoop jar ./libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 2 5 运行结果 Estimated value of Pi is 3.60000000000000000000 靠谱的一篇正经人，Hadoop完全分布式教程完全分布式和伪分布式区别就是集群的搭建，加入了zk的配置。具体流程其实不复杂。 相关文档：https://blog.csdn.net/sunqingok/article/details/87210767]]></content>
      <tags>
        <tag>大数据，hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅分析HDFS]]></title>
    <url>%2F2019%2F04%2F28%2F%E6%B5%85%E5%88%86%E6%9E%90HDFS%2F</url>
    <content type="text"><![CDATA[前言和大家说说我接触大数据的经历吧。在去年刚毕业的我，一脸懵逼的进了公司，当时所属我的一个标签就是菜狗。什么都是新的，什么也都是挑战的。更糟糕的是面对突入其来的压力，走了很多弯路。其实回想下陌生的恐惧支配了我大部分的经历。唯一的欣慰点是这样的压力促进了我加快学习。压力和工作气氛稍有好转的几天后，一个叫做HBase的玩具交给了我，让我玩一玩。简要来说— 目标 运用HBase把海量数据查询和写入搞定，代替传统的关系型数据库mysql，我们的mysql日均写入量百万。在接手的时候总库的量已经破了4亿。没错，你在一个亿量级的表里面找条数据（索引失效），你想想后果。其实可以避免索引失效的，具体什么场景我后续来说。 HBase属于新应用，怎么搞自己去看吧，我tm居然从服务端学了起来，虽然很浅读，但基本HBase架构读了一遍。数据流怎么走的也有了基础掌握，返回头才看client端。我们应用方属于c端，大数据部门原来替我们做了server的管理。（其实我觉得，服务端和客户端都要学，因为调优的参数应用要懂吧、实际业务难免要做业务补偿，比如旧的历史消息bulkload进HBase、安全机制） 实际业务需求，这里所说的就是我们不敢，甚至任何一家公司新运用HBase也不敢直接取代mysql的地位。但想用HBase，那怎么保留mysql和HBase平等地位呢？欢迎留言探讨。 压测，我居然搞了jemeter接口测试，过程你懂的。其实HBase有专业的压测，支持多线程。YCSB！ HBase的基础—HDFS大家不要被大数据搞晕了，其实没什么恐怖的，虽然学起来有些难度，HDFS是大数据存储的底层系统了，从这里我们可以了解到整个Hadoop生态圈，我们先搞定这个，再去玩HBase。进而玩起spark、sqoop、Flink。 先给大家讲个故事一次赵栋请客吃饭，他叫了二十多个人，忆江南并没有大桌，只能分小桌落座。so，众口难调，为了照顾大家，每桌都是自己点菜，赵栋也是忙前忙后，忙着了解大家都点了些啥，毕竟他要结账。服务员一下子面对六桌，菜有点多，有些菜忘了上哪桌，赵栋也会告诉他们。吃着吃着，突然博英桌点的一道猪蹄汤进了一个苍蝇。赵栋说：“没事儿，隔壁桌也点了这个汤。我去给你盛一碗。” 吃完饭，大家都很开心，一起踏上去小竹签的路上… 你以为我在说废话，讲故事。这个故事就是HDFS的精髓！ 初识HDFS—分布式文件系统先不谈分布式，先说说文件系统：文件系统就是用来管理文件的一个系统，比如你今天送我的mac pro，我在命令行敲一个ls，就能看到当前目录的文件。就算你没有钱、没送我mac。没关系，你给我装一个linux系统，我运行ls出来的显示，同样是直观的文件系统。文件系统官方来说是存储文件、管理文件，并且提供文件系统的crud。问题来了，这些文件存在哪？硬盘！ 知道了吧，万事万物想要持久化都要必进硬盘，别跟我扯什么云….云最后去哪了。 单台机器，单个硬盘的场景。假设，我有一个文件（600G），但我的mac只有256G，我存个鬼。so，怎么办。有人跟我说加硬盘…你干点别的吧。把这个文件拆开呀！！！我有钱，我有10台mac，那我就拆成10份。每份60G，完全可以放进单台256G的mac了。多台机器形成的系统就是分布式文件系统。那么这里又有一个问题，单台机器单台ls，多台机器每个都要ls吗？否，提供一个统一的查询接口，对使用者而言就像操作一台机器一样！这就是HDFS下的 hdfs dfs -ls ，命令的精髓。也就是分布式文件系统的意思。 文件切片赵栋请吃饭，一下子来20多人，怎么可能坐一桌，拆啊！！！这不就是文件切片的一个意思么。大文件切块后分别存储在多台机器，然后提供统一的操作接口，是不是分布式文件系统hdfs也是挺简单的。 潜在问题我把文件分了这么多份，我想找一个文件所在位置。我怎么去找到确定的一台机器呢？有人说，遍历一遍所有机器….哈哈，这样就太慢了，而且效率很低，是一个o(n)的算法。还有啊，我们刚刚说一个文件分别存在几台机器上，假如其中一台机器坏了，那么这个文件就不能访问了？有人说，机器故障率是0.000001%，然后他就觉得没问题。他这样想属于正常思维，但是一点就是他说的是单台机器。我们有10000000台机器，每台机器的事故率如上，那么这套系统的事故率是多少呢？10%….哪有这么多台机器！？你问问阿里的工程师他们有多少台虚机、物理机….如果要存储PB级或者EB级的数据，成千上万台机器组成的集群是很常见的，所以说分布式系统比单机系统要复杂得多呀。 潜在问题破壁1——上菜是溯源破壁者的使命都是从他们的宿愿说起，了解下HDFS架构至关重要，才能解决这些潜在问题。再回溯到刚才说的吃饭情景。 服务员上菜的同时，并不知道这是哪桌的菜，“what fuck！5桌人，我该问谁？”。只能问赵栋，菜都是赵栋点的 同等，我们看看HDFS的架构图像不像… 自己类比下啊，你可以把每一桌看作是一个DataNode、每一道菜看成一个文件切片，服务员相当于Client。文件切片具体放到哪个文件中，要去NameNode中寻找。DataNode是真正存储数据的地方，NameNode相当于一个管理者master，它知道每一个DataNode的存储情况，client其实就是那个对外操作的统一接口。讲到这，引申一下，服务员不管上菜（写数据），还是找哪桌的盘子空了（读数据）都要先去赵栋这里询问下，赵栋（NameNode）统一回复服务员。这样时间复杂度是O(1)。效率提升了不少。 破壁2——掉进猪蹄汤的苍蝇假如说某一个DataNode坏了怎么办？像我刚才说的坏了一台机器。再类比吃饭什么情景呢？对，苍蝇掉进了猪蹄汤里。这一桌的人都不想动筷了，但又对这些美食抗拒不住。其他桌有相同的菜啊！这桌人全部去了隔壁桌…. 听到这里，有没有发现什么。没错，hdfs在写入一个数据块的时候，不会仅仅写入一个DataNode，而是会写入到多个DataNode中，这样，如果其中一个DataNode坏了，还可以从其余的DataNode中拿到数据，保证了数据不丢失。多数据备份不仅在这里体现，大数据生态里面有很多也用到了这样的做法，例如kafka的多数据备份，但是都要分布在不同的机器上，保证一台机器挂了，可以在其他机器节点上找到数据。这样对存储空间的要求就很大，现在SSD都降价了，别说你的存储空间不够用。 破壁3——上菜都是服务员的事儿服务员把美食从后厨端过来的时候，就已经知道要把四盘一样的菜轮番上到不同的四桌。这里代表数据分片已经由Client端做了。 破壁4—— 一套完整的酒席从服务员上菜分桌，到同学们吃饭，再到离场撤盘子。这一套完整的酒席，就是HDFS一套完整的交互，流读取。HDFS查询： HDFS读取： 写入这里有个问题，就是我们之前说的多数据备份问题。这里不能交给Client做，那不得累死服务员，由同学们自己安排就好了。也就是采用目前比较流行的binlog方法。mysql读写分离也采用这个方法，mysql转数据仓库也用binlog。DataNode之间自己同步，效率有很大提升。服务员只专心上菜。 中间难免的小插曲，比如说赵栋出去接电话，他这一个NameNode不在了（Down机），服务员正好来上菜。怎么办？所以只能准备多个NameNode，也就是NameNode集群，选举Master进行更替。如果只有一个赵栋的话，出现不在场行为，这酒席进行不下去了。必备的NameNode是必要的。NameNode中存储了整个系统的元数据metadata，是指描述数据的数据，这里指描述文件的数据，比如文件路径，文件被分为几块？每个块在哪些DataNode上等。 假如所有NameNode都要重启怎么办？防止这种情况，可以持久化到硬盘上啊，重启之后再从硬盘把数据恢复到内存不就行了？那要把所有数据都先写一遍硬盘、再重启后读一遍硬盘、每个metadata都要访问一遍….想想效率问题。HDFS采用WAl思想，所有的写入行为都被追踪到了日志中，一旦系统重启，恢复这个wal日志行为到重启前的样子就可以。 那WAL这个日志行为你想想跑了几个月有多大，一旦重启…还要回溯到几个月前的行为再来一遍写入吗？不现实。所以hdfs的设计者也想到了这个问题。你看看之前hdfs的架构图，里面有一个SecondNameNode，就是用来解决这个问题。 刚刚其实只说了一半，NameNode确实会回放editlog，但是不是每次都从头回放，它会先加载一个fsimage，这个文件是之前某一个时刻整个NameNode的文件元数据的内存快照，然后再在这个基础上回放editlog，完成后，会清空editlog，再把当前文件元数据的内存状态写入fsimage，方便下一次加载。一句话，简单来说全量导入变成了增量导入。 WAL只是解决了不全部回追数据的问题，NameNode还是很大的问题没有解决。SecondNameNode出场，他会定期将WAL日志存入fsimage中，然后清空WAL日志，就相当于跑个定时。刚才说的，主NameNode挂了，集群中另一个NameNode顶上，可不是这个SecondNameNode啊，完全两回事。 破壁4—相爱相杀的Master和Activehadoop2.x之前，整个集群只能有一个NameNode，是有可能发生单点故障的，所以hadoop1.x有本身的不稳定性。但是hadoop2.x之后，我们可以在集群中配置多个NameNode，就不会有这个问题了，但是配置多个NameNode，需要注意的地方就更多了，系统就更加复杂了。Hadoop1.0也想到了系统的稳定性，但这和系统的设计理念是有关系的，虽然hadoop1.x存在一个NameNode单点，但是它大大简化了系统的复杂度，并且数据量在一定范围内时，NameNode并没有这么容易挂，所以那个时代是被接受的。但是随着数据量越来越大，这个单点始终是个隐患，所以设计者不得不升级为更加复杂的hadoop2.x，来保证NameNode的高可靠。俗话说一山不容二虎，两个NameNode只能有一个是活跃状态active，另一个是备份状态standby，我们看一下两个NameNode的架构图。 变的复杂了吧，多看几遍就会了。active的NameNode挂了之后，standby的NameNode要马上接替它，所以它们的数据要时刻保持一致，在写入数据的时候，两个NameNode内存中都要记录数据的元信息，并保持一致。这个JournalNode就是用来在两个NameNode中同步数据的，并且standby NameNode实现了SecondNameNode的功能。注意，hadoop2.x如果只部署一个NameNode，还是会用SecondNameNode。 总结HDFS优缺点：hdfs可以存储海量数据，并且是高可用的，任何一台机器挂了都有备份，不会影响整个系统的使用，也不会造成数据丢失。HDFS不适合存大批量的小文件，每一个小文件都有元信息，它们都存在NameNode里面，可能造成NameNode的内存不足。HDFS设计之初就是为了存储，不实用与编辑。所以，控制好你的业务方向…随机读写的效率很低。 1、hdfs是一个分布式文件系统，简单理解就是多台机器组成的一个文件系统。 2、hdfs中有3个重要的模块，client对外提供统一操作接口，DataNode真正存储数据，NameNode协调和管理数据，是一个典型的master-slave架构。 3、hdfs会对大文件进行切块，并且每个切块会存储备份，保证数据的高可用，适合存储大数据。 4、NameNode通过fsimage和editlog来实现数据恢复和高可用。 5、hdfs不适用于大量小文件存储，不支持并发写入，不支持文件随机修改，查询效率大概在秒级。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次生动的TCP3次握手和4次挥手]]></title>
    <url>%2F2019%2F04%2F26%2F%E7%94%9F%E5%8A%A8%E7%9A%84%E4%B8%80%E6%AC%A1TCP3%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C4%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[前言TCP握手挥手过程很简单，并不复杂。问题是记了忘，忘了记。以此循环，地老天荒。本渣还是决定用简单的描述手码一遍握手挥手的过程。师弟师妹们也可以清楚一下零散的知识点。 TCP三次握手TCP三次握手就好比两个人在街上隔着一条大街互相瞅了一眼（别担心，这就是发生在东北），但是因为天儿不好，这俩人不能确定是否是心里的那个他，没法直接交手。只能尴尬的先招手确定下对方是否认识自己。重点来了！！！刘能首先向赵四招手（syn），赵四看到刘能向自己招手，嘴角撇了撇。刘能看到赵四嘴角撇了，确认了赵四成功辨认出了自己(进入estalished状态)。但是赵四还有点狐疑，向四周看了一看，有没有可能刘能是在看别人呢，他也需要确认一下。所以赵四也向刘能招了招手(syn)，刘能看到赵四向自己招手后知道对方是在寻求自己的确认，于是也学着嘴角撇了撇(ack)，赵四看到对方的撇嘴后确认了刘能就是在向自己打招呼(进入established状态)。于是俩人加快步伐，走到了一起，相互拥抱，互喊“亲家！！！”。我们看到这个过程中一共是四个动作，刘能招手–赵四撇嘴–赵四招手–刘能撇嘴。其中赵四连续进行了2个动作，先是撇嘴(回复对方)，然后再次招手(寻求确认)，实际上可以将这两个动作合一，招手的同时撇嘴(syn+ack)。于是四个动作就简化成了三个动作，刘能招手–赵四撇嘴并招手–刘能撇嘴。这就是三次握手的本质，中间的一次动作是两个动作的合并。我们看到有两个中间状态，synsent和synrcvd，这两个状态叫着「半打开」状态，就是向对方招手了，但是还没来得及看到对方的撇嘴。synsent是主动打开方的「半打开」状态，synrcvd是被动打开方的「半打开」状态。客户端是主动打开方，服务器是被动打开方。 syn_sent: syn package has been sentsyn_rcvd: syn package has been received TCP传输TCP 数据传输就是两个人隔空对话，差了一点距离，所以需要对方反复确认听见了自己的话。刘能喊了一句话(data)，赵四听见了之后要向刘能回复自己听见了(ack)。如果刘能喊了一句，半天没听到赵四回复，刘能就认为自己的话被大风吹走了，赵四没听见，所以需要重新喊话，这就是tcp重传。也有可能是赵四听到了刘能的话，但是赵四向刘能的回复被大风吹走了，以至于刘能没听见赵四的回复。刘能并不能判断究竟是自己的话被大风吹走了还是赵四的回复被大风吹走了，刘能也不用管，重传一下就是。 既然会重传，赵四就有可能同一句话听见了两次，这就是「去重」。「重传」和「去重」工作操作系统的网络内核模块都已经帮我们处理好了，用户层是不用关心的。 刘能可以向赵四喊话，同样赵四也可以向刘能喊话，因为tcp链接是「双工的」，双方都可以主动发起数据传输。不过无论是哪方喊话，都需要收到对方的确认才能认为对方收到了自己的喊话。 刘能虽然磕巴但话多，一说连说了八句话，赵四说话也费劲，这时候赵四可以不用一句一句回复，而是连续听了这八句话之后，一起向对方回复说前面你说的八句话我都听见了，这就是批量ack。但是刘能也不能一次性说了太多话，赵四的脑子短时间可能无法消化太多，两人之间需要有协商好的合适的发送和接受速率，这个就是「TCP窗口大小」。 网络环境的数据交互同人类之间的对话还要复杂一些，它存在数据包乱序的现象。同一个来源发出来的不同数据包在「网际路由」上可能会走过不同的路径，最终达到同一个地方时，顺序就不一样了。操作系统的网络内核模块会负责对数据包进行排序，到用户层时顺序就已经完全一致了。 TCP 四次挥手TCP断开链接的过程和建立链接的过程比较类似，只不过中间的两部并不总是会合成一步走，所以它分成了4个动作，刘能挥手(fin)——赵四伤感地撇嘴(ack)——赵四挥手(fin)——刘能伤感地撇嘴(ack)。 之所以中间的两个动作没有合并，是因为tcp存在「半关闭」状态，也就是单向关闭。刘能已经挥了手，可是人还没有走，只是不再说话，但是耳朵还是可以继续听，赵四呢继续喊话。等待赵四累了，也不再说话了，超刘能挥了挥手，刘能伤感地撇嘴了一下，才彻底结束了。上面有一个非常特殊的状态time_wait，它是主动关闭的一方在回复完对方的挥手后进入的一个长期状态，这个状态标准的持续时间是4分钟，4分钟后才会进入到closed状态，释放套接字资源。不过在具体实现上这个时间是可以调整的。 它就好比主动分手方要承担的责任，是你提出的要分手，你得付出代价。这个后果就是持续4分钟的time_wait状态，不能释放套接字资源(端口)，就好比守寡期，这段时间内套接字资源(端口)不得回收利用。 它的作用是重传最后一个ack报文，确保对方可以收到。因为如果对方没有收到ack的话，会重传fin报文，处于time_wait状态的套接字会立即向对方重发ack报文。 同时在这段时间内，该链接在对话期间于网际路由上产生的残留报文(因为路径过于崎岖，数据报文走的时间太长，重传的报文都收到了，原始报文还在路上)传过来时，都会被立即丢弃掉。4分钟的时间足以使得这些残留报文彻底消逝。不然当新的端口被重复利用时，这些残留报文可能会干扰新的链接。 4分钟就是2个MSL，每个MSL是2分钟。MSL就是maximium segment lifetime——最长报文寿命。这个时间是由官方RFC协议规定的。至于为什么是2个MSL而不是1个MSL，我还没有看到一个非常满意的解释。四次挥手也并不总是四次挥手，中间的两个动作有时候是可以合并一起进行的，这个时候就成了三次挥手，主动关闭方就会从finwait1状态直接进入到timewait状态，跳过了finwait_2状态。 总结TCP状态转换是一个非常复杂的过程，本文仅对一些简单的基础知识点进行了类比讲解。关于TCP的更多知识还需要读者去搜寻相关技术文章进入深入学习。如果读者对TCP的基础知识掌握得比较牢固，高级的知识理解起来就不会太过于吃力。]]></content>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[破解数据库连接工具navicat（亲测可用）]]></title>
    <url>%2F2019%2F04%2F25%2F%E7%A0%B4%E8%A7%A3%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E5%B7%A5%E5%85%B7navicat%EF%BC%88%E4%BA%B2%E6%B5%8B%E5%8F%AF%E7%94%A8%EF%BC%89%2F</url>
    <content type="text"><![CDATA[破解步骤下载1.最好下载Navicat Premium 12版本，至于mac 还是 win 自行选择。另外选择破解版本也要看下网上的介绍。本人用12版本已经够用了。生成秘钥对2.生成自己的RSA公钥私钥对 （这里直接使用大神的密钥，也可以自己生成测试可以用） 这步与windows版破解相同，可以用open ssl工具生成，也可以使用其他工具生成，注意密钥是2048位的，PKCS#8格式为了节省时间，可以使用我提供的一对密钥。公钥： —–BEGIN PUBLIC KEY—–MIIBITANBgkqhkiG9w0BAQEFAAOCAQ4AMIIBCQKCAQB8vXG0ImYhLHvHhpi5FS3gd2QhxSQiU6dQ04F1OHB0yRRQ3NXF5py2NNDw962i4WP1zpUOHh94/mg/KA8KHNJXHtQVLXMRms+chomsQCwkDi2jbgUa4jRFN/6N3QejJ42jHasY3MJfALcnHCY3KDEFh0N89FV4yGLyDLr+TLqpRecg9pkPnOp++UTSsxz/e0ONlPYrra/DiaBjsleAESZSI69sPD9xZRt+EciXVQfybI/2SYeAdXMm1B7tHCcFlOxeUgqYV03VEqiC0jVMwRCd+03NU3wvEmLBvGOmNGudocWIF/y3VOqyW1byXFLeZxl7s+Y/SthxOYXzu3mF+2/pAgMBAAE=—–END PUBLIC KEY—– 私钥： —–BEGIN RSA PRIVATE KEY—–MIIEogIBAAKCAQB8vXG0ImYhLHvHhpi5FS3gd2QhxSQiU6dQ04F1OHB0yRRQ3NXF5py2NNDw962i4WP1zpUOHh94/mg/KA8KHNJXHtQVLXMRms+chomsQCwkDi2jbgUa4jRFN/6N3QejJ42jHasY3MJfALcnHCY3KDEFh0N89FV4yGLyDLr+TLqpRecg9pkPnOp++UTSsxz/e0ONlPYrra/DiaBjsleAESZSI69sPD9xZRt+EciXVQfybI/2SYeAdXMm1B7tHCcFlOxeUgqYV03VEqiC0jVMwRCd+03NU3wvEmLBvGOmNGudocWIF/y3VOqyW1byXFLeZxl7s+Y/SthxOYXzu3mF+2/pAgMBAAECggEAK5qZbYt8wenn1uZg6onRwJ5bfUaJjApL+YAFx/ETtm83z9ByVbx4WWT7CNC7fK1nINy20/mJrOTZkgIxx6otiNC4+DIsACJqol+RLoo8I9pk77Ucybn65ZteOz7hVZIU+8j6LzW0KDt6yowXe75r7G/NEpfibNc3Zz81+oDd2x+bHyGbzc9QcePIVuEzkof6jgpbWrQZU14itx9lVxEgj/fbMccvBx8brR/l9ClmDZd9Y6TWsF1rfJpF3+DPeqFkKCiD7PGz3bs4O/ZdZrfV21ZNVusBW49G6bU63gQVKsOf1qGo3efbAW1HVxgTQ/lExVdcMvdenZm+ADKpL4/wUQKBgQDOfBjn3OC2IerUFu18EgCS7pSjTSibXw+TeX3D5zwszLC091G2rGlT5DihBUhMfesNdpoZynrs4YB6Sz9C3wSGAB8AM/tNvPhtSVtbMHmrdT2DEEKCvLkORNBnt+8aTu2hGRanw9aL1189gzwrmXK5ZuuURfgLrB9ihrvjo4VznQKBgQCapx13dEA1MwapBiIa3k8hVBCoGPsEPWqM33RBdUqUsP33f9/PCx00j/akwmjgQNnBlAJoY7LOqPCyiwOkEf40T4IlHdzYntWQQvHhfBwqSgdkTE9tKj43Ddr7JVFRL6yMSbW39qAp5UX/+VzOLGAlfzJ8CBnkXwGrnKPCVbnZvQKBgQCd+iof80jlcCu3GteVrjxMLkcAbb8cqG1FWpVTNe4/JFgqDHKzPVPUgG6nG2CGTWxxv4UFKHpGE/11E28SHYjbcOpHAH5LqsGy84X2za649JkcVmtclUFMXm/Ietxvl2WNdKF1t4rFMQFIEckOXnd8y/Z/Wcz+OTFF82l7L5ehrQKBgFXl9m7v6e3ijpN5LZ5A1jDL0Yicf2fmePUP9DGbZTZbbGR46SXFpY4ZXEQ9GyVbv9dOT1wN7DXvDeoNXpNVzxzdAIt/H7hN2I8NL+4vEjHG9n4WCJO4v9+yWWvfWWA/m5Y8JqusV1+N0iiQJ6T4btrE4JSVp1P6FSJtmWOKW/T9AoGAcMhPMCL+N+AvWcYt4Y4mhelvDG8e/Jj4U+lwS3g7YmuQuYx7h5tjrS33w4o20g/3XudPMJHhA3z+d8b3GaVM3ZtcRM3+Rvk+zSOcGSwn3yDy4NYlv9bdUj/4H+aU1Qu1ZYojFM1Gmbe4HeYDOzRsJ5BhNrrV12h27JWkiRJ4F/Q= —–END RSA PRIVATE KEY—–安装程序3 安装程序，并替换应用包内容目录中rpk文件的公钥 安装完毕后打开finder，找到应用程序，右键显示包内容，打开目录 /Contents/Resources，编辑rpk文件，将公钥替换并保存。算出有效的Mac版序列号密钥4 使用我算好的密钥可以跳过此步，继续第四步，节省时间。中文版64位密钥序列号： NAVH-T4PX-WT8W-QBL5 英文版64位密钥序列号： NAVG-UJ8Z-EVAP-JAUW解密请求码，生成激活码打开应用，断网！！！，点击注册，输入密钥 NAVH-T4PX-WT8W-QBL5，然后手动激活复制请求码，使用私钥解密请求码，得到激活码明文注意必须自己解密，因为解密后得到的“DI”是不同的激活码明文示例：{ “K” : “NAVHT4PXWT8WQBL5”, “P” : “Mac 10.13”, “DI” : “ODQ2Yjg2ZDBjMTEzMjhh”}在线RSA私钥解密：http://tool.chacuo.net/cryptrsaprikey 将得到的激活码明文进行修改，修改后格式如下{“K”:”NAVHT4PXWT8WQBL5”, “N”:”52pojie”, “O”:”52pojie.cn”, “DI”:”ODQ2Yjg2ZDBjMTEzMjhh”, “T”:1516939200}激活码明文格式最好复制我的，改变 “ “ 内的字符即可，在同一行哦，不要换行，否则激活失败！！！“K” “DI” 都替换成自己机器解密的信息，”N” “O” “T”自己定义 加密激活码明文，使用私钥加密激活码明文在线RSA私钥加密：http://tool.chacuo.net/cryptrsaprikey 复制加密后激活码信息到程序激活窗口，点击激活，即可成功激活完整教程贴图地址https://blog.csdn.net/xhd731568849/article/details/79751188]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rest微服务构建案例]]></title>
    <url>%2F2019%2F04%2F24%2FRest%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%84%E5%BB%BA%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[记住一句话：约定 &gt; 配置 &gt; 编码Rest微服务一个基础构建是整个微服务的一个基础，先说下总体介绍。 有以下几个部分： 1.mvc、mybatis（一带而过了） 2.maven（坐标、仓库、依赖、聚合、继承）。理论要熟练下。 maven的命令：maven -U clean package install 这是我常用的，也是在更新了包所必备去更新到本地的命令。其他工程才能依赖最新的jar包。 父工程包含很多子工程，子工程公用的依赖包可以提到父工程中。 撸码provider （一）构建父工程，分布式开发所必备的建工程流程。构建父工程要点就是maven 的打包packaging要为pom。goupId和artifactId都要起好，否则很容易混淆。 构建完pom父工程后，新构建的pom中有几大几个部分—- 1.头部(重要！)Other-&gt; Maven Moudle。goupId和artifactId都要起好还是一样的要起好，packaging这时侯 变了！变成了jar包。在构建框里面的置灰部分，你也会看见parent的工程也就是上方所说的父工程。点击确定！ 创建完毕后，返回头去父工程的pom文件中查看下，发现新增moudleName 构建实体类的样子记得最好用工具lombok （1）@AllArgsConstructor @NoArgsConstructor @Data @Accessors(chain = true) public class entity { private Long id; private String msg; private String DBSource; } （2）在yml中配置我们所需要的配置信息，例：mybatis、spring日志config、重要的是spring-application-name！！！！暴露出去的微服务名称。 （3）创建数据库genetator，自动生成mapper、entity、dao。 （4）给数据库造点假数据。对了，这里数据库的库和表要自己建好，否则（3）进行不下去。这一步省去了你很多时间，真正提高工作效率。如何运用，请看管理系统标签 （5）整合service层 （6）整合controller层 都在各层记得打@啊！！！ 添加主启动类：SpringApplication.run(所属类,args);启动！done consumer 构建consumer过程和上方大部分一致，直到（5）部，不再调用service层进行数据库操作了。 这里！！！！在controller中加入@Autowired RestTemplate restTemplate; 操作— restTemplate.getForObject();//传参3个参数，请求服务的URL（provider的controller地址），传递参数（provider api 对应参数），返回类型（可选boollean、有很多） restTemplate.postForObject();//同上 添加主启动类：SpringApplication.run(所属类,args);启动！done。实验下，发现consumer的调用直接调到了provider。 Eureka 假如我们要引进clould一个新组件，将provider引入Eureka，基本上有两步 1.新增Eureka依赖 2.@EnableEurekaServer 3.zuul 4.业务逻辑 5.注册中心/info/自定义ip/修改显示节点名称 6.Eureka的自我保护，在server主页面上出现红色字体，注意这个不是报错。是server的一种监听机制。 这里我们做个测试，我们频繁操作下provider配置文件下面的节点名称（instance id），会在server发现出现了上方的红色字体。自我保护机制：一句话， 好死不如赖活着。server 会保留了我们抖动的节点名称。某一时刻某个微服务不可用了，Eureka它不会立刻清理，依旧对微服务的信息进行保存。 应对网络异常的一种措施。 总结本期微服务的构建案例，总结其精髓就是rest api！同时也是和dubbo不同的所在。剩下搞过mvc开发的，熟悉其开发流程的同学来说，只是增添了一些小应用有没有，当然这些都是构建在SpringBoot的基础上搞的，SpringBoot这个框架，本人是真的喜欢，但是喜欢的同时千万要对其自动配置、起步依赖、Actuator、命令行界面要有深入理解。]]></content>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka的生产+消费]]></title>
    <url>%2F2019%2F04%2F23%2Fkafka%E7%9A%84%E7%94%9F%E4%BA%A7-%E6%B6%88%E8%B4%B9%2F</url>
    <content type="text"><![CDATA[消息队列的内容很丰富，目前我常用的是kafka，所以第一篇献给我的kafka，后续我们继续钻研下，RocketMQ、ActiveMQ。kafka的原生写法，不繁琐，直接看见配置项就👌，如果你是刚入门大数据或者是刚接手kafka的话难免有蒙圈的感觉，But！！！这不要紧，先蒙在其中，后续你接触多了大数据的东西，就会发现，很多组件用了很多的配置项，加载进配置项就可以用了。当然我说的还是比较原生的写法，刚刚写完了一遍HBase的原生，一时脱离不出底层的范围哈哈哈。 心灵鸡汤一下：刚入职的小白遇到不会的问题，甚者被批评，千万不要气馁，觉得自己不适合干这个。行业没有不适合的，只有干的开不开心。坚持一下没准你就有小骄傲了呢？废话少说，开始撸码生产者public class KafkaProvider{ private final Producer&lt;String, String&gt; producer; public final static String TOPIC = &quot;TEST-TOPIC&quot;; public KafkaProvider(){ Properties props = new Properties();//配置项！这行代码是kafka的源头。 props.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);//zookeeper地址，集群用逗号分隔 //这里如果不熟悉的同学，可以看看kafa的架构图。每一个 ![](kafka架构.jpg) props.put(&quot;acks&quot;, &quot;all&quot;);//记录完整提交，最慢但是最大可能的持久化 props.put(&quot;retries&quot;, 3);//请求失败的重试次数 props.put(&quot;batch.size&quot;, 16384);//batch大小 props.put(&quot;linger.ms&quot;, 1);// 默认情况即使缓冲区有剩余的空间，也会立即发送请求，设置一段时间用来等待从而将缓冲区填的更多，单位为毫秒，producer发送数据会延迟1ms，可以减少发送到kafka服务器的请求数据 props.put(&quot;buffer.memory&quot;, 33554432);// 提供给生产者缓冲内存总量 props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);//写话方式 props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //生成生产者 producer = new KafkaProducer&lt;String, String&gt;(props); } //在入口函数调用即可，具体应用情形根据项目实际来看 public void producer(){ int messageNo = 1; final int COUNT =1000; while (1){ String num = String.valueOf(messageNo); String data = &quot;hello kafka message:&quot; + num; ProducerRecord record = new ProducerRecord(TOPIC,data); producer.send(record); messageNo ++ ; System.out.println(messageNo); try { sleep(1000); } catch ( InterruptedException e ) { e.printStackTrace(); } } } }消费者public class KafkaConsumer { private final Consumer consumer ; public final static String TOPIC = “TEST-TOPIC”; private ExecutorService executors; public KafkaCon(){ Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);//zk同生产者一致 props.put(&quot;group.id&quot;, &quot;2&quot;);//分组Id！！！！ props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);//自动提交，这是对offset的操作，有些需要对offset更加精准的处理，需要进行手动提交。 props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); consumer = new KafkaConsumer&lt;String, String&gt;(props); consumer.subscribe(Arrays.asList(TOPIC)); execute(10); } public void execute(int workerNum) { executors = new ThreadPoolExecutor(workerNum, workerNum, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue(1000), new ThreadPoolExecutor.CallerRunsPolicy()); Thread t = new Thread(new Runnable(){//启动一个子线程来监听kafka消息 @Override public void run(){ while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(200); for (final ConsumerRecord record : records) { System.out.println(&quot;【Kafka】监听到kafka的TOPIC【&quot; + record.topic() + &quot;】的消息&quot;); System.out.println(&quot;【Kafka】消息内容：&quot; + record.value()); executors.submit(new ConsumerWorker(record)); } } }}); t.start(); } } 这是标准的一个kafka配置信息，生产实战中的应用很简单，千万记住几个关键词：produce、consumer、topic、group.id、partition、broker，schema的含义，主要还是理解吧。提几个问题哈？如果你想多个消费者消费一个topic怎么办？partion中的消息是顺序的，多个partion间的消息呢？]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>消息队列，kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 代码请求URl样例]]></title>
    <url>%2F2019%2F04%2F23%2Fjava-%E4%BB%A3%E7%A0%81%E8%AF%B7%E6%B1%82URl%E6%A0%B7%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[习惯了在开发中运用工具（postman等）调试接口，几乎忘了用代码可以更好的处理实际情况。 Demo样例 /** * 发送post请求 * @param url 路径 * @param jsonObject 参数(json类型) * @param encoding 编码格式 * @return * @throws ParseException * @throws IOException */ public static String send(String url, JSONObject jsonObject,String encoding) throws ParseException, IOException{ String body = &quot;&quot;; //创建httpclient对象 CloseableHttpClient client = HttpClients.createDefault(); //创建post方式请求对象 HttpPost httpPost = new HttpPost(url); //装填参数 StringEntity s = new StringEntity(jsonObject.toString(), &quot;utf-8&quot;); //参数体有按要求也要进行Content-type赋值 s.setContentType(&quot;application/json&quot;); s.setContentEncoding(new BasicHeader(HTTP.CONTENT_TYPE, &quot;application/json&quot;)); //设置参数到请求对象中 httpPost.setEntity(s); System.out.println(&quot;请求地址：&quot;+url); // System.out.println(“请求参数：”+nvps.toString()); //设置header信息 //指定报文头【Content-type】、【User-Agent】选择其一就ok httpPost.setHeader(&quot;Content-type&quot;, &quot;application/x-www-form-urlencoded&quot;); httpPost.setHeader(&quot;Content-type&quot;, &quot;application/json&quot;); httpPost.setHeader(&quot;User-Agent&quot;, &quot;Mozilla/4.0 (compatible; MSIE 5.0; Windows NT; DigExt)&quot;); //执行请求操作，并拿到结果（同步阻塞） CloseableHttpResponse response = client.execute(httpPost); //获取结果实体 HttpEntity entity = response.getEntity(); if (entity != null) { //按指定编码转换结果实体为String类型 body = EntityUtils.toString(entity, encoding); } EntityUtils.consume(entity); //释放链接 response.close(); return body; }]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot深入剖析，够用了。]]></title>
    <url>%2F2019%2F04%2F21%2FSpringBoot%E6%94%B9%E9%80%A0%2F</url>
    <content type="text"><![CDATA[刚入职的时候，发现我们接手项目是ssm和SpringBoot混用，这个”混用”听起来奇怪哈？一句话就是很多依赖项用了SpringBoot又用了原生，还用了ssm中的一些写法。直观看上去一句my god～～～对新入职的应届生来说，多少会有混淆，用起来也会很撇脚（碍手？）。但本菜狗认为这真是一个学习的好机会，从最原生的写一遍，写到SpringBoot，写到地老天荒。另外悄悄告诉你，我们的之前组件都是原生的哈哈哈哈，根本没有Compent，你知道怎么实现的吗？ 以上都是废话 干货 我介绍下SpringBoot的原理，也偷偷抓紧复习下。—2019年4月21日 构建耽误了几天哈—2019年4月29日本文不东扯西扯、只专注剖析SpringBoot。废话不多说，撸码！ 这是一个已经建好的启动类，我用的编译器是IDEA，我也推荐各位用上这款软件，我只专注java开发哈，至于kotlin，Lua….我是没有用这个软件玩过。至于这么到这一步，麻烦看下标签中题目：构建微服务实战的介绍，很简单。简单说两句吧，我用IDEA构建项目一般都是用在线化构建，很方便，可以选择很多starter。不过新手我建议这么做， 还是老老实实用在线化下方的maven模块去构建。具体什么原因，请看完本篇再翻过头想想。 自己造一个启动类在我们建好的工程中，添加一个启动函数，这个应该会吧？注意在入口函数上打上@SpringBootApplication，完整入口函数如下。123456@SpringBootApplicationpublic class Main &#123; public static void main(String []args) &#123; SpringApplication.run(Main.class, args); &#125;&#125; 启动！没问题。 世界从@SpringBootApplication开始。我刚让大家在启动类打上的@SpringBootApplication，为什么被称为世界的入口。来进入这个@看看。这里面讲解有点长啊，我建议大家坚持下，一口气读完，断断续续的读效果不好。我开始断断续续的看源码，效果真的差。 这里面有三个重要标签@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan。即 @SpringBootApplication = (默认属性)@Configuration + @EnableAutoConfiguration + @ComponentScan。 @SpringBootConfiguratio拿@SpringBootConfiguration来说，这里面其实就是要一个@Configuration的注解，来定义了这个Class为配置类。再引申来说@Configuration的作用，你暂且把它看成要往Spring注入实例的一个大门。 官方来说，javaConfig形式的ioc容器的配置类使用的都是@Configuration的注解。SpringBoot社区推荐javaConfig的配置形式。过去常写mvc的同学对javaConfig有点陌生哈，其实简单一句话就是把过去的xml的各种配置类，放进了代码中。有人就很反感这种，认为配置就是配置，代码就是代码啊，把配置融进代码反而复杂了感觉，我开始也这么认为。后来，我的表现….真香！ 举几个简单例子回顾下，XML跟config配置方式的区别： （1）表达形式层面： 基于XML配置的方式是这样：1234567&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd" default-lazy-init="true"&gt; &lt;!--bean定义--&gt;&lt;/beans&gt; 而基于JavaConfig的配置方式是这样： 1234@Configurationpublic class Configuration&#123; //bean定义&#125; （2）注册bean定义层面 基于XML的配置形式是这样：123&lt;bean id="DubboService" class="..DubboServiceImpl"&gt; ...&lt;/bean&gt; 而基于JavaConfig的配置形式是这样的：1234567@Configurationpublic class DubboConfiguration&#123; @Bean public DubboService dubboService()&#123; return new DubboServiceImpl(); &#125;&#125; 任何一个标注了@Bean的方法，其返回值将作为一个bean定义注册到Spring的IoC容器，方法名将默认成该bean定义的id。也可以自己取名字，起名字按规范一般也都是方法名。 1@Bean(name = "dubboService") （3）表达依赖注入关系层面 为了表达bean与bean之间的依赖关系，在XML形式中一般是这样： 1234&lt;bean id="DubboService" class="..DubboServiceImpl"&gt; &lt;propery name ="dependencyService" ref="dependencyService" /&gt;&lt;/bean&gt;&lt;bean id="dependencyService" class="DependencyServiceImpl"&gt;&lt;/bean&gt; 而基于JavaConfig的配置形式是这样的：123456789101112@Configurationpublic class Configuration&#123; @Bean public DubboService dubboService()&#123; return new DubboServiceImpl(dependencyService()); &#125; @Bean public DependencyService dependencyService()&#123; return new DependencyServiceImpl(); &#125;&#125; 如果一个bean的定义依赖其他bean，则直接调用对应的JavaConfig类中依赖bean的创建方法就可以了。Spring注入实例大门@Configuration，现在请记住他的搭档--@Bean，他们是一对的。我们来看一个完整的案例。 .xml的123456&lt;beans&gt; &lt;bean id = "car" class="com.test.Car"&gt; &lt;property name="wheel" ref = "wheel"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id = "wheel" class="com.test.Wheel"&gt;&lt;/bean&gt; &lt;/beans&gt; 等同于1234567891011121314@Configuration public class Configuration &#123; @Bean public Car car() &#123; Car car = new Car(); car.setWheel(wheel()); return car; &#125; @Bean public Wheel wheel() &#123; return new Wheel(); &#125; &#125; @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。@Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan这个注解在xml中也有同样体现，@ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义]、@Controller、@Service最终将这些bean定义加载到IoC容器中。1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans"&gt; &lt;bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;&lt;/value&gt; ... &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package="url" /&gt; &lt;import resource="url"/&lt;/beans&gt; 如果不指定某一个扫描路径，则默认Spring框架实现会从声明@ComponentScan所在类的package进行扫描。所以这里面有个知识点，main启动类要在最根的目录上，会有组件扫描不进来的现象。 @EnableAutoConfiguration为什么这个@EnableAutoConfiguration我要放在最后说，这个是SpringBoot牛逼的体现，各位有没有之前在Spring上开启某些组件的，@Enable**等，@EnableScheduling是通过@Import将Spring调度框架相关的bean定义都加载到IoC容器。@EnableMBeanExport是通过@Import将JMX相关的bean定义加载到IoC容器。而@EnableAutoConfiguration也是借助@Import的帮助，将所有符合自动配置条件的bean定义加载到IoC容器，其实核心意思是一个。我们进入@EnableAutoConfiguration这个注解中看看。 由于import的支持，SpringBoot得以收集了各类场景的组件的支持。@Import(AutoConfigurationImportSelector.class)，在这个import的类AutoConfigurationImportSelector.class中，可以帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器。就像一只“八爪鱼”一样，借助于Spring框架原有的一个工具类：SpringFactoriesLoader的支持，@EnableAutoConfiguration可以智能的自动配置功效才得以大功告成！ 来看看这个Boss—SpringFactoriesLoader藏的并不深。123456789protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, "No auto configuration classes found in META-INF/spring.factories. If you " + "are using a custom packaging, make sure that file is correct."); return configurations;&#125; 其中 SpringFactoriesLoader.loadFactoryNames 是关键。再进入一层看看。 12345678910111213141516171819202122232425262728293031323334353637383940public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) &#123; String factoryClassName = factoryClass.getName(); return (List)loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());&#125;private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; MultiValueMap&lt;String, String&gt; result = (MultiValueMap)cache.get(classLoader); if (result != null) &#123; return result; &#125; else &#123; try &#123; Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources("META-INF/spring.factories") : ClassLoader.getSystemResources("META-INF/spring.factories"); LinkedMultiValueMap result = new LinkedMultiValueMap(); while(urls.hasMoreElements()) &#123; URL url = (URL)urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); Iterator var6 = properties.entrySet().iterator(); while(var6.hasNext()) &#123; Entry&lt;?, ?&gt; entry = (Entry)var6.next(); String factoryClassName = ((String)entry.getKey()).trim(); String[] var9 = StringUtils.commaDelimitedListToStringArray((String)entry.getValue()); int var10 = var9.length; for(int var11 = 0; var11 &lt; var10; ++var11) &#123; String factoryName = var9[var11]; result.add(factoryClassName, factoryName.trim()); &#125; &#125; &#125; cache.put(classLoader, result); return result; &#125; catch (IOException var13) &#123; throw new IllegalArgumentException("Unable to load factories from location [META-INF/spring.factories]", var13); &#125; &#125;&#125; 稳住别慌，到关键了！！！loadFactoryNames方法中的loadSpringFactories执行了这么一段Enumeration urls = classLoader != null ? classLoader.getResources(“META-INF/spring.factories”) : ClassLoader.getSystemResources(“META-INF/spring.factories”); classLoader.getResources(“META-INF/spring.factories”)看到没，这个META-INF文件下的spring.factories内容中获取我们准备加载的class的URL，这个META-INF就是再那spring.factories有什么呢？来睁大眼睛看！神不神奇，所以，@EnableAutoConfiguration自动配置的魔法骑士就变成了：从classpath中搜寻所有的META-INF/spring.factories配置文件，并将其中org.springframework.boot.autoconfigure.EnableutoConfiguration对应的配置项通过反射（JavaRefletion）实例化为对应的标注了@Configuration的JavaConfig形式的IoC容器配置类，然后汇总为一个并加载到IoC容器。从AutoConfiguration往下看，会看到很多自动配置的很多信息。有我们经常使用的种种比如这个 1org.springframework.boot.autoconfigure.quartz.QuartzAutoConfiguration,\ 这是一个定时任务的自动配置项，我们进入这个类里看看从@EnableConfigurationProperties(QuartzProperties.class)再进入QuartzProperties.class类中，发现注解项@ConfigurationProperties(“spring.quartz”)，这是什么啊？！这不就是，你在yml配置文件写入Spring.quartz所属的配置项了么，立图为证。 所以一个EnableAutoConfiguration帮我们解决了很多事情，不用再引入一对properties了，只需要在yml（有的使用properties，我推荐使用yml）配置文件中，加上自己想要的配置参数就可以了。 跟我来一遍SpringBoot的跑的流程run()方法—执行说完我们启动类的三大注解我们再次回到我们的启动类哈哈哈！ 123456789101112/** * @program: microservicecloudprovider * @description: * @author: Nou * @create: 2019-04-29 18:48 **/@SpringBootApplicationpublic class Main &#123; public static void main(String []args) &#123; SpringApplication.run(Main.class, args); &#125;&#125; 在SpringApplcation执行的run方法中，执行一个静态方法 123public static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) &#123; return run(new Class[]&#123;primarySource&#125;, args);&#125; new Class[]{primarySource}, args —new 出了一个Spring的实例，然后直接调用这个实例。在初始化的时候，SpringBoot会提前干这么几件事。 在类路径下（classPath）面找下是否有org.springframework.web.context.ConfigurableWebApplicationContext这个类，这是创建web环境的准备。是否创建一个为web准备的ApplicationContext类。 使用SpringFactoriesLoader在应用的classpath中查找并加载所有可用的ApplicationContextInitializer。 使用SpringFactoriesLoader在应用的classpath中查找并加载所有可用的ApplicationListener。以上两个也就是我刚才圈出来的启动项，联系起来了吧！！！ 推断并设置main方法的定义类。在启动类里面可以有些针对SpringBoot的配置，例如banner还有关闭webContext等。 run()方法—执行SpringApplication实例初始化完成并且完成设置后，就开始执行run方法的逻辑了，首先遍历执行所有通过SpringFactoriesLoader可以查找到并加载的SpringApplicationRunListener。调用它们的started()方法，告诉这些SpringApplicationRunListener，“嘿，SpringBoot应用要开始执行咯！”。 run()方法—加载配置参数创建并配置当前Spring Boot应用将要使用的Environment（包括配置要使用的PropertySource以及Profile），就是我们刚才所说在yml加载所有可用的参数。联系起来了吧。 run()方法—回调通知遍历所有SpringApplicationRunListener的environmentPrepared()，回调通知已经准备好环境 run()方法—遍历 根据用户是否明确设置了applicationContextClass类型以及初始化阶段的推断结果，决定该为当前SpringBoot应用创建什么类型的ApplicationContext并创建完成，然后根据条件决定是否添加ShutdownHook，决定是否使用自定义的BeanNameGenerator，决定是否使用自定义的ResourceLoader，当然，最重要的，将之前准备好的Environment设置给创建好的ApplicationContext使用。 钩子ShutdownHook的作用我这里没有深入研究，总之想要优雅关闭，准备一个钩子是最好的，保证所有线程都执行完毕。钩子也可以直接用我的哈哈。 1234567891011121314151617 Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; try &#123; LOG.info("shutdown start"); Thread.sleep(2000); LOG.info("shutdown end"); &#125; catch (InterruptedException e) &#123; &#125; synchronized (Main.class) &#123; running = false; Main.class.notify(); &#125; &#125;&#125;); ApplicationContext创建好之后，SpringApplication会再次借助Spring-FactoriesLoader，查找并加载classpath中所有可用的ApplicationContext-Initializer，然后遍历调用这些ApplicationContextInitializer的initialize（applicationContext）方法来对已经创建好的ApplicationContext进行进一步的处理。 run()方法—ApplicationContext 遍历调用所有SpringApplicationRunListener的contextPrepared()方法。 把刚才yml中加载的配置参数加入ApplicationContext中。 遍历调用所有SpringApplicationRunListener的contextLoaded()方法。 遍历调用所有SpringApplicationRunListener的contextLoaded()方法。 查找当前ApplicationContext中是否注册有CommandLineRunner，如果有，则遍历执行它们。 正常情况下，遍历执行SpringApplicationRunListener的finished()方法、（如果整个过程出现异常，则依然调用所有SpringApplicationRunListener的finished()方法，只不过这种情况下会将异常信息一并传入处理） run()背后的故事 这是run进入的创建SpringBoot的实例最后的一个构造类 123456789101112131415161718 public SpringApplication(ResourceLoader resourceLoader, Class... primarySources) &#123; this.sources = new LinkedHashSet(); this.bannerMode = Mode.CONSOLE; this.logStartupInfo = true; this.addCommandLineProperties = true; this.addConversionService = true; this.headless = true; this.registerShutdownHook = true; this.additionalProfiles = new HashSet(); this.isCustomEnvironment = false; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, "PrimarySources must not be null"); this.primarySources = new LinkedHashSet(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); this.setInitializers(this.getSpringFactoriesInstances(ApplicationContextInitializer.class)); this.setListeners(this.getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = this.deduceMainApplicationClass();&#125; 看看有没有我们刚才说的listener和ShutdownHook以及nitializers等，想再再深入就去SpringApplication这个类中看看这里面初始化的一些东西。 这些代码我教你怎么截有人问我，你这些原理和源码怎么收集和找到，我说下，我挺痛苦的。一篇博文至少要搞一下午时间，还要截图、撸码、码字、甚至还要早早搞完手头上的需求。所以有些遗漏在所难免，所以教你怎么去看这些东西吧，就是打Debug。每一个关键点，打上Debug，一层一层执行，框架也是代码码出来的，没有什么难的住的。 总结执行流程SpringBoot启动结构，我们发现启动流程主要分为三个部分： 第一部分进行SpringApplication的初始化模块，配置一些基本的环境变量、资源、构造器、监听器；第二部分实现了应用具体的启动方案，包括启动流程的监听模块、加载配置环境模块、及核心的创建上下文环境模块；第三部分是自动化配置模块，该模块作为springboot自动配置核心，在后面的分析中会详细讨论。在下面的启动程序中我们会串联起结构中的主要功能。 关键点都在SpringApplication这个类里面。 我们不如来再跟我走一次图示再看一遍流程，方便大家用撸码结合的来看。 在该构造方法内，我们可以发现其调用了一个初始化的this方法 主要是为SpringApplication对象赋一些初值。构造函数执行完毕后，我们回到run方法 该方法中实现了如下几个关键步骤： 创建了应用的监听器SpringApplicationRunListeners并开始监听 加载SpringBoot配置环境(ConfigurableEnvironment)，如果是通过web容器发布，会加载StandardEnvironment，其最终也是继承了ConfigurableEnvironment。 配置环境(Environment)加入到监听器对象中(SpringApplicationRunListeners) 创建run方法的返回对象：ConfigurableApplicationContext(应用配置上下文)，我们可以看一下创建方法：方法会先获取显式设置的应用上下文(applicationContextClass)，如果不存在，再加载默认的环境配置（通过是否是web environment判断），默认选择AnnotationConfigApplicationContext注解上下文（通过扫描所有注解类来加载bean），最后通过BeanUtils实例化上下文对象，并返回。ConfigurableApplicationContext类图如下：主要看其继承的两个方向：LifeCycle：生命周期类，定义了start启动、stop结束、isRunning是否运行中等生命周期空值方法ApplicationContext：应用上下文类，其主要继承了beanFactory(bean的工厂类) 接下来的refreshContext(context)方法，将是实现Spring-Boot-stater-*的关键。包括spring.factories的加载，bean的实例化等核心工作。 配置结束后，Springboot做了一些基本的收尾工作，返回了应用环境上下文。回顾整体流程，Springboot的启动，主要创建了配置环境(environment)、事件监听(listeners)、应用上下文(applicationContext)，并基于以上条件，在容器中开始实例化我们需要的Bean，至此，通过SpringBoot启动的程序已经构造完成，接下来我们来探讨自动化配置是如何实现。 自动化配置的话，你就按照我开篇的思路思考就可以，完全是你在yml写什么相关配置参数，并且在maven中导入相关的依赖，SpringBoot就会自动给你加进入。有没有很爽。总结一个图，虽然是别人的，我感觉很经典的一个导入图。 为什么这么爽，因为maven依赖的传递性，我们只要依赖starter就可以依赖到所有需要自动配置的类，实现开箱即用的功能。也体现出Springboot简化了Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。所以Spring-boot为我们提供了统一的starter可以直接配置好相关的类，触发自动配置所需的依赖。 结束的寡逼谢谢大家，有问题，下面留言一起探讨。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
        <tag>SpringBoot</tag>
        <tag>cloud</tag>
        <tag>kafka</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
</search>
